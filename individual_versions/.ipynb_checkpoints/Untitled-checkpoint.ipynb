{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RedditNews = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/Data/RedditNews.csv')\n",
    "combined = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/Data/Combined_News_DJIA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RedditNews = RedditNews[RedditNews['Date'].isin(combined['Date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54798</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"South Korea's parliament votes to legalize c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54804</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"The German economy is rapidly improving, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54819</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"BBC News - Russian spy suspect missing in Cy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54821</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"Iraq inquiry: secret documents showing Tony ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54822</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"Apartheid loves apartheid: Israel's secret r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                               News\n",
       "54798  2010-06-30  b\"South Korea's parliament votes to legalize c...\n",
       "54804  2010-06-30  b\"The German economy is rapidly improving, wit...\n",
       "54819  2010-06-30  b\"BBC News - Russian spy suspect missing in Cy...\n",
       "54821  2010-06-30  b\"Iraq inquiry: secret documents showing Tony ...\n",
       "54822  2010-06-30  b\"Apartheid loves apartheid: Israel's secret r..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54799</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'Pope rebukes cardinal who exposed abuse: \\nP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54800</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'This depression is similar to the Great Pani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54801</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'The Niger Delta has experienced oil spills o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54802</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'G20 Toronto - So Black Block get green light...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54803</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'Half of Afghanistans 476 women prisoners wer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                               News\n",
       "54799  2010-06-30  b'Pope rebukes cardinal who exposed abuse: \\nP...\n",
       "54800  2010-06-30  b'This depression is similar to the Great Pani...\n",
       "54801  2010-06-30  b'The Niger Delta has experienced oil spills o...\n",
       "54802  2010-06-30  b'G20 Toronto - So Black Block get green light...\n",
       "54803  2010-06-30  b'Half of Afghanistans 476 women prisoners wer..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Hay error en la codificación de caracteres especiales, encontré ese, pero hay que ver que otros surgen\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"year.old\", \" year old \", regex=True)\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"\\d year\", \" year \", regex=True)\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"year old\", \"years old\", regex=True)\n",
    "\n",
    "\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"years.old\", \" years old \", regex=True)\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"\\d years\", \" years \", regex=True)\n",
    "## Hay error en la codificación de caracteres especiales, encontré ese, pero hay que ver que otros surgen\n",
    "index_review = RedditNews[(RedditNews['News'].str.startswith('b\"')) |\n",
    "                         (RedditNews['News'].str.startswith(\"b'\"))].index\n",
    "\n",
    "display(RedditNews[RedditNews['News'].str.startswith('b\"')].head(),\n",
    "        RedditNews[RedditNews['News'].str.startswith(\"b'\")].head())\n",
    "\n",
    "\n",
    "RedditNews['News'] = RedditNews['News'].str.replace('^b\\\"', \" \", regex=True)\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"^b\\'\", \" \", regex=True)\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words(\"english\")\n",
    "REPLACE_BY_SPACE_RE = re.compile('[(){}\\[\\]\\|@,;-]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z ^#\\+_]')\n",
    "SEP_NUMBER = re.compile('(?<=\\d)\\,|\\.(?=\\d)')\n",
    "USA_ABREV = re.compile('U\\.S|u\\.s\\|u\\.s\\.a\\.|US')\n",
    "DOT_ABREV = re.compile('\\.(?![a-zA-Z]{2})')\n",
    "STARTING_B = re.compile(\"^\\\"b' |^b \")\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    \n",
    "    text = USA_ABREV.sub(' usa ', text) # replace U.S U.S. u.s US for usa\n",
    "    text = text.lower() # lowercase text\n",
    "    text = text.replace(\"al-qaeda\", \"alqaeda\")\n",
    "    text = text.replace(\"al-qa'eda\", \"alqaeda\")\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&', '')    \n",
    "\n",
    "    text = DOT_ABREV.sub('', text) # removes abrevetion dot, ej: L.G.B.T  = LGBT\n",
    "    text = SEP_NUMBER.sub('', text) # removes . and , seprating numbers\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub(' ', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = STARTING_B.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    text = text.strip()\n",
    "    return text\n",
    "    \n",
    "RedditNews['News'] = RedditNews['News'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>117 years old woman mexico city finally receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>imf chief backs athens permanent olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>president france says brexit donald trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>british man must give police 24 hours notice s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>100+ nobel laureates urge greenpeace stop oppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72078</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>pentagon thinks attacking iran bad idea usa ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72079</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>caucasus crisis georgia invades south ossetia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72080</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>indian shoe manufactory series like work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72081</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>visitors suffering mental illnesses banned oly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72082</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>help mexico kidnapping surge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49718 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                               News\n",
       "0      2016-07-01  117 years old woman mexico city finally receiv...\n",
       "1      2016-07-01      imf chief backs athens permanent olympic host\n",
       "2      2016-07-01          president france says brexit donald trump\n",
       "3      2016-07-01  british man must give police 24 hours notice s...\n",
       "4      2016-07-01  100+ nobel laureates urge greenpeace stop oppo...\n",
       "...           ...                                                ...\n",
       "72078  2008-08-08  pentagon thinks attacking iran bad idea usa ne...\n",
       "72079  2008-08-08      caucasus crisis georgia invades south ossetia\n",
       "72080  2008-08-08           indian shoe manufactory series like work\n",
       "72081  2008-08-08  visitors suffering mental illnesses banned oly...\n",
       "72082  2008-08-08                       help mexico kidnapping surge\n",
       "\n",
       "[49718 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RedditNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RedditNews.drop_duplicates(subset=['News'], keep='first', inplace=True)\n",
    "RedditNews = RedditNews.reset_index(drop = True)\n",
    "topics_sentiment = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/Data/topics_sentiment.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = pd.to_datetime('2014-12-31')\n",
    "RedditNews['Date'] =  pd.to_datetime(RedditNews['Date'], format='%Y-%m-%d')\n",
    "num_training = len(RedditNews[(RedditNews[\"Date\"]) <= training_end])\n",
    "train_data = RedditNews.drop([ \"Date\"], axis=1)[:num_training]\n",
    "test_data = RedditNews.drop([ 'Date'], axis=1)[num_training:]\n",
    "train_data['Label'] = topics_sentiment[\"Label\"].values[:num_training]\n",
    "test_data['Label'] = topics_sentiment[\"Label\"].values[num_training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=train_data.sample(frac=0.2,random_state=200)\n",
    "train_data=train_data.drop(val_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=train_data.News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        117 years old woman mexico city finally receiv...\n",
       "1            imf chief backs athens permanent olympic host\n",
       "2                president france says brexit donald trump\n",
       "3        british man must give police 24 hours notice s...\n",
       "4        100+ nobel laureates urge greenpeace stop oppo...\n",
       "                               ...                        \n",
       "40202    unidentified man become hero argentina pushed ...\n",
       "40204    drug related violence rise latin america forme...\n",
       "40205    knew uk government lost court appeal case judg...\n",
       "40206    austrian millionaire gives away fortune made m...\n",
       "40207    operation titstorm anonymous target internet c...\n",
       "Name: News, Length: 32166, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28910 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=20000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts)\n",
    "sequences_valid=tokenizer.texts_to_sequences(val_data.News)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (32166, 38) (8042, 38)\n",
      "Shape of label train and validation tensor: (32166, 2) (8042, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "y_train = to_categorical(np.asarray(train_data['Label']))\n",
    "y_val = to_categorical(np.asarray(val_data['Label']))\n",
    "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
    "print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
