{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4dLP23xZmpC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "# Importar librerias\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "from datetime import date\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, GlobalMaxPool1D, Conv1D, Flatten,  MaxPooling1D, Activation, GlobalMaxPooling1D, Bidirectional, GRU\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.41 ms\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n",
      "time: 1.68 ms\n"
     ]
    }
   ],
   "source": [
    "exp_name = '7_c'\n",
    "folder = 'Resultados/' + exp_name\n",
    "my_file = Path(folder)\n",
    "if os.path.exists(my_file):\n",
    "    print('already exists')\n",
    "else:\n",
    "    os.makedirs(folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n",
      "time: 1.6 ms\n"
     ]
    }
   ],
   "source": [
    "ch_folder = folder + '/Checkpoints'\n",
    "my_file = Path(ch_folder)\n",
    "if os.path.exists(my_file):\n",
    "    print('already exists')\n",
    "else:\n",
    "    os.makedirs(ch_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nzv66BqFbl92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>Top</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49649</th>\n",
       "      <td>-0.492432</td>\n",
       "      <td>0.599487</td>\n",
       "      <td>-0.093018</td>\n",
       "      <td>0.693359</td>\n",
       "      <td>-0.435669</td>\n",
       "      <td>0.422607</td>\n",
       "      <td>0.255676</td>\n",
       "      <td>-0.246155</td>\n",
       "      <td>0.072754</td>\n",
       "      <td>-0.123047</td>\n",
       "      <td>0.260254</td>\n",
       "      <td>-1.105225</td>\n",
       "      <td>-0.861328</td>\n",
       "      <td>0.712006</td>\n",
       "      <td>-0.057861</td>\n",
       "      <td>0.604210</td>\n",
       "      <td>-0.070557</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>-0.253174</td>\n",
       "      <td>-0.049194</td>\n",
       "      <td>0.509827</td>\n",
       "      <td>-0.094482</td>\n",
       "      <td>0.330078</td>\n",
       "      <td>-0.333115</td>\n",
       "      <td>-0.522522</td>\n",
       "      <td>-1.142212</td>\n",
       "      <td>0.393860</td>\n",
       "      <td>-0.221680</td>\n",
       "      <td>0.310547</td>\n",
       "      <td>-0.728760</td>\n",
       "      <td>-0.543945</td>\n",
       "      <td>-0.105408</td>\n",
       "      <td>-0.695801</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>0.059509</td>\n",
       "      <td>-0.523254</td>\n",
       "      <td>-0.332424</td>\n",
       "      <td>0.226318</td>\n",
       "      <td>0.164551</td>\n",
       "      <td>1.169922</td>\n",
       "      <td>-0.001709</td>\n",
       "      <td>-0.466415</td>\n",
       "      <td>0.883545</td>\n",
       "      <td>-0.151978</td>\n",
       "      <td>0.344681</td>\n",
       "      <td>-0.574351</td>\n",
       "      <td>-0.012207</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>-1.159424</td>\n",
       "      <td>0.613647</td>\n",
       "      <td>-0.368536</td>\n",
       "      <td>0.491577</td>\n",
       "      <td>-0.683350</td>\n",
       "      <td>0.499237</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>-0.878418</td>\n",
       "      <td>-0.342377</td>\n",
       "      <td>0.133621</td>\n",
       "      <td>0.046997</td>\n",
       "      <td>-1.108765</td>\n",
       "      <td>0.212402</td>\n",
       "      <td>0.634277</td>\n",
       "      <td>0.986328</td>\n",
       "      <td>-0.915161</td>\n",
       "      <td>-0.344727</td>\n",
       "      <td>0.647949</td>\n",
       "      <td>0.238525</td>\n",
       "      <td>0.470703</td>\n",
       "      <td>-0.391079</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>-0.365234</td>\n",
       "      <td>-0.641724</td>\n",
       "      <td>0.220276</td>\n",
       "      <td>-0.104639</td>\n",
       "      <td>-0.633545</td>\n",
       "      <td>-0.388855</td>\n",
       "      <td>-0.221191</td>\n",
       "      <td>-0.955444</td>\n",
       "      <td>-1.069092</td>\n",
       "      <td>0.416504</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>-0.89978</td>\n",
       "      <td>0.149170</td>\n",
       "      <td>0.351822</td>\n",
       "      <td>-0.302734</td>\n",
       "      <td>-0.247536</td>\n",
       "      <td>-1.155273</td>\n",
       "      <td>0.153753</td>\n",
       "      <td>0.148926</td>\n",
       "      <td>-0.778564</td>\n",
       "      <td>0.809753</td>\n",
       "      <td>-0.926025</td>\n",
       "      <td>-0.226562</td>\n",
       "      <td>-0.794434</td>\n",
       "      <td>0.538044</td>\n",
       "      <td>0.575439</td>\n",
       "      <td>0.972168</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>1.219727</td>\n",
       "      <td>0.657959</td>\n",
       "      <td>0.019409</td>\n",
       "      <td>1.007690</td>\n",
       "      <td>-0.352432</td>\n",
       "      <td>0.953613</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>-0.411011</td>\n",
       "      <td>0.151123</td>\n",
       "      <td>-0.057129</td>\n",
       "      <td>0.549316</td>\n",
       "      <td>-0.941864</td>\n",
       "      <td>-0.280273</td>\n",
       "      <td>0.674316</td>\n",
       "      <td>0.817871</td>\n",
       "      <td>-0.033081</td>\n",
       "      <td>0.217041</td>\n",
       "      <td>-0.405029</td>\n",
       "      <td>-0.095459</td>\n",
       "      <td>-0.589111</td>\n",
       "      <td>0.143066</td>\n",
       "      <td>0.345703</td>\n",
       "      <td>-1.480469</td>\n",
       "      <td>0.490479</td>\n",
       "      <td>-0.330811</td>\n",
       "      <td>-0.719460</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>-0.819824</td>\n",
       "      <td>0.159012</td>\n",
       "      <td>0.303375</td>\n",
       "      <td>-0.033234</td>\n",
       "      <td>0.617432</td>\n",
       "      <td>-0.545166</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>-0.759766</td>\n",
       "      <td>-0.610535</td>\n",
       "      <td>-1.591797</td>\n",
       "      <td>-1.320068</td>\n",
       "      <td>-0.268829</td>\n",
       "      <td>0.932373</td>\n",
       "      <td>-0.601868</td>\n",
       "      <td>0.573608</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>0.267883</td>\n",
       "      <td>0.271973</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>-0.249573</td>\n",
       "      <td>-0.525879</td>\n",
       "      <td>0.053955</td>\n",
       "      <td>-1.141113</td>\n",
       "      <td>-0.465973</td>\n",
       "      <td>1.397949</td>\n",
       "      <td>-0.469971</td>\n",
       "      <td>-0.290771</td>\n",
       "      <td>-0.273438</td>\n",
       "      <td>-0.590820</td>\n",
       "      <td>-0.065552</td>\n",
       "      <td>0.558838</td>\n",
       "      <td>-1.258057</td>\n",
       "      <td>-0.475342</td>\n",
       "      <td>-1.066162</td>\n",
       "      <td>0.994263</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>1.062988</td>\n",
       "      <td>-0.077209</td>\n",
       "      <td>-0.551392</td>\n",
       "      <td>-0.672363</td>\n",
       "      <td>0.343262</td>\n",
       "      <td>0.047791</td>\n",
       "      <td>-0.126709</td>\n",
       "      <td>-0.347717</td>\n",
       "      <td>-0.871460</td>\n",
       "      <td>0.351929</td>\n",
       "      <td>-0.394928</td>\n",
       "      <td>-0.150787</td>\n",
       "      <td>-0.191040</td>\n",
       "      <td>-0.317139</td>\n",
       "      <td>0.836761</td>\n",
       "      <td>-0.863525</td>\n",
       "      <td>-0.372559</td>\n",
       "      <td>-0.375732</td>\n",
       "      <td>-0.088928</td>\n",
       "      <td>0.257446</td>\n",
       "      <td>0.235229</td>\n",
       "      <td>-0.357178</td>\n",
       "      <td>0.405518</td>\n",
       "      <td>0.442749</td>\n",
       "      <td>-0.997925</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.151611</td>\n",
       "      <td>0.729248</td>\n",
       "      <td>0.102905</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.212646</td>\n",
       "      <td>0.300171</td>\n",
       "      <td>0.185791</td>\n",
       "      <td>0.425049</td>\n",
       "      <td>0.461548</td>\n",
       "      <td>-0.781372</td>\n",
       "      <td>-1.406250</td>\n",
       "      <td>-0.413208</td>\n",
       "      <td>0.093262</td>\n",
       "      <td>0.378479</td>\n",
       "      <td>-0.932495</td>\n",
       "      <td>-0.204803</td>\n",
       "      <td>0.772461</td>\n",
       "      <td>-0.195068</td>\n",
       "      <td>0.344238</td>\n",
       "      <td>-0.710205</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>0.614014</td>\n",
       "      <td>-0.857391</td>\n",
       "      <td>0.180359</td>\n",
       "      <td>-0.596069</td>\n",
       "      <td>0.726891</td>\n",
       "      <td>0.012939</td>\n",
       "      <td>-0.928711</td>\n",
       "      <td>0.442383</td>\n",
       "      <td>0.057129</td>\n",
       "      <td>-0.417480</td>\n",
       "      <td>-0.898407</td>\n",
       "      <td>0.023597</td>\n",
       "      <td>-0.171570</td>\n",
       "      <td>0.528809</td>\n",
       "      <td>-0.144043</td>\n",
       "      <td>0.058113</td>\n",
       "      <td>-0.012451</td>\n",
       "      <td>-0.117798</td>\n",
       "      <td>-0.091248</td>\n",
       "      <td>-0.506897</td>\n",
       "      <td>0.209167</td>\n",
       "      <td>0.256836</td>\n",
       "      <td>-0.285400</td>\n",
       "      <td>-0.053955</td>\n",
       "      <td>0.405945</td>\n",
       "      <td>-0.092773</td>\n",
       "      <td>-0.511871</td>\n",
       "      <td>0.540527</td>\n",
       "      <td>0.442810</td>\n",
       "      <td>1.110840</td>\n",
       "      <td>-0.280029</td>\n",
       "      <td>0.704468</td>\n",
       "      <td>-0.080627</td>\n",
       "      <td>0.484619</td>\n",
       "      <td>-0.789856</td>\n",
       "      <td>-0.094971</td>\n",
       "      <td>0.971191</td>\n",
       "      <td>-0.034668</td>\n",
       "      <td>0.125916</td>\n",
       "      <td>0.033936</td>\n",
       "      <td>-1.453369</td>\n",
       "      <td>0.509521</td>\n",
       "      <td>0.103210</td>\n",
       "      <td>0.616211</td>\n",
       "      <td>0.146118</td>\n",
       "      <td>-0.011353</td>\n",
       "      <td>-0.479004</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>-0.363647</td>\n",
       "      <td>-0.464844</td>\n",
       "      <td>0.925293</td>\n",
       "      <td>-0.263794</td>\n",
       "      <td>0.36499</td>\n",
       "      <td>0.435211</td>\n",
       "      <td>0.563721</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>-0.992920</td>\n",
       "      <td>-0.175537</td>\n",
       "      <td>0.729858</td>\n",
       "      <td>0.235107</td>\n",
       "      <td>-0.244629</td>\n",
       "      <td>0.309692</td>\n",
       "      <td>0.883301</td>\n",
       "      <td>0.621094</td>\n",
       "      <td>-0.956299</td>\n",
       "      <td>0.448975</td>\n",
       "      <td>-1.060547</td>\n",
       "      <td>-0.401428</td>\n",
       "      <td>-0.236328</td>\n",
       "      <td>-0.862305</td>\n",
       "      <td>0.466370</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>0.425537</td>\n",
       "      <td>0.515320</td>\n",
       "      <td>-1.036926</td>\n",
       "      <td>-0.244751</td>\n",
       "      <td>-1.072510</td>\n",
       "      <td>0.337830</td>\n",
       "      <td>0.576355</td>\n",
       "      <td>0.708496</td>\n",
       "      <td>0.508301</td>\n",
       "      <td>0.269348</td>\n",
       "      <td>-0.207764</td>\n",
       "      <td>-0.373291</td>\n",
       "      <td>-0.470825</td>\n",
       "      <td>0.276049</td>\n",
       "      <td>0.034302</td>\n",
       "      <td>-0.428711</td>\n",
       "      <td>0.735352</td>\n",
       "      <td>0.457321</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1759</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49650</th>\n",
       "      <td>-0.258057</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>-0.115906</td>\n",
       "      <td>0.234863</td>\n",
       "      <td>-0.313477</td>\n",
       "      <td>-0.229248</td>\n",
       "      <td>-0.310059</td>\n",
       "      <td>-0.937866</td>\n",
       "      <td>0.203003</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.329590</td>\n",
       "      <td>-0.594727</td>\n",
       "      <td>-0.261230</td>\n",
       "      <td>0.403076</td>\n",
       "      <td>-0.385742</td>\n",
       "      <td>-0.096497</td>\n",
       "      <td>-0.145752</td>\n",
       "      <td>0.237793</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>-0.175148</td>\n",
       "      <td>0.281189</td>\n",
       "      <td>-0.389648</td>\n",
       "      <td>0.484558</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>-0.670044</td>\n",
       "      <td>0.123779</td>\n",
       "      <td>-0.199829</td>\n",
       "      <td>-0.436523</td>\n",
       "      <td>0.048401</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>0.318436</td>\n",
       "      <td>-0.038940</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>-0.281738</td>\n",
       "      <td>-0.228043</td>\n",
       "      <td>0.239136</td>\n",
       "      <td>-0.112305</td>\n",
       "      <td>-0.213867</td>\n",
       "      <td>0.698975</td>\n",
       "      <td>0.353271</td>\n",
       "      <td>0.328308</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>0.832031</td>\n",
       "      <td>0.217529</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>-0.202026</td>\n",
       "      <td>0.01355</td>\n",
       "      <td>0.061218</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>-0.275391</td>\n",
       "      <td>0.182007</td>\n",
       "      <td>0.280396</td>\n",
       "      <td>0.302490</td>\n",
       "      <td>-0.465576</td>\n",
       "      <td>-0.339355</td>\n",
       "      <td>-0.695312</td>\n",
       "      <td>-0.036865</td>\n",
       "      <td>0.404053</td>\n",
       "      <td>-0.281738</td>\n",
       "      <td>-0.027588</td>\n",
       "      <td>0.327637</td>\n",
       "      <td>-0.825439</td>\n",
       "      <td>0.327423</td>\n",
       "      <td>-0.248535</td>\n",
       "      <td>-0.402832</td>\n",
       "      <td>-0.592773</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>-0.565018</td>\n",
       "      <td>0.037292</td>\n",
       "      <td>-0.114990</td>\n",
       "      <td>0.431519</td>\n",
       "      <td>-0.108398</td>\n",
       "      <td>0.574432</td>\n",
       "      <td>-0.851562</td>\n",
       "      <td>-0.138763</td>\n",
       "      <td>0.311768</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>-0.187439</td>\n",
       "      <td>-0.066284</td>\n",
       "      <td>0.209839</td>\n",
       "      <td>-0.26709</td>\n",
       "      <td>-0.481445</td>\n",
       "      <td>-0.300110</td>\n",
       "      <td>-0.277466</td>\n",
       "      <td>-0.432129</td>\n",
       "      <td>-0.587402</td>\n",
       "      <td>0.077637</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.252930</td>\n",
       "      <td>0.315674</td>\n",
       "      <td>0.123535</td>\n",
       "      <td>-0.776367</td>\n",
       "      <td>-0.557098</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>0.483154</td>\n",
       "      <td>0.260620</td>\n",
       "      <td>0.376160</td>\n",
       "      <td>0.638306</td>\n",
       "      <td>0.986328</td>\n",
       "      <td>-0.373093</td>\n",
       "      <td>0.019897</td>\n",
       "      <td>0.391479</td>\n",
       "      <td>-0.322021</td>\n",
       "      <td>0.295990</td>\n",
       "      <td>-0.312866</td>\n",
       "      <td>-0.869629</td>\n",
       "      <td>-0.454102</td>\n",
       "      <td>0.456055</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>0.426880</td>\n",
       "      <td>-0.137451</td>\n",
       "      <td>0.107422</td>\n",
       "      <td>0.191406</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>-0.028809</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.105957</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>0.493164</td>\n",
       "      <td>-0.234985</td>\n",
       "      <td>0.404297</td>\n",
       "      <td>-0.385742</td>\n",
       "      <td>0.074463</td>\n",
       "      <td>1.004395</td>\n",
       "      <td>-0.280029</td>\n",
       "      <td>-0.436523</td>\n",
       "      <td>-0.438904</td>\n",
       "      <td>0.290527</td>\n",
       "      <td>0.266602</td>\n",
       "      <td>-0.253540</td>\n",
       "      <td>-0.022827</td>\n",
       "      <td>0.304382</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>-0.058228</td>\n",
       "      <td>0.016541</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>-0.156265</td>\n",
       "      <td>-0.451172</td>\n",
       "      <td>0.034912</td>\n",
       "      <td>-0.089355</td>\n",
       "      <td>-0.754395</td>\n",
       "      <td>-0.276855</td>\n",
       "      <td>0.501465</td>\n",
       "      <td>-0.333496</td>\n",
       "      <td>0.098755</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>-0.033569</td>\n",
       "      <td>0.116943</td>\n",
       "      <td>-0.671875</td>\n",
       "      <td>1.583504</td>\n",
       "      <td>-0.754883</td>\n",
       "      <td>-0.025146</td>\n",
       "      <td>0.285645</td>\n",
       "      <td>-0.026917</td>\n",
       "      <td>-0.535889</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>0.599854</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>-0.144287</td>\n",
       "      <td>0.193726</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.182922</td>\n",
       "      <td>-0.533951</td>\n",
       "      <td>-0.299133</td>\n",
       "      <td>0.013428</td>\n",
       "      <td>0.681641</td>\n",
       "      <td>-0.178619</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>0.606445</td>\n",
       "      <td>-0.443573</td>\n",
       "      <td>0.505371</td>\n",
       "      <td>-0.325684</td>\n",
       "      <td>-0.482910</td>\n",
       "      <td>-0.018433</td>\n",
       "      <td>0.029480</td>\n",
       "      <td>1.098633</td>\n",
       "      <td>-0.079346</td>\n",
       "      <td>-0.238770</td>\n",
       "      <td>0.270264</td>\n",
       "      <td>-0.903320</td>\n",
       "      <td>-0.242188</td>\n",
       "      <td>-0.070435</td>\n",
       "      <td>-1.223145</td>\n",
       "      <td>-0.221191</td>\n",
       "      <td>-0.625732</td>\n",
       "      <td>-0.186142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273682</td>\n",
       "      <td>-0.068115</td>\n",
       "      <td>0.105553</td>\n",
       "      <td>0.395508</td>\n",
       "      <td>0.645020</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.313721</td>\n",
       "      <td>-0.559570</td>\n",
       "      <td>0.174561</td>\n",
       "      <td>-0.647705</td>\n",
       "      <td>-0.745117</td>\n",
       "      <td>0.041260</td>\n",
       "      <td>0.552856</td>\n",
       "      <td>0.130981</td>\n",
       "      <td>-0.513550</td>\n",
       "      <td>-0.107666</td>\n",
       "      <td>0.055573</td>\n",
       "      <td>-0.132080</td>\n",
       "      <td>-1.150879</td>\n",
       "      <td>-0.062378</td>\n",
       "      <td>0.181885</td>\n",
       "      <td>-0.403442</td>\n",
       "      <td>-0.019287</td>\n",
       "      <td>-0.468384</td>\n",
       "      <td>0.393311</td>\n",
       "      <td>-0.334717</td>\n",
       "      <td>0.044189</td>\n",
       "      <td>0.203369</td>\n",
       "      <td>-0.232910</td>\n",
       "      <td>-0.618652</td>\n",
       "      <td>-0.156982</td>\n",
       "      <td>0.359863</td>\n",
       "      <td>-0.232666</td>\n",
       "      <td>0.709473</td>\n",
       "      <td>-0.321289</td>\n",
       "      <td>-0.165527</td>\n",
       "      <td>-0.463867</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.094757</td>\n",
       "      <td>0.157959</td>\n",
       "      <td>0.127258</td>\n",
       "      <td>-0.415039</td>\n",
       "      <td>0.185791</td>\n",
       "      <td>-0.010254</td>\n",
       "      <td>-0.418457</td>\n",
       "      <td>-0.280029</td>\n",
       "      <td>0.126465</td>\n",
       "      <td>-0.240967</td>\n",
       "      <td>0.640564</td>\n",
       "      <td>-0.163635</td>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.456543</td>\n",
       "      <td>-0.292938</td>\n",
       "      <td>0.212646</td>\n",
       "      <td>-0.594727</td>\n",
       "      <td>1.074707</td>\n",
       "      <td>0.055054</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>-0.213867</td>\n",
       "      <td>-0.138794</td>\n",
       "      <td>0.852051</td>\n",
       "      <td>-0.023926</td>\n",
       "      <td>0.450684</td>\n",
       "      <td>0.403076</td>\n",
       "      <td>0.361084</td>\n",
       "      <td>-0.089752</td>\n",
       "      <td>0.354492</td>\n",
       "      <td>-0.213623</td>\n",
       "      <td>0.581955</td>\n",
       "      <td>-0.482666</td>\n",
       "      <td>-0.010254</td>\n",
       "      <td>-0.24292</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>-0.127441</td>\n",
       "      <td>0.358154</td>\n",
       "      <td>0.014893</td>\n",
       "      <td>-0.362549</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>-0.917969</td>\n",
       "      <td>-0.123184</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.146729</td>\n",
       "      <td>-0.232422</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.103760</td>\n",
       "      <td>-0.323242</td>\n",
       "      <td>-0.191406</td>\n",
       "      <td>-0.320312</td>\n",
       "      <td>0.141617</td>\n",
       "      <td>-0.324951</td>\n",
       "      <td>-0.458740</td>\n",
       "      <td>0.090210</td>\n",
       "      <td>0.647705</td>\n",
       "      <td>-0.433105</td>\n",
       "      <td>-0.286011</td>\n",
       "      <td>-0.431885</td>\n",
       "      <td>-0.086792</td>\n",
       "      <td>0.141357</td>\n",
       "      <td>0.180603</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>0.161865</td>\n",
       "      <td>-0.108765</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>0.325134</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>-0.435913</td>\n",
       "      <td>0.135132</td>\n",
       "      <td>0.115479</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>25</td>\n",
       "      <td>0.5175</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6         7         8         9        10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46       47        48        49        50        51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72        73        74        75        76        77        78        79        80       81        82        83        84        85        86        87        88        89        90        91        92        93        94        95        96        97        98  \\\n",
       "49649 -0.492432  0.599487 -0.093018  0.693359 -0.435669  0.422607  0.255676 -0.246155  0.072754 -0.123047  0.260254 -1.105225 -0.861328  0.712006 -0.057861  0.604210 -0.070557  0.921875 -0.253174 -0.049194  0.509827 -0.094482  0.330078 -0.333115 -0.522522 -1.142212  0.393860 -0.221680  0.310547 -0.728760 -0.543945 -0.105408 -0.695801  0.081055  0.059509 -0.523254 -0.332424  0.226318  0.164551  1.169922 -0.001709 -0.466415  0.883545 -0.151978  0.344681 -0.574351 -0.012207  0.78125 -1.159424  0.613647 -0.368536  0.491577 -0.683350  0.499237  0.472900 -0.878418 -0.342377  0.133621  0.046997 -1.108765  0.212402  0.634277  0.986328 -0.915161 -0.344727  0.647949  0.238525  0.470703 -0.391079  0.539062 -0.365234 -0.641724  0.220276 -0.104639 -0.633545 -0.388855 -0.221191 -0.955444 -1.069092  0.416504  0.083984 -0.89978  0.149170  0.351822 -0.302734 -0.247536 -1.155273  0.153753  0.148926 -0.778564  0.809753 -0.926025 -0.226562 -0.794434  0.538044  0.575439  0.972168  0.004501  1.219727   \n",
       "49650 -0.258057  0.055664 -0.115906  0.234863 -0.313477 -0.229248 -0.310059 -0.937866  0.203003  0.082031  0.329590 -0.594727 -0.261230  0.403076 -0.385742 -0.096497 -0.145752  0.237793  0.144287 -0.175148  0.281189 -0.389648  0.484558  0.013672 -0.670044  0.123779 -0.199829 -0.436523  0.048401  0.183594  0.318436 -0.038940 -0.228760 -0.281738 -0.228043  0.239136 -0.112305 -0.213867  0.698975  0.353271  0.328308 -0.015869  0.832031  0.217529 -0.078613 -0.142578 -0.202026  0.01355  0.061218  0.618652 -0.275391  0.182007  0.280396  0.302490 -0.465576 -0.339355 -0.695312 -0.036865  0.404053 -0.281738 -0.027588  0.327637 -0.825439  0.327423 -0.248535 -0.402832 -0.592773 -0.267578 -0.565018  0.037292 -0.114990  0.431519 -0.108398  0.574432 -0.851562 -0.138763  0.311768  0.054688 -0.187439 -0.066284  0.209839 -0.26709 -0.481445 -0.300110 -0.277466 -0.432129 -0.587402  0.077637  0.050781  0.252930  0.315674  0.123535 -0.776367 -0.557098  0.103516  0.483154  0.260620  0.376160  0.638306   \n",
       "\n",
       "             99       100       101       102       103       104       105       106       107       108       109       110       111       112       113       114       115       116       117       118       119       120       121       122       123       124       125       126       127       128       129       130       131       132       133       134       135       136       137       138       139       140       141       142       143       144       145       146       147       148       149       150       151       152       153       154       155       156       157       158       159       160       161       162       163       164       165       166       167       168       169       170       171       172       173       174       175       176       177       178       179       180       181       182       183       184       185       186       187       188       189       190       191       192       193       194       195       196       197  \\\n",
       "49649  0.657959  0.019409  1.007690 -0.352432  0.953613  0.660156 -0.411011  0.151123 -0.057129  0.549316 -0.941864 -0.280273  0.674316  0.817871 -0.033081  0.217041 -0.405029 -0.095459 -0.589111  0.143066  0.345703 -1.480469  0.490479 -0.330811 -0.719460  0.008667 -0.819824  0.159012  0.303375 -0.033234  0.617432 -0.545166  0.055664 -0.759766 -0.610535 -1.591797 -1.320068 -0.268829  0.932373 -0.601868  0.573608  0.033691  0.267883  0.271973  0.112793  0.429688 -0.249573 -0.525879  0.053955 -1.141113 -0.465973  1.397949 -0.469971 -0.290771 -0.273438 -0.590820 -0.065552  0.558838 -1.258057 -0.475342 -1.066162  0.994263  0.515625  1.062988 -0.077209 -0.551392 -0.672363  0.343262  0.047791 -0.126709 -0.347717 -0.871460  0.351929 -0.394928 -0.150787 -0.191040 -0.317139  0.836761 -0.863525 -0.372559 -0.375732 -0.088928  0.257446  0.235229 -0.357178  0.405518  0.442749 -0.997925  0.037109  0.151611  0.729248  0.102905  0.770508  0.212646  0.300171  0.185791  0.425049  0.461548 -0.781372   \n",
       "49650  0.986328 -0.373093  0.019897  0.391479 -0.322021  0.295990 -0.312866 -0.869629 -0.454102  0.456055 -0.264496  0.426880 -0.137451  0.107422  0.191406  0.108398 -0.028809  0.057617  0.105957  0.108398  0.493164 -0.234985  0.404297 -0.385742  0.074463  1.004395 -0.280029 -0.436523 -0.438904  0.290527  0.266602 -0.253540 -0.022827  0.304382  0.025391 -0.058228  0.016541  0.055664 -0.156265 -0.451172  0.034912 -0.089355 -0.754395 -0.276855  0.501465 -0.333496  0.098755  0.018555 -0.033569  0.116943 -0.671875  1.583504 -0.754883 -0.025146  0.285645 -0.026917 -0.535889  0.098206  0.599854 -0.015625 -0.144287  0.193726  0.200195  0.182922 -0.533951 -0.299133  0.013428  0.681641 -0.178619  0.488281  0.606445 -0.443573  0.505371 -0.325684 -0.482910 -0.018433  0.029480  1.098633 -0.079346 -0.238770  0.270264 -0.903320 -0.242188 -0.070435 -1.223145 -0.221191 -0.625732 -0.186142  0.000000  0.273682 -0.068115  0.105553  0.395508  0.645020 -0.189453  0.056641  0.313721 -0.559570  0.174561   \n",
       "\n",
       "            198       199       200       201       202       203       204       205       206       207       208       209       210       211       212       213       214       215       216       217       218       219       220       221       222       223       224       225       226       227       228       229       230       231       232       233       234       235       236       237       238       239       240       241       242       243       244       245       246       247       248       249       250       251       252       253       254       255       256       257       258       259       260      261       262       263       264       265       266       267       268       269       270       271       272       273       274       275       276       277       278       279       280       281       282       283       284       285       286       287       288       289       290       291       292       293       294       295       296  \\\n",
       "49649 -1.406250 -0.413208  0.093262  0.378479 -0.932495 -0.204803  0.772461 -0.195068  0.344238 -0.710205  0.108398  0.614014 -0.857391  0.180359 -0.596069  0.726891  0.012939 -0.928711  0.442383  0.057129 -0.417480 -0.898407  0.023597 -0.171570  0.528809 -0.144043  0.058113 -0.012451 -0.117798 -0.091248 -0.506897  0.209167  0.256836 -0.285400 -0.053955  0.405945 -0.092773 -0.511871  0.540527  0.442810  1.110840 -0.280029  0.704468 -0.080627  0.484619 -0.789856 -0.094971  0.971191 -0.034668  0.125916  0.033936 -1.453369  0.509521  0.103210  0.616211  0.146118 -0.011353 -0.479004  0.070801 -0.363647 -0.464844  0.925293 -0.263794  0.36499  0.435211  0.563721  0.004272 -0.992920 -0.175537  0.729858  0.235107 -0.244629  0.309692  0.883301  0.621094 -0.956299  0.448975 -1.060547 -0.401428 -0.236328 -0.862305  0.466370  0.064941 -0.168457  0.425537  0.515320 -1.036926 -0.244751 -1.072510  0.337830  0.576355  0.708496  0.508301  0.269348 -0.207764 -0.373291 -0.470825  0.276049  0.034302   \n",
       "49650 -0.647705 -0.745117  0.041260  0.552856  0.130981 -0.513550 -0.107666  0.055573 -0.132080 -1.150879 -0.062378  0.181885 -0.403442 -0.019287 -0.468384  0.393311 -0.334717  0.044189  0.203369 -0.232910 -0.618652 -0.156982  0.359863 -0.232666  0.709473 -0.321289 -0.165527 -0.463867  0.076904 -0.081543  0.094757  0.157959  0.127258 -0.415039  0.185791 -0.010254 -0.418457 -0.280029  0.126465 -0.240967  0.640564 -0.163635 -0.126953  0.456543 -0.292938  0.212646 -0.594727  1.074707  0.055054  0.012695 -0.213867 -0.138794  0.852051 -0.023926  0.450684  0.403076  0.361084 -0.089752  0.354492 -0.213623  0.581955 -0.482666 -0.010254 -0.24292 -0.009033  0.281250 -0.127441  0.358154  0.014893 -0.362549 -0.072266 -0.917969 -0.123184  0.562500 -0.146729 -0.232422  0.429688  0.103760 -0.323242 -0.191406 -0.320312  0.141617 -0.324951 -0.458740  0.090210  0.647705 -0.433105 -0.286011 -0.431885 -0.086792  0.141357  0.180603 -0.018555  0.161865 -0.108765 -0.041992  0.325134  0.161133 -0.435913   \n",
       "\n",
       "            297       298       299  Label       Date  Top  topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  topic_7  topic_8  topic_9  \n",
       "49649 -0.428711  0.735352  0.457321      0 2008-08-08   24   0.0295   0.0222   0.0178   0.0149   0.0128   0.0113     0.01   0.1759    0.698      0.0  \n",
       "49650  0.135132  0.115479  0.136719      0 2008-08-08   25   0.5175   0.0141   0.0113   0.0000   0.1980   0.2274     0.00   0.0000    0.000      0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "#Importar los datasets\n",
    "url_embeddings_average_individual = zipfile.ZipFile('../Data/sum_topics.zip')\n",
    "\n",
    "embeddings_average_individual = pd.read_csv(url_embeddings_average_individual.open('sum_topics.csv'))\n",
    "\n",
    "\n",
    "embeddings_average_individual['Date'] =  pd.to_datetime(embeddings_average_individual['Date'], format='%Y-%m-%d')\n",
    "\n",
    "embeddings_average_individual.reset_index()\n",
    "embeddings_average_individual.fillna(0, inplace=True)\n",
    "embeddings_average_individual.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAXOJcEcbmed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono la fecha para la cual hago el corte de train y test\n",
    "training_end = pd.to_datetime(\"2013-12-31\")\n",
    "num_training = len(embeddings_average_individual[(embeddings_average_individual[\"Date\"]) <= training_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418076004775169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>Top</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9443</th>\n",
       "      <td>-0.414062</td>\n",
       "      <td>-0.006348</td>\n",
       "      <td>-0.658142</td>\n",
       "      <td>0.591064</td>\n",
       "      <td>-0.103027</td>\n",
       "      <td>-0.636108</td>\n",
       "      <td>0.143799</td>\n",
       "      <td>-0.545385</td>\n",
       "      <td>0.495117</td>\n",
       "      <td>0.868652</td>\n",
       "      <td>0.374756</td>\n",
       "      <td>-0.693848</td>\n",
       "      <td>-0.126572</td>\n",
       "      <td>-0.297241</td>\n",
       "      <td>-0.302734</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>-0.122284</td>\n",
       "      <td>0.49115</td>\n",
       "      <td>-0.153564</td>\n",
       "      <td>-0.068848</td>\n",
       "      <td>-0.349243</td>\n",
       "      <td>-0.21582</td>\n",
       "      <td>-0.74231</td>\n",
       "      <td>-0.924805</td>\n",
       "      <td>0.099846</td>\n",
       "      <td>-0.367065</td>\n",
       "      <td>-0.512451</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>0.49675</td>\n",
       "      <td>-0.347755</td>\n",
       "      <td>-0.48877</td>\n",
       "      <td>0.314697</td>\n",
       "      <td>-0.244385</td>\n",
       "      <td>-0.281799</td>\n",
       "      <td>-0.506836</td>\n",
       "      <td>0.142334</td>\n",
       "      <td>-0.194809</td>\n",
       "      <td>0.20752</td>\n",
       "      <td>0.243652</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>-0.213608</td>\n",
       "      <td>-0.341537</td>\n",
       "      <td>0.331818</td>\n",
       "      <td>-0.067871</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>-0.682373</td>\n",
       "      <td>-0.365723</td>\n",
       "      <td>-0.066406</td>\n",
       "      <td>0.358643</td>\n",
       "      <td>0.351807</td>\n",
       "      <td>-0.117188</td>\n",
       "      <td>0.37207</td>\n",
       "      <td>0.265381</td>\n",
       "      <td>-0.429077</td>\n",
       "      <td>-0.137939</td>\n",
       "      <td>-0.450928</td>\n",
       "      <td>0.133118</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.156219</td>\n",
       "      <td>-0.109497</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>-0.022705</td>\n",
       "      <td>-0.417908</td>\n",
       "      <td>-0.169189</td>\n",
       "      <td>0.049438</td>\n",
       "      <td>-0.525635</td>\n",
       "      <td>-0.460419</td>\n",
       "      <td>0.049866</td>\n",
       "      <td>0.150635</td>\n",
       "      <td>0.080322</td>\n",
       "      <td>-0.030396</td>\n",
       "      <td>-0.021729</td>\n",
       "      <td>0.157593</td>\n",
       "      <td>-0.223633</td>\n",
       "      <td>-1.036133</td>\n",
       "      <td>-0.653809</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>-0.060547</td>\n",
       "      <td>0.035095</td>\n",
       "      <td>-0.402344</td>\n",
       "      <td>0.231689</td>\n",
       "      <td>-0.289612</td>\n",
       "      <td>0.120026</td>\n",
       "      <td>-0.40918</td>\n",
       "      <td>-0.146851</td>\n",
       "      <td>0.306885</td>\n",
       "      <td>-0.096558</td>\n",
       "      <td>0.358215</td>\n",
       "      <td>0.116455</td>\n",
       "      <td>0.228027</td>\n",
       "      <td>-0.61377</td>\n",
       "      <td>-0.193115</td>\n",
       "      <td>-0.411133</td>\n",
       "      <td>-0.213379</td>\n",
       "      <td>-0.114746</td>\n",
       "      <td>0.188599</td>\n",
       "      <td>0.197998</td>\n",
       "      <td>-0.073486</td>\n",
       "      <td>0.683838</td>\n",
       "      <td>-0.172363</td>\n",
       "      <td>-0.041016</td>\n",
       "      <td>-0.573608</td>\n",
       "      <td>0.504395</td>\n",
       "      <td>0.211914</td>\n",
       "      <td>-0.162354</td>\n",
       "      <td>0.30603</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.170898</td>\n",
       "      <td>0.420959</td>\n",
       "      <td>-0.34317</td>\n",
       "      <td>-0.181152</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.48082</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>-0.721191</td>\n",
       "      <td>-0.030029</td>\n",
       "      <td>0.185043</td>\n",
       "      <td>0.528809</td>\n",
       "      <td>0.291992</td>\n",
       "      <td>0.503784</td>\n",
       "      <td>-0.742676</td>\n",
       "      <td>-0.444824</td>\n",
       "      <td>-0.24707</td>\n",
       "      <td>0.33374</td>\n",
       "      <td>-0.035797</td>\n",
       "      <td>-0.171753</td>\n",
       "      <td>-0.677246</td>\n",
       "      <td>-0.61499</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.260742</td>\n",
       "      <td>-0.167786</td>\n",
       "      <td>-0.194824</td>\n",
       "      <td>-0.513184</td>\n",
       "      <td>0.361633</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.396118</td>\n",
       "      <td>0.56665</td>\n",
       "      <td>-0.304199</td>\n",
       "      <td>-0.401611</td>\n",
       "      <td>0.452225</td>\n",
       "      <td>-0.276642</td>\n",
       "      <td>-0.838867</td>\n",
       "      <td>-0.537598</td>\n",
       "      <td>0.104797</td>\n",
       "      <td>-0.045776</td>\n",
       "      <td>0.562012</td>\n",
       "      <td>-0.362854</td>\n",
       "      <td>-0.109131</td>\n",
       "      <td>-0.427734</td>\n",
       "      <td>-0.440674</td>\n",
       "      <td>0.237793</td>\n",
       "      <td>0.057251</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0.299011</td>\n",
       "      <td>-0.286743</td>\n",
       "      <td>-0.726807</td>\n",
       "      <td>0.112427</td>\n",
       "      <td>-0.127563</td>\n",
       "      <td>-0.206787</td>\n",
       "      <td>-0.080811</td>\n",
       "      <td>-0.570801</td>\n",
       "      <td>-0.179321</td>\n",
       "      <td>-0.192383</td>\n",
       "      <td>0.131897</td>\n",
       "      <td>-0.119019</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>0.565674</td>\n",
       "      <td>-0.371094</td>\n",
       "      <td>-0.144043</td>\n",
       "      <td>-0.60498</td>\n",
       "      <td>-0.501953</td>\n",
       "      <td>-0.079529</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>-0.310852</td>\n",
       "      <td>-0.274902</td>\n",
       "      <td>0.186768</td>\n",
       "      <td>-0.002441</td>\n",
       "      <td>-0.156967</td>\n",
       "      <td>-0.385742</td>\n",
       "      <td>0.306641</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.262329</td>\n",
       "      <td>-0.338379</td>\n",
       "      <td>0.130371</td>\n",
       "      <td>0.160645</td>\n",
       "      <td>0.204346</td>\n",
       "      <td>-0.092529</td>\n",
       "      <td>0.104492</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>-0.153397</td>\n",
       "      <td>0.031677</td>\n",
       "      <td>-0.549194</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>0.195557</td>\n",
       "      <td>-0.581543</td>\n",
       "      <td>1.931641</td>\n",
       "      <td>-0.022217</td>\n",
       "      <td>0.245361</td>\n",
       "      <td>-0.24292</td>\n",
       "      <td>-1.124756</td>\n",
       "      <td>0.058044</td>\n",
       "      <td>0.23848</td>\n",
       "      <td>-0.640137</td>\n",
       "      <td>-0.992188</td>\n",
       "      <td>0.419434</td>\n",
       "      <td>0.507011</td>\n",
       "      <td>0.213318</td>\n",
       "      <td>0.111694</td>\n",
       "      <td>-0.348694</td>\n",
       "      <td>-0.495483</td>\n",
       "      <td>-0.67981</td>\n",
       "      <td>0.643921</td>\n",
       "      <td>-1.069824</td>\n",
       "      <td>0.618263</td>\n",
       "      <td>-0.6521</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.918945</td>\n",
       "      <td>0.277588</td>\n",
       "      <td>-0.323242</td>\n",
       "      <td>0.42276</td>\n",
       "      <td>-0.108826</td>\n",
       "      <td>-0.62207</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.128052</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.029053</td>\n",
       "      <td>-0.512634</td>\n",
       "      <td>0.399902</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0.43811</td>\n",
       "      <td>0.553818</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>-0.586914</td>\n",
       "      <td>-0.579834</td>\n",
       "      <td>0.151703</td>\n",
       "      <td>0.275513</td>\n",
       "      <td>0.43335</td>\n",
       "      <td>0.116516</td>\n",
       "      <td>0.621368</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.289185</td>\n",
       "      <td>-0.162201</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>-0.993164</td>\n",
       "      <td>-0.434937</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.22876</td>\n",
       "      <td>-0.136292</td>\n",
       "      <td>0.04541</td>\n",
       "      <td>-0.318115</td>\n",
       "      <td>-0.021729</td>\n",
       "      <td>0.220459</td>\n",
       "      <td>0.481445</td>\n",
       "      <td>0.604004</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>-1.235352</td>\n",
       "      <td>-0.437988</td>\n",
       "      <td>-0.087646</td>\n",
       "      <td>0.066284</td>\n",
       "      <td>-0.027008</td>\n",
       "      <td>0.597168</td>\n",
       "      <td>0.156006</td>\n",
       "      <td>0.050171</td>\n",
       "      <td>0.951904</td>\n",
       "      <td>-0.282959</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>-0.397812</td>\n",
       "      <td>-0.916504</td>\n",
       "      <td>-0.266693</td>\n",
       "      <td>-0.746643</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.55542</td>\n",
       "      <td>-0.49585</td>\n",
       "      <td>0.096924</td>\n",
       "      <td>-0.25354</td>\n",
       "      <td>0.218994</td>\n",
       "      <td>-0.921387</td>\n",
       "      <td>-0.67749</td>\n",
       "      <td>0.320801</td>\n",
       "      <td>0.324951</td>\n",
       "      <td>0.417236</td>\n",
       "      <td>0.280579</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>0.679016</td>\n",
       "      <td>0.473511</td>\n",
       "      <td>-0.006714</td>\n",
       "      <td>-0.256409</td>\n",
       "      <td>0.536255</td>\n",
       "      <td>-0.487305</td>\n",
       "      <td>0.786133</td>\n",
       "      <td>0.134155</td>\n",
       "      <td>-0.624756</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.336426</td>\n",
       "      <td>0.259277</td>\n",
       "      <td>-0.118896</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.100067</td>\n",
       "      <td>0.601562</td>\n",
       "      <td>0.181763</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9113</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6         7         8         9        10        11        12        13        14        15        16       17        18        19        20       21       22        23        24        25        26        27       28        29       30        31        32        33        34        35        36       37        38        39        40        41        42        43        44        45        46        47        48        49        50       51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72        73        74        75        76        77        78        79        80        81        82       83        84        85        86        87        88        89       90        91        92        93        94        95        96        97        98        99  \\\n",
       "9443 -0.414062 -0.006348 -0.658142  0.591064 -0.103027 -0.636108  0.143799 -0.545385  0.495117  0.868652  0.374756 -0.693848 -0.126572 -0.297241 -0.302734  0.503906 -0.122284  0.49115 -0.153564 -0.068848 -0.349243 -0.21582 -0.74231 -0.924805  0.099846 -0.367065 -0.512451  0.687988  0.49675 -0.347755 -0.48877  0.314697 -0.244385 -0.281799 -0.506836  0.142334 -0.194809  0.20752  0.243652  0.040039 -0.213608 -0.341537  0.331818 -0.067871 -0.065918 -0.682373 -0.365723 -0.066406  0.358643  0.351807 -0.117188  0.37207  0.265381 -0.429077 -0.137939 -0.450928  0.133118  0.085449  0.156219 -0.109497  0.001923 -0.022705 -0.417908 -0.169189  0.049438 -0.525635 -0.460419  0.049866  0.150635  0.080322 -0.030396 -0.021729  0.157593 -0.223633 -1.036133 -0.653809  0.386719 -0.060547  0.035095 -0.402344  0.231689 -0.289612  0.120026 -0.40918 -0.146851  0.306885 -0.096558  0.358215  0.116455  0.228027 -0.61377 -0.193115 -0.411133 -0.213379 -0.114746  0.188599  0.197998 -0.073486  0.683838 -0.172363   \n",
       "\n",
       "           100       101       102       103       104      105       106       107       108      109       110       111      112       113       114       115       116       117       118       119       120       121      122      123       124       125       126      127       128       129       130       131       132       133       134       135      136       137       138       139       140       141       142       143       144       145       146       147       148       149       150       151    152       153       154       155       156       157       158       159       160       161       162       163       164       165       166       167       168      169       170       171       172       173       174       175       176       177       178       179       180       181       182       183       184       185       186       187       188       189       190       191       192       193       194       195       196       197      198       199  \\\n",
       "9443 -0.041016 -0.573608  0.504395  0.211914 -0.162354  0.30603  0.001465  0.170898  0.420959 -0.34317 -0.181152  0.071289 -0.48082  0.001099 -0.721191 -0.030029  0.185043  0.528809  0.291992  0.503784 -0.742676 -0.444824 -0.24707  0.33374 -0.035797 -0.171753 -0.677246 -0.61499  0.263672  0.260742 -0.167786 -0.194824 -0.513184  0.361633  0.079102 -0.396118  0.56665 -0.304199 -0.401611  0.452225 -0.276642 -0.838867 -0.537598  0.104797 -0.045776  0.562012 -0.362854 -0.109131 -0.427734 -0.440674  0.237793  0.057251 -0.625  0.299011 -0.286743 -0.726807  0.112427 -0.127563 -0.206787 -0.080811 -0.570801 -0.179321 -0.192383  0.131897 -0.119019 -0.262329  0.565674 -0.371094 -0.144043 -0.60498 -0.501953 -0.079529  0.210938 -0.310852 -0.274902  0.186768 -0.002441 -0.156967 -0.385742  0.306641  0.006348 -0.262329 -0.338379  0.130371  0.160645  0.204346 -0.092529  0.104492  0.511719 -0.153397  0.031677 -0.549194 -0.080078  0.195557 -0.581543  1.931641 -0.022217  0.245361 -0.24292 -1.124756   \n",
       "\n",
       "           200      201       202       203       204       205       206       207       208       209      210       211       212       213     214       215       216       217       218      219       220      221     222       223       224       225       226       227       228      229       230       231       232       233       234       235      236       237       238       239       240       241       242       243       244       245      246       247      248       249       250       251       252       253       254       255       256       257       258       259       260       261       262       263       264       265       266       267       268       269       270      271      272       273      274       275       276      277       278       279       280       281       282       283       284       285       286       287       288       289       290       291       292       293       294       295       296       297       298       299  Label  \\\n",
       "9443  0.058044  0.23848 -0.640137 -0.992188  0.419434  0.507011  0.213318  0.111694 -0.348694 -0.495483 -0.67981  0.643921 -1.069824  0.618263 -0.6521 -0.007568  0.918945  0.277588 -0.323242  0.42276 -0.108826 -0.62207  0.2258  0.128052 -0.058838 -0.029053 -0.512634  0.399902  0.194336  0.43811  0.553818 -0.179688 -0.586914 -0.579834  0.151703  0.275513  0.43335  0.116516  0.621368  0.091797  0.289185 -0.162201 -0.308594 -0.993164 -0.434937  0.127319  0.22876 -0.136292  0.04541 -0.318115 -0.021729  0.220459  0.481445  0.604004 -0.044678 -1.235352 -0.437988 -0.087646  0.066284 -0.027008  0.597168  0.156006  0.050171  0.951904 -0.282959  0.050781 -0.397812 -0.916504 -0.266693 -0.746643  0.036865  0.55542 -0.49585  0.096924 -0.25354  0.218994 -0.921387 -0.67749  0.320801  0.324951  0.417236  0.280579  0.020386  0.679016  0.473511 -0.006714 -0.256409  0.536255 -0.487305  0.786133  0.134155 -0.624756  0.000488  0.336426  0.259277 -0.118896  0.110352  0.100067  0.601562  0.181763      0   \n",
       "\n",
       "           Date  Top  topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  topic_7  topic_8  topic_9  \n",
       "9443 2014-12-31    1   0.9113   0.0174   0.0139   0.0116      0.0      0.0      0.0      0.0      0.0      0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono el archivo con el que se corre el modelo\n",
    "data = embeddings_average_individual[embeddings_average_individual['Date']<='2014-12-31']\n",
    "print(data['Label'].mean())\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6294, 1, 310)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 70.6 ms\n"
     ]
    }
   ],
   "source": [
    "training = data[:num_training]\n",
    "testing = data[num_training:]\n",
    "\n",
    "# Se separa en train y test\n",
    "x_train = data.drop([\"Top\",\"Label\", \"Date\"], axis=1)[:num_training]\n",
    "x_test = data.drop([\"Top\",'Label', 'Date'], axis=1)[num_training:]\n",
    "y_train = data[\"Label\"].values[:num_training]\n",
    "y_test = data[\"Label\"].values[num_training:]\n",
    "\n",
    "\n",
    "x_train_array = x_train.to_numpy()\n",
    "reshape_x_train = x_train_array.reshape(len(x_train), 1, x_train.shape[1])\n",
    "reshape_x_train.shape\n",
    "\n",
    "x_test_array = x_test.to_numpy()\n",
    "reshape_x_test = x_test_array.reshape(len(x_test), 1, x_train.shape[1])\n",
    "reshape_x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir espacio de busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.76 ms\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'units1': hp.choice('units1', [8, 16, 32, 64, 128, 256, 512]),\n",
    "    'units2': hp.choice('units2', [8, 16, 32, 64, 128, 256, 512]),\n",
    "    'units3': hp.choice('units3', [8, 16, 32, 64, 128, 256, 512]),\n",
    "                 \n",
    "    'dropout1': hp.choice('dropout1', [0.1,0.2,0.3, 0.4]),\n",
    "    'dropout2': hp.choice('dropout2', [0.1,0.2,0.3, 0.4]),\n",
    "\n",
    "    'batch_size' : hp.choice('batch_size', [128,256,512]),\n",
    "    \n",
    "    'nb_epochs' : hp.choice('nb_epochs', [50]),\n",
    "\n",
    "    'optimizer':  hp.choice('optimizer', [ 'adam','adadelta']),   \n",
    "    'activation': hp.choice('activation', [ 'relu','sigmoid','softmax']), \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir busqueda bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.75 ms\n"
     ]
    }
   ],
   "source": [
    "#Objective function that hyperopt will minimize\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def objective(params):\n",
    "    \n",
    "#     import ml_metrics\n",
    "\n",
    "    \n",
    "    start = timer()\n",
    "    print ('Params testing: ', params)\n",
    "    print ('\\n ')\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(CuDNNGRU(128, return_sequences = True)))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "    model.add(Bidirectional(CuDNNGRU(params['units1'])))\n",
    "    model.add(Dropout(params['dropout2']))\n",
    "    model.add(Dense(params['units2'], activation='relu'))\n",
    "    model.add(Dense(params['units3'], activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(params['optimizer'], loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    logdir = \"Resultados\\\\\" + exp_name +\"\\\\logs\\\\model\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #includes the call back object\n",
    "    model.fit(reshape_x_train, y_train, epochs=params['nb_epochs'], batch_size=params['batch_size'],\n",
    "                verbose = 0, validation_data=(reshape_x_test, y_test))\n",
    "     \n",
    "    #predict the test set \n",
    "    ypred = model.predict_proba(reshape_x_test)\n",
    "    testing_cp = testing.copy()\n",
    "    testing_cp['Prob'] = ypred\n",
    "    testing_cp['Prob_dia'] = testing_cp['Prob'].groupby(testing_cp['Date']).transform('mean')\n",
    "    testing_cp['Prediction'] = 0\n",
    "    testing_cp.loc[testing_cp['Prob_dia']>0.5, 'Prediction'] = 1\n",
    "    testing_cp.drop_duplicates(subset=['Date','Prediction','Label'], inplace=True)\n",
    "    \n",
    "    acc = accuracy_score(testing_cp.Label, testing_cp.Prediction)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([-acc, params, run_time])\n",
    "    of_connection.close()\n",
    "    \n",
    "    \n",
    "    print('Test accuracy:', acc)\n",
    " \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'train_time': run_time,}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenar resultados de cada iteracin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.19 ms\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import tpe\n",
    "\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.82 ms\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from hyperopt import Trials\n",
    "\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# File to save first results\n",
    "out_file = folder + '/gbm_results.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanzar optimizacin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params testing:                                      \n",
      "{'activation': 'softmax', 'batch_size': 512, 'dropout1': 0.3, 'dropout2': 0.2, 'nb_epochs': 50, 'optimizer': 'adadelta', 'units1': 32, 'units2': 256, 'units3': 32}\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: No OpKernel was registered to support Op 'CudnnRNNV2' used by {{node sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2}} with these attrs: [dropout=0, seed=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"gru\", is_training=true, seed2=0]\n",
      "Registered devices: [CPU, XLA_CPU]\n",
      "Registered kernels:\n",
      "  device='GPU'; T in [DT_DOUBLE]\n",
      "  device='GPU'; T in [DT_FLOAT]\n",
      "  device='GPU'; T in [DT_HALF]\n",
      "\n",
      "\t [[sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2]] [Op:__inference_train_function_6480]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "No OpKernel was registered to support Op 'CudnnRNNV2' used by {{node sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2}} with these attrs: [dropout=0, seed=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"gru\", is_training=true, seed2=0]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2]] [Op:__inference_train_function_6480]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c64c5ec5a626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m best = fmin(fn = objective, space = space, algo = tpe.suggest, \n\u001b[1;32m      3\u001b[0m             \u001b[0mmax_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayes_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             verbose = 1, rstate= np.random.RandomState(50))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         )\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         )\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-14eff3e5c027>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#includes the call back object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     model.fit(reshape_x_train, y_train, epochs=params['nb_epochs'], batch_size=params['batch_size'],\n\u001b[0;32m---> 32\u001b[0;31m                 verbose = 0, validation_data=(reshape_x_test, y_test))\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#predict the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNNV2' used by {{node sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2}} with these attrs: [dropout=0, seed=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"gru\", is_training=true, seed2=0]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2]] [Op:__inference_train_function_6480]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 s\n"
     ]
    }
   ],
   "source": [
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = 2, trials = bayes_trials,\n",
    "            verbose = 1, rstate= np.random.RandomState(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar bayesiana, por si quisiera retomar donde queda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 52 ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(bayes_trials, open(folder + '/trials.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer mejores parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': -0.4901185770750988,\n",
       "  'status': 'ok',\n",
       "  'train_time': 101.01375379999999},\n",
       " {'loss': -0.4901185770750988, 'status': 'ok', 'train_time': 370.1786396}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 440 ms\n"
     ]
    }
   ],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "bayes_trials_results  = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results [:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.490119</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 512, '...</td>\n",
       "      <td>101.013754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.490119</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 256, '...</td>\n",
       "      <td>370.178640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                             params        time\n",
       "0 -0.490119  {'activation': 'softmax', 'batch_size': 512, '...  101.013754\n",
       "1 -0.490119  {'activation': 'softmax', 'batch_size': 256, '...  370.178640"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 402 ms\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(folder + '/gbm_results.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'softmax',\n",
       " 'batch_size': 512,\n",
       " 'dropout1': 0.3,\n",
       " 'dropout2': 0.2,\n",
       " 'nb_epochs': 50,\n",
       " 'optimizer': 'adadelta',\n",
       " 'units1': 32,\n",
       " 'units2': 256,\n",
       " 'units3': 32}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 430 ms\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'softmax',\n",
       " 'batch_size': 512,\n",
       " 'dropout1': 0.3,\n",
       " 'dropout2': 0.2,\n",
       " 'nb_epochs': 50,\n",
       " 'optimizer': 'adadelta',\n",
       " 'units1': 32,\n",
       " 'units2': 256,\n",
       " 'units3': 32}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.99 ms\n"
     ]
    }
   ],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_params = {'activation': 'softmax',\n",
    " 'batch_size': 512,\n",
    " 'dropout1': 0.3,\n",
    " 'dropout2': 0.2,\n",
    " 'nb_epochs': 50,\n",
    " 'optimizer': 'adadelta',\n",
    " 'units1': 32,\n",
    " 'units2': 256,\n",
    " 'units3': 32}\n",
    "best_bayes_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir datasets de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 43.2 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono la fecha para la cual hago el corte de train y test\n",
    "training_end = pd.to_datetime(\"2014-12-31\")\n",
    "num_training = len(embeddings_average_individual[(embeddings_average_individual[\"Date\"]) <= training_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>Top</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.297791</td>\n",
       "      <td>0.11853</td>\n",
       "      <td>-0.052536</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.072166</td>\n",
       "      <td>-1.084717</td>\n",
       "      <td>1.007492</td>\n",
       "      <td>-2.67218</td>\n",
       "      <td>0.917847</td>\n",
       "      <td>1.146057</td>\n",
       "      <td>1.12793</td>\n",
       "      <td>-2.239624</td>\n",
       "      <td>-1.407562</td>\n",
       "      <td>0.146851</td>\n",
       "      <td>-0.678009</td>\n",
       "      <td>1.267929</td>\n",
       "      <td>1.222971</td>\n",
       "      <td>1.804688</td>\n",
       "      <td>-0.047852</td>\n",
       "      <td>0.218285</td>\n",
       "      <td>-1.77063</td>\n",
       "      <td>-0.215424</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>-1.160614</td>\n",
       "      <td>0.128967</td>\n",
       "      <td>-1.029907</td>\n",
       "      <td>-0.813415</td>\n",
       "      <td>1.22113</td>\n",
       "      <td>1.369812</td>\n",
       "      <td>-0.730957</td>\n",
       "      <td>0.654297</td>\n",
       "      <td>-1.886658</td>\n",
       "      <td>-1.5625</td>\n",
       "      <td>0.483795</td>\n",
       "      <td>-0.255005</td>\n",
       "      <td>0.373047</td>\n",
       "      <td>1.035034</td>\n",
       "      <td>-1.348648</td>\n",
       "      <td>-0.239014</td>\n",
       "      <td>0.952393</td>\n",
       "      <td>0.652237</td>\n",
       "      <td>-1.431641</td>\n",
       "      <td>3.088379</td>\n",
       "      <td>-0.654663</td>\n",
       "      <td>1.638916</td>\n",
       "      <td>-0.813848</td>\n",
       "      <td>-0.523727</td>\n",
       "      <td>1.449463</td>\n",
       "      <td>-1.920776</td>\n",
       "      <td>0.446259</td>\n",
       "      <td>-1.054275</td>\n",
       "      <td>-0.693581</td>\n",
       "      <td>0.313599</td>\n",
       "      <td>0.445923</td>\n",
       "      <td>-0.774315</td>\n",
       "      <td>0.324829</td>\n",
       "      <td>-2.125046</td>\n",
       "      <td>-0.77417</td>\n",
       "      <td>-0.235962</td>\n",
       "      <td>-1.508881</td>\n",
       "      <td>0.390869</td>\n",
       "      <td>0.966186</td>\n",
       "      <td>-0.396484</td>\n",
       "      <td>-0.774658</td>\n",
       "      <td>0.999695</td>\n",
       "      <td>-0.124207</td>\n",
       "      <td>0.379761</td>\n",
       "      <td>-0.023071</td>\n",
       "      <td>-0.669434</td>\n",
       "      <td>0.229187</td>\n",
       "      <td>-0.226074</td>\n",
       "      <td>1.675415</td>\n",
       "      <td>-0.828003</td>\n",
       "      <td>2.10791</td>\n",
       "      <td>-2.284882</td>\n",
       "      <td>0.790832</td>\n",
       "      <td>1.19873</td>\n",
       "      <td>1.407288</td>\n",
       "      <td>1.212646</td>\n",
       "      <td>2.559891</td>\n",
       "      <td>0.635025</td>\n",
       "      <td>-1.069824</td>\n",
       "      <td>-0.805265</td>\n",
       "      <td>0.192871</td>\n",
       "      <td>-2.137085</td>\n",
       "      <td>0.504837</td>\n",
       "      <td>-0.904266</td>\n",
       "      <td>1.42218</td>\n",
       "      <td>1.345215</td>\n",
       "      <td>1.027527</td>\n",
       "      <td>0.762054</td>\n",
       "      <td>-0.05188</td>\n",
       "      <td>-2.539307</td>\n",
       "      <td>-0.972656</td>\n",
       "      <td>-2.322021</td>\n",
       "      <td>0.566742</td>\n",
       "      <td>0.817749</td>\n",
       "      <td>0.96283</td>\n",
       "      <td>1.350586</td>\n",
       "      <td>-0.401001</td>\n",
       "      <td>-0.84729</td>\n",
       "      <td>-0.432098</td>\n",
       "      <td>0.95166</td>\n",
       "      <td>-0.709259</td>\n",
       "      <td>0.125244</td>\n",
       "      <td>-3.625977</td>\n",
       "      <td>-0.025024</td>\n",
       "      <td>-1.140137</td>\n",
       "      <td>0.981644</td>\n",
       "      <td>-1.072884</td>\n",
       "      <td>0.804871</td>\n",
       "      <td>1.616699</td>\n",
       "      <td>2.054932</td>\n",
       "      <td>-0.590088</td>\n",
       "      <td>-0.769553</td>\n",
       "      <td>0.638794</td>\n",
       "      <td>-0.878517</td>\n",
       "      <td>-1.217651</td>\n",
       "      <td>1.406311</td>\n",
       "      <td>0.803253</td>\n",
       "      <td>-0.589294</td>\n",
       "      <td>-0.740234</td>\n",
       "      <td>-1.189941</td>\n",
       "      <td>-0.287292</td>\n",
       "      <td>1.729004</td>\n",
       "      <td>0.595703</td>\n",
       "      <td>1.505981</td>\n",
       "      <td>-0.935486</td>\n",
       "      <td>-0.111374</td>\n",
       "      <td>1.776184</td>\n",
       "      <td>0.098328</td>\n",
       "      <td>-1.184326</td>\n",
       "      <td>-0.800232</td>\n",
       "      <td>-1.227539</td>\n",
       "      <td>-0.969788</td>\n",
       "      <td>-0.560791</td>\n",
       "      <td>-0.950134</td>\n",
       "      <td>-1.536499</td>\n",
       "      <td>-0.18042</td>\n",
       "      <td>2.770508</td>\n",
       "      <td>-0.004639</td>\n",
       "      <td>-1.064209</td>\n",
       "      <td>0.266174</td>\n",
       "      <td>0.337097</td>\n",
       "      <td>-1.557068</td>\n",
       "      <td>1.394409</td>\n",
       "      <td>-1.084351</td>\n",
       "      <td>-0.284393</td>\n",
       "      <td>-1.847412</td>\n",
       "      <td>-2.96294</td>\n",
       "      <td>2.604217</td>\n",
       "      <td>-0.180145</td>\n",
       "      <td>-1.456436</td>\n",
       "      <td>0.188629</td>\n",
       "      <td>1.366577</td>\n",
       "      <td>-0.243599</td>\n",
       "      <td>-1.397461</td>\n",
       "      <td>-0.035675</td>\n",
       "      <td>-0.481445</td>\n",
       "      <td>-0.673584</td>\n",
       "      <td>-0.662354</td>\n",
       "      <td>0.59227</td>\n",
       "      <td>2.191803</td>\n",
       "      <td>0.24913</td>\n",
       "      <td>-0.701172</td>\n",
       "      <td>-1.954224</td>\n",
       "      <td>1.064941</td>\n",
       "      <td>-0.872711</td>\n",
       "      <td>-0.317383</td>\n",
       "      <td>-0.844971</td>\n",
       "      <td>-1.293579</td>\n",
       "      <td>-0.468643</td>\n",
       "      <td>-1.184338</td>\n",
       "      <td>-0.915222</td>\n",
       "      <td>-0.354103</td>\n",
       "      <td>-0.234558</td>\n",
       "      <td>2.518311</td>\n",
       "      <td>0.69397</td>\n",
       "      <td>0.318817</td>\n",
       "      <td>-0.560059</td>\n",
       "      <td>-0.669189</td>\n",
       "      <td>-1.111862</td>\n",
       "      <td>-0.242504</td>\n",
       "      <td>-0.766937</td>\n",
       "      <td>1.185242</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>-0.056015</td>\n",
       "      <td>-0.442749</td>\n",
       "      <td>1.220459</td>\n",
       "      <td>1.546211</td>\n",
       "      <td>1.34462</td>\n",
       "      <td>1.296707</td>\n",
       "      <td>-0.106125</td>\n",
       "      <td>1.265228</td>\n",
       "      <td>0.357681</td>\n",
       "      <td>2.07373</td>\n",
       "      <td>0.662231</td>\n",
       "      <td>-0.05957</td>\n",
       "      <td>-1.016968</td>\n",
       "      <td>-1.346008</td>\n",
       "      <td>-1.765442</td>\n",
       "      <td>0.503052</td>\n",
       "      <td>0.230667</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>1.595276</td>\n",
       "      <td>-2.131592</td>\n",
       "      <td>0.569336</td>\n",
       "      <td>0.911575</td>\n",
       "      <td>1.56543</td>\n",
       "      <td>-1.719421</td>\n",
       "      <td>-0.355591</td>\n",
       "      <td>0.974976</td>\n",
       "      <td>-0.93049</td>\n",
       "      <td>0.628174</td>\n",
       "      <td>-2.59491</td>\n",
       "      <td>-0.384369</td>\n",
       "      <td>0.158997</td>\n",
       "      <td>-0.220093</td>\n",
       "      <td>-0.813904</td>\n",
       "      <td>1.058258</td>\n",
       "      <td>-0.875198</td>\n",
       "      <td>0.345886</td>\n",
       "      <td>0.79657</td>\n",
       "      <td>-0.622925</td>\n",
       "      <td>1.770767</td>\n",
       "      <td>-2.032227</td>\n",
       "      <td>0.16489</td>\n",
       "      <td>1.068115</td>\n",
       "      <td>-0.985771</td>\n",
       "      <td>-1.562866</td>\n",
       "      <td>0.276443</td>\n",
       "      <td>-1.768646</td>\n",
       "      <td>-0.855682</td>\n",
       "      <td>-1.012405</td>\n",
       "      <td>-0.632263</td>\n",
       "      <td>0.454788</td>\n",
       "      <td>2.258499</td>\n",
       "      <td>-1.554581</td>\n",
       "      <td>1.240238</td>\n",
       "      <td>-0.591064</td>\n",
       "      <td>0.942139</td>\n",
       "      <td>-0.950928</td>\n",
       "      <td>-0.43219</td>\n",
       "      <td>-0.908936</td>\n",
       "      <td>-0.974302</td>\n",
       "      <td>1.992859</td>\n",
       "      <td>0.100708</td>\n",
       "      <td>0.734985</td>\n",
       "      <td>0.810295</td>\n",
       "      <td>-0.974106</td>\n",
       "      <td>2.37027</td>\n",
       "      <td>0.53833</td>\n",
       "      <td>0.216553</td>\n",
       "      <td>1.148193</td>\n",
       "      <td>0.181946</td>\n",
       "      <td>-0.096233</td>\n",
       "      <td>0.223198</td>\n",
       "      <td>0.597595</td>\n",
       "      <td>-1.044861</td>\n",
       "      <td>-1.081238</td>\n",
       "      <td>0.177124</td>\n",
       "      <td>-0.373474</td>\n",
       "      <td>-0.043915</td>\n",
       "      <td>-0.296265</td>\n",
       "      <td>1.672363</td>\n",
       "      <td>1.96521</td>\n",
       "      <td>-0.057861</td>\n",
       "      <td>-0.500366</td>\n",
       "      <td>-0.535094</td>\n",
       "      <td>-0.482941</td>\n",
       "      <td>0.751038</td>\n",
       "      <td>-0.80249</td>\n",
       "      <td>-0.013702</td>\n",
       "      <td>-0.374512</td>\n",
       "      <td>1.786119</td>\n",
       "      <td>-1.035339</td>\n",
       "      <td>-2.053467</td>\n",
       "      <td>-0.839661</td>\n",
       "      <td>-0.804504</td>\n",
       "      <td>-1.061066</td>\n",
       "      <td>0.308296</td>\n",
       "      <td>0.618309</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>1.387695</td>\n",
       "      <td>-0.731941</td>\n",
       "      <td>-0.389038</td>\n",
       "      <td>-0.86499</td>\n",
       "      <td>1.035759</td>\n",
       "      <td>-0.69553</td>\n",
       "      <td>2.501327</td>\n",
       "      <td>-0.733643</td>\n",
       "      <td>0.134048</td>\n",
       "      <td>-2.371773</td>\n",
       "      <td>0.57251</td>\n",
       "      <td>0.695374</td>\n",
       "      <td>0.051147</td>\n",
       "      <td>-1.660706</td>\n",
       "      <td>-0.961337</td>\n",
       "      <td>0.775513</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>0.038754</td>\n",
       "      <td>0.031172</td>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.287754</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>0.490658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1         2         3         4         5         6        7         8         9       10        11        12        13        14        15        16        17        18        19       20        21        22        23        24        25        26       27        28        29        30        31      32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49        50        51        52        53        54        55        56       57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72       73        74        75       76        77        78        79        80        81        82        83        84        85        86       87        88        89        90       91        92        93        94        95        96       97        98        99      100  \\\n",
       "0  0.297791  0.11853 -0.052536  0.001251  0.072166 -1.084717  1.007492 -2.67218  0.917847  1.146057  1.12793 -2.239624 -1.407562  0.146851 -0.678009  1.267929  1.222971  1.804688 -0.047852  0.218285 -1.77063 -0.215424  0.030884 -1.160614  0.128967 -1.029907 -0.813415  1.22113  1.369812 -0.730957  0.654297 -1.886658 -1.5625  0.483795 -0.255005  0.373047  1.035034 -1.348648 -0.239014  0.952393  0.652237 -1.431641  3.088379 -0.654663  1.638916 -0.813848 -0.523727  1.449463 -1.920776  0.446259 -1.054275 -0.693581  0.313599  0.445923 -0.774315  0.324829 -2.125046 -0.77417 -0.235962 -1.508881  0.390869  0.966186 -0.396484 -0.774658  0.999695 -0.124207  0.379761 -0.023071 -0.669434  0.229187 -0.226074  1.675415 -0.828003  2.10791 -2.284882  0.790832  1.19873  1.407288  1.212646  2.559891  0.635025 -1.069824 -0.805265  0.192871 -2.137085  0.504837 -0.904266  1.42218  1.345215  1.027527  0.762054 -0.05188 -2.539307 -0.972656 -2.322021  0.566742  0.817749  0.96283  1.350586 -0.401001 -0.84729   \n",
       "\n",
       "        101      102       103       104       105       106       107       108       109       110       111       112       113       114       115       116       117       118       119       120       121       122       123       124       125       126       127       128       129       130       131       132       133       134       135       136       137      138       139       140       141       142       143       144       145       146       147       148      149       150       151       152       153       154       155       156       157       158       159       160      161       162      163       164       165       166       167       168       169       170       171       172       173       174       175       176      177       178       179       180       181       182       183       184       185       186       187       188       189      190       191       192       193       194      195       196      197       198       199       200  \\\n",
       "0 -0.432098  0.95166 -0.709259  0.125244 -3.625977 -0.025024 -1.140137  0.981644 -1.072884  0.804871  1.616699  2.054932 -0.590088 -0.769553  0.638794 -0.878517 -1.217651  1.406311  0.803253 -0.589294 -0.740234 -1.189941 -0.287292  1.729004  0.595703  1.505981 -0.935486 -0.111374  1.776184  0.098328 -1.184326 -0.800232 -1.227539 -0.969788 -0.560791 -0.950134 -1.536499 -0.18042  2.770508 -0.004639 -1.064209  0.266174  0.337097 -1.557068  1.394409 -1.084351 -0.284393 -1.847412 -2.96294  2.604217 -0.180145 -1.456436  0.188629  1.366577 -0.243599 -1.397461 -0.035675 -0.481445 -0.673584 -0.662354  0.59227  2.191803  0.24913 -0.701172 -1.954224  1.064941 -0.872711 -0.317383 -0.844971 -1.293579 -0.468643 -1.184338 -0.915222 -0.354103 -0.234558  2.518311  0.69397  0.318817 -0.560059 -0.669189 -1.111862 -0.242504 -0.766937  1.185242  0.289062 -0.056015 -0.442749  1.220459  1.546211  1.34462  1.296707 -0.106125  1.265228  0.357681  2.07373  0.662231 -0.05957 -1.016968 -1.346008 -1.765442   \n",
       "\n",
       "        201       202       203       204       205       206       207      208       209       210       211      212       213      214       215       216       217       218       219       220       221      222       223       224       225      226       227       228       229       230       231       232       233       234       235       236       237       238       239       240       241      242       243       244       245       246       247       248       249      250      251       252       253       254       255       256       257       258       259       260       261       262       263       264      265       266       267       268       269       270      271       272       273       274       275       276       277       278       279       280       281       282       283       284       285      286       287      288       289       290       291       292      293       294       295       296       297       298       299  Label       Date  \\\n",
       "0  0.503052  0.230667  0.227539  1.595276 -2.131592  0.569336  0.911575  1.56543 -1.719421 -0.355591  0.974976 -0.93049  0.628174 -2.59491 -0.384369  0.158997 -0.220093 -0.813904  1.058258 -0.875198  0.345886  0.79657 -0.622925  1.770767 -2.032227  0.16489  1.068115 -0.985771 -1.562866  0.276443 -1.768646 -0.855682 -1.012405 -0.632263  0.454788  2.258499 -1.554581  1.240238 -0.591064  0.942139 -0.950928 -0.43219 -0.908936 -0.974302  1.992859  0.100708  0.734985  0.810295 -0.974106  2.37027  0.53833  0.216553  1.148193  0.181946 -0.096233  0.223198  0.597595 -1.044861 -1.081238  0.177124 -0.373474 -0.043915 -0.296265  1.672363  1.96521 -0.057861 -0.500366 -0.535094 -0.482941  0.751038 -0.80249 -0.013702 -0.374512  1.786119 -1.035339 -2.053467 -0.839661 -0.804504 -1.061066  0.308296  0.618309  0.014404  1.387695 -0.731941 -0.389038 -0.86499  1.035759 -0.69553  2.501327 -0.733643  0.134048 -2.371773  0.57251  0.695374  0.051147 -1.660706 -0.961337  0.775513 -0.056641      1 2016-07-01   \n",
       "\n",
       "   Top   topic_0   topic_1   topic_2   topic_3   topic_4   topic_5  topic_6   topic_7   topic_8   topic_9  \n",
       "0    1  0.051395  0.038754  0.031172  0.026116  0.022466  0.019712  0.01756  0.287754  0.014414  0.490658  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 245 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono el archivo con el que se corre el modelo\n",
    "data = embeddings_average_individual\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9443, 1, 310)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 388 ms\n"
     ]
    }
   ],
   "source": [
    "training = data[:num_training]\n",
    "testing = data[num_training:]\n",
    "\n",
    "# Se separa en train y test\n",
    "x_train = data.drop([\"Top\",\"Label\", \"Date\"], axis=1)[:num_training]\n",
    "x_test = data.drop([\"Top\",'Label', 'Date'], axis=1)[num_training:]\n",
    "y_train = data[\"Label\"].values[:num_training]\n",
    "y_test = data[\"Label\"].values[num_training:]\n",
    "\n",
    "\n",
    "x_train_array = x_train.to_numpy()\n",
    "reshape_x_train = x_train_array.reshape(len(x_train), 1, x_train.shape[1])\n",
    "reshape_x_train.shape\n",
    "\n",
    "x_test_array = x_test.to_numpy()\n",
    "reshape_x_test = x_test_array.reshape(len(x_test), 1, x_train.shape[1])\n",
    "reshape_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIKq7z8tnIWl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 56.3 ms\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNGRU(128, return_sequences = True)))\n",
    "model.add(Dropout(best_bayes_params['dropout1']))\n",
    "model.add(Bidirectional(CuDNNGRU(best_bayes_params['units1'])))\n",
    "model.add(Dropout(best_bayes_params['dropout2']))\n",
    "model.add(Dense(best_bayes_params['units2'], activation='relu'))\n",
    "model.add(Dense(best_bayes_params['units3'], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=best_bayes_params['optimizer'], loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "# define the checkpoint\n",
    "filepath= ch_folder + \"/word2vec-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 41.6 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logdir = \"Resultados\\\\\" + exp_name +\"\\\\logs\\\\model\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1589755592715,
     "user": {
      "displayName": "Melina D'Alessandro",
      "photoUrl": "https://lh4.googleusercontent.com/-AU_sxBOTu8w/AAAAAAAAAAI/AAAAAAAAAR8/nO0zS5J_9Wo/s64/photo.jpg",
      "userId": "09190509655785270416"
     },
     "user_tz": 180
    },
    "id": "JsHgNLFnnTLN",
    "outputId": "4c22910d-c7b2-4dff-eb32-15c2574174ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "No OpKernel was registered to support Op 'CudnnRNNV2' used by {{node sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2}} with these attrs: [seed=0, dropout=0, input_mode=\"linear_input\", T=DT_FLOAT, direction=\"unidirectional\", rnn_mode=\"gru\", seed2=0, is_training=true]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2]] [Op:__inference_train_function_6540]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3cf8259f0285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m model.fit(reshape_x_train, y_train,\n\u001b[1;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           batch_size=100)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNNV2' used by {{node sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2}} with these attrs: [seed=0, dropout=0, input_mode=\"linear_input\", T=DT_FLOAT, direction=\"unidirectional\", rnn_mode=\"gru\", seed2=0, is_training=true]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[sequential_1/bidirectional_2/forward_cu_dnngru_2/CudnnRNNV2]] [Op:__inference_train_function_6540]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "# # fit the model\n",
    "# model.fit(reshape_x_train, y_train,\n",
    "#           epochs=best_bayes_params['nb_epochs'], \n",
    "#           batch_size=best_bayes_params['batch_size'], callbacks=[tensor_board])\n",
    "\n",
    "# fit the model\n",
    "model.fit(reshape_x_train, y_train,\n",
    "          epochs=50,\n",
    "          batch_size=100)\n",
    "\n",
    "\n",
    "model.save(folder + '/keras_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5391\n",
      "Testing Accuracy:  0.5195\n",
      "time: 12 s\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(reshape_x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(reshape_x_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1165,
     "status": "ok",
     "timestamp": 1589755595730,
     "user": {
      "displayName": "Melina D'Alessandro",
      "photoUrl": "https://lh4.googleusercontent.com/-AU_sxBOTu8w/AAAAAAAAAAI/AAAAAAAAAR8/nO0zS5J_9Wo/s64/photo.jpg",
      "userId": "09190509655785270416"
     },
     "user_tz": 180
    },
    "id": "mdsR4Qngv5Q1",
    "outputId": "433bbcf6-d6eb-44c1-a1bb-afee72ac0ffb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5271095 ],\n",
       "       [0.53398144],\n",
       "       [0.52964747],\n",
       "       ...,\n",
       "       [0.5279911 ],\n",
       "       [0.52568513],\n",
       "       [0.5369585 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "ypred = model.predict_proba(reshape_x_test)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de das en testing:  379\n",
      "Numero de registros en testing:  379\n",
      "time: 70 ms\n"
     ]
    }
   ],
   "source": [
    "testing_cp = testing.copy()\n",
    "testing_cp['Prob'] = ypred\n",
    "testing_cp['Prob_dia'] = testing_cp['Prob'].groupby(testing_cp['Date']).transform('mean')\n",
    "testing_cp['Prediction'] = 0\n",
    "testing_cp.loc[testing_cp['Prob_dia']> 0.5, 'Prediction'] = 1\n",
    "testing_cp.drop_duplicates(subset=['Date','Prediction','Label'], inplace=True)\n",
    "testing_cp.head(1)\n",
    "\n",
    "print('Numero de das en testing: ', testing['Date'].nunique())\n",
    "print('Numero de registros en testing: ', testing_cp['Date'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 364 ms\n"
     ]
    }
   ],
   "source": [
    "testing_cp['Label'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    197\n",
       "0    182\n",
       "Name: Label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    379\n",
       "Name: Prediction, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "display(testing_cp['Label'].value_counts(),\n",
    "        testing_cp['Prediction'].value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.00      0.00      0.00       182\n",
      "     class 1       0.52      1.00      0.68       197\n",
      "\n",
      "    accuracy                           0.52       379\n",
      "   macro avg       0.26      0.50      0.34       379\n",
      "weighted avg       0.27      0.52      0.36       379\n",
      "\n",
      "time: 405 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(testing_cp.Label, testing_cp.Prediction, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creates a confusion matrix\n",
    "cm = confusion_matrix(testing_cp.Label, testing_cp.Prediction) \n",
    "\n",
    "# Transform to df for easier plotting\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['Sube','Baja'], \n",
    "                     columns = ['Sube','Baja'])\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cm_df, annot=True, cmap=\"Greens\", fmt='g')\n",
    "plt.title('Neuronal Network \\nAccuracy:{0:.3f}'.format(accuracy_score(testing_cp.Label, testing_cp.Prediction)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.save_checkpoint()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resultados/7/RNN_Model_Base.ipynb'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 274 ms\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "copyfile('RNN_Model_Base_GPU_m3.ipynb', folder + '/RNN_Model_Base.ipynb' )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhtKTbzPEwz7PyaEe0FkIO",
   "collapsed_sections": [],
   "name": "RNN - Meli.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
