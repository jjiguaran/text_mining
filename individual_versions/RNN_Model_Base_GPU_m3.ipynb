{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4dLP23xZmpC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "# Importar librerias\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "from datetime import date\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, GlobalMaxPool1D, Conv1D, Flatten,  MaxPooling1D, Activation, GlobalMaxPooling1D, Bidirectional, GRU\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.98 ms\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n",
      "time: 95 ms\n"
     ]
    }
   ],
   "source": [
    "exp_name = '7'\n",
    "folder = 'Resultados/' + exp_name\n",
    "my_file = Path(folder)\n",
    "if os.path.exists(my_file):\n",
    "    print('already exists')\n",
    "else:\n",
    "    os.makedirs(folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n",
      "time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "ch_folder = folder + '/Checkpoints'\n",
    "my_file = Path(ch_folder)\n",
    "if os.path.exists(my_file):\n",
    "    print('already exists')\n",
    "else:\n",
    "    os.makedirs(ch_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nzv66BqFbl92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Top</th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49649</th>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.050684</td>\n",
       "      <td>-0.078711</td>\n",
       "      <td>0.069385</td>\n",
       "      <td>-0.120630</td>\n",
       "      <td>0.095508</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.096631</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.134375</td>\n",
       "      <td>-0.150244</td>\n",
       "      <td>-0.100488</td>\n",
       "      <td>0.123291</td>\n",
       "      <td>0.020605</td>\n",
       "      <td>0.104590</td>\n",
       "      <td>0.079565</td>\n",
       "      <td>0.159546</td>\n",
       "      <td>-0.014038</td>\n",
       "      <td>-0.019336</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>-0.091040</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>-0.058362</td>\n",
       "      <td>-0.175696</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>-0.047949</td>\n",
       "      <td>0.057214</td>\n",
       "      <td>-0.001563</td>\n",
       "      <td>-0.074683</td>\n",
       "      <td>-0.089453</td>\n",
       "      <td>-0.136575</td>\n",
       "      <td>-0.035522</td>\n",
       "      <td>-0.044775</td>\n",
       "      <td>-0.084277</td>\n",
       "      <td>-0.087115</td>\n",
       "      <td>0.092236</td>\n",
       "      <td>-0.074023</td>\n",
       "      <td>0.171582</td>\n",
       "      <td>0.055505</td>\n",
       "      <td>-0.031738</td>\n",
       "      <td>0.079736</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.062964</td>\n",
       "      <td>-0.156643</td>\n",
       "      <td>0.037891</td>\n",
       "      <td>0.053223</td>\n",
       "      <td>-0.184277</td>\n",
       "      <td>0.015015</td>\n",
       "      <td>-0.042213</td>\n",
       "      <td>0.110443</td>\n",
       "      <td>-0.159143</td>\n",
       "      <td>0.075684</td>\n",
       "      <td>0.050726</td>\n",
       "      <td>-0.124219</td>\n",
       "      <td>-0.053876</td>\n",
       "      <td>0.066498</td>\n",
       "      <td>0.046753</td>\n",
       "      <td>-0.142627</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>0.155615</td>\n",
       "      <td>-0.119690</td>\n",
       "      <td>0.023047</td>\n",
       "      <td>0.107422</td>\n",
       "      <td>-0.012073</td>\n",
       "      <td>0.052051</td>\n",
       "      <td>-0.038714</td>\n",
       "      <td>0.096625</td>\n",
       "      <td>0.032764</td>\n",
       "      <td>-0.105762</td>\n",
       "      <td>0.075488</td>\n",
       "      <td>-0.013354</td>\n",
       "      <td>-0.159668</td>\n",
       "      <td>-0.022070</td>\n",
       "      <td>-0.127051</td>\n",
       "      <td>-0.002856</td>\n",
       "      <td>0.020410</td>\n",
       "      <td>0.096093</td>\n",
       "      <td>0.051074</td>\n",
       "      <td>-0.160498</td>\n",
       "      <td>-0.044824</td>\n",
       "      <td>0.037259</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>-0.170763</td>\n",
       "      <td>0.044312</td>\n",
       "      <td>-0.043982</td>\n",
       "      <td>-0.170801</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>-0.107178</td>\n",
       "      <td>-0.018710</td>\n",
       "      <td>-0.023686</td>\n",
       "      <td>0.083203</td>\n",
       "      <td>-0.066431</td>\n",
       "      <td>0.240039</td>\n",
       "      <td>0.015967</td>\n",
       "      <td>0.080176</td>\n",
       "      <td>0.083057</td>\n",
       "      <td>0.085913</td>\n",
       "      <td>0.160815</td>\n",
       "      <td>-0.024756</td>\n",
       "      <td>0.089258</td>\n",
       "      <td>0.134863</td>\n",
       "      <td>-0.183105</td>\n",
       "      <td>0.093701</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>-0.030859</td>\n",
       "      <td>-0.171924</td>\n",
       "      <td>-0.070312</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>0.073975</td>\n",
       "      <td>-0.145459</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>-0.124481</td>\n",
       "      <td>-0.067749</td>\n",
       "      <td>-0.072168</td>\n",
       "      <td>0.152429</td>\n",
       "      <td>0.090430</td>\n",
       "      <td>-0.165698</td>\n",
       "      <td>0.082666</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>-0.021973</td>\n",
       "      <td>-0.027893</td>\n",
       "      <td>-0.214258</td>\n",
       "      <td>0.093848</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.018594</td>\n",
       "      <td>0.048389</td>\n",
       "      <td>0.057397</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>-0.235205</td>\n",
       "      <td>-0.159955</td>\n",
       "      <td>-0.205176</td>\n",
       "      <td>-0.246289</td>\n",
       "      <td>-0.039508</td>\n",
       "      <td>0.218457</td>\n",
       "      <td>-0.163428</td>\n",
       "      <td>0.123743</td>\n",
       "      <td>0.073145</td>\n",
       "      <td>-0.007524</td>\n",
       "      <td>-0.051318</td>\n",
       "      <td>0.064874</td>\n",
       "      <td>0.069727</td>\n",
       "      <td>-0.036743</td>\n",
       "      <td>-0.072217</td>\n",
       "      <td>-0.016357</td>\n",
       "      <td>-0.172266</td>\n",
       "      <td>-0.006573</td>\n",
       "      <td>0.173486</td>\n",
       "      <td>-0.005371</td>\n",
       "      <td>-0.003857</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>-0.031494</td>\n",
       "      <td>-0.094775</td>\n",
       "      <td>0.090625</td>\n",
       "      <td>-0.196759</td>\n",
       "      <td>-0.032446</td>\n",
       "      <td>-0.116797</td>\n",
       "      <td>0.097949</td>\n",
       "      <td>0.073145</td>\n",
       "      <td>0.187024</td>\n",
       "      <td>-0.023914</td>\n",
       "      <td>-0.021729</td>\n",
       "      <td>-0.093010</td>\n",
       "      <td>0.054492</td>\n",
       "      <td>0.035657</td>\n",
       "      <td>-0.014432</td>\n",
       "      <td>-0.093689</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>-0.125958</td>\n",
       "      <td>-0.115039</td>\n",
       "      <td>-0.030200</td>\n",
       "      <td>-0.080908</td>\n",
       "      <td>0.124854</td>\n",
       "      <td>-0.017725</td>\n",
       "      <td>0.061914</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>-0.130652</td>\n",
       "      <td>-0.037939</td>\n",
       "      <td>0.100732</td>\n",
       "      <td>-0.101514</td>\n",
       "      <td>0.065918</td>\n",
       "      <td>0.064258</td>\n",
       "      <td>-0.105762</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>0.134717</td>\n",
       "      <td>-0.051904</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.099512</td>\n",
       "      <td>0.149072</td>\n",
       "      <td>-0.068091</td>\n",
       "      <td>-0.006396</td>\n",
       "      <td>0.045746</td>\n",
       "      <td>-0.036450</td>\n",
       "      <td>-0.158325</td>\n",
       "      <td>-0.030737</td>\n",
       "      <td>0.049097</td>\n",
       "      <td>0.045508</td>\n",
       "      <td>-0.149609</td>\n",
       "      <td>0.038666</td>\n",
       "      <td>0.093359</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>-0.016589</td>\n",
       "      <td>-0.057178</td>\n",
       "      <td>0.11460</td>\n",
       "      <td>-0.064404</td>\n",
       "      <td>-0.166699</td>\n",
       "      <td>0.026172</td>\n",
       "      <td>-0.048755</td>\n",
       "      <td>-0.047766</td>\n",
       "      <td>-0.108203</td>\n",
       "      <td>-0.067676</td>\n",
       "      <td>0.05293</td>\n",
       "      <td>0.105859</td>\n",
       "      <td>-0.162329</td>\n",
       "      <td>-0.151929</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>0.120850</td>\n",
       "      <td>-0.032813</td>\n",
       "      <td>-0.003711</td>\n",
       "      <td>-0.147778</td>\n",
       "      <td>0.150977</td>\n",
       "      <td>-0.049023</td>\n",
       "      <td>-0.017871</td>\n",
       "      <td>0.169336</td>\n",
       "      <td>0.044727</td>\n",
       "      <td>-0.014014</td>\n",
       "      <td>-0.022339</td>\n",
       "      <td>-0.033347</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>-0.017822</td>\n",
       "      <td>-0.022852</td>\n",
       "      <td>0.067737</td>\n",
       "      <td>0.113086</td>\n",
       "      <td>-0.056299</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>-0.058374</td>\n",
       "      <td>0.073743</td>\n",
       "      <td>-0.199902</td>\n",
       "      <td>0.022217</td>\n",
       "      <td>0.091113</td>\n",
       "      <td>-0.015039</td>\n",
       "      <td>0.064966</td>\n",
       "      <td>-0.006104</td>\n",
       "      <td>-0.209302</td>\n",
       "      <td>0.096289</td>\n",
       "      <td>0.058203</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>-0.074133</td>\n",
       "      <td>-0.156067</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>0.082349</td>\n",
       "      <td>-0.107092</td>\n",
       "      <td>0.155078</td>\n",
       "      <td>-0.066010</td>\n",
       "      <td>0.070361</td>\n",
       "      <td>-0.042505</td>\n",
       "      <td>0.131006</td>\n",
       "      <td>0.076392</td>\n",
       "      <td>-0.072461</td>\n",
       "      <td>-0.046094</td>\n",
       "      <td>0.154224</td>\n",
       "      <td>-0.020947</td>\n",
       "      <td>-0.030371</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.204150</td>\n",
       "      <td>0.089087</td>\n",
       "      <td>-0.191699</td>\n",
       "      <td>-0.013916</td>\n",
       "      <td>-0.139600</td>\n",
       "      <td>-0.072852</td>\n",
       "      <td>-0.053857</td>\n",
       "      <td>-0.142529</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.062061</td>\n",
       "      <td>-0.052393</td>\n",
       "      <td>0.167090</td>\n",
       "      <td>0.058887</td>\n",
       "      <td>-0.076538</td>\n",
       "      <td>-0.095724</td>\n",
       "      <td>-0.230847</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.106544</td>\n",
       "      <td>0.180566</td>\n",
       "      <td>0.070203</td>\n",
       "      <td>0.108008</td>\n",
       "      <td>-0.031592</td>\n",
       "      <td>-0.192676</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.075034</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>-0.095508</td>\n",
       "      <td>0.155450</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>24</td>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1759</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49650</th>\n",
       "      <td>0.073608</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>-0.069580</td>\n",
       "      <td>-0.088684</td>\n",
       "      <td>-0.126465</td>\n",
       "      <td>-0.135132</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>-0.334961</td>\n",
       "      <td>0.136230</td>\n",
       "      <td>0.150879</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.105713</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.161499</td>\n",
       "      <td>-0.276367</td>\n",
       "      <td>-0.014648</td>\n",
       "      <td>-0.018066</td>\n",
       "      <td>-0.157227</td>\n",
       "      <td>0.057129</td>\n",
       "      <td>-0.074219</td>\n",
       "      <td>0.015472</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.035583</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.050232</td>\n",
       "      <td>-0.231201</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.112549</td>\n",
       "      <td>0.104164</td>\n",
       "      <td>-0.083252</td>\n",
       "      <td>-0.042847</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>-0.002022</td>\n",
       "      <td>-0.015259</td>\n",
       "      <td>0.102051</td>\n",
       "      <td>-0.054443</td>\n",
       "      <td>0.074585</td>\n",
       "      <td>0.190308</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.017212</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>0.052979</td>\n",
       "      <td>0.127441</td>\n",
       "      <td>0.041626</td>\n",
       "      <td>-0.037476</td>\n",
       "      <td>0.186035</td>\n",
       "      <td>-0.058472</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>-0.024414</td>\n",
       "      <td>-0.101318</td>\n",
       "      <td>0.059448</td>\n",
       "      <td>-0.038818</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>0.194824</td>\n",
       "      <td>-0.150024</td>\n",
       "      <td>0.104996</td>\n",
       "      <td>-0.084717</td>\n",
       "      <td>-0.017090</td>\n",
       "      <td>-0.228516</td>\n",
       "      <td>-0.140137</td>\n",
       "      <td>-0.166992</td>\n",
       "      <td>-0.025635</td>\n",
       "      <td>0.056763</td>\n",
       "      <td>0.053955</td>\n",
       "      <td>-0.024170</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>-0.187988</td>\n",
       "      <td>-0.006149</td>\n",
       "      <td>0.095215</td>\n",
       "      <td>-0.068298</td>\n",
       "      <td>0.083374</td>\n",
       "      <td>-0.022217</td>\n",
       "      <td>0.172363</td>\n",
       "      <td>-0.237793</td>\n",
       "      <td>-0.091675</td>\n",
       "      <td>0.073486</td>\n",
       "      <td>-0.096924</td>\n",
       "      <td>-0.298096</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>-0.009766</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>0.118530</td>\n",
       "      <td>-0.108398</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>0.016373</td>\n",
       "      <td>0.094238</td>\n",
       "      <td>0.097534</td>\n",
       "      <td>0.108337</td>\n",
       "      <td>0.149597</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.243896</td>\n",
       "      <td>-0.097191</td>\n",
       "      <td>-0.044495</td>\n",
       "      <td>0.030579</td>\n",
       "      <td>-0.127319</td>\n",
       "      <td>0.216476</td>\n",
       "      <td>-0.082153</td>\n",
       "      <td>-0.138916</td>\n",
       "      <td>-0.220703</td>\n",
       "      <td>0.077881</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>0.236877</td>\n",
       "      <td>-0.114258</td>\n",
       "      <td>-0.154053</td>\n",
       "      <td>0.232910</td>\n",
       "      <td>-0.101074</td>\n",
       "      <td>-0.201172</td>\n",
       "      <td>0.088623</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>-0.002197</td>\n",
       "      <td>0.283691</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>0.333496</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>0.044067</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>-0.086182</td>\n",
       "      <td>-0.043671</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.148682</td>\n",
       "      <td>0.055725</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>-0.068359</td>\n",
       "      <td>-0.011810</td>\n",
       "      <td>0.112152</td>\n",
       "      <td>0.275391</td>\n",
       "      <td>-0.065193</td>\n",
       "      <td>-0.439453</td>\n",
       "      <td>0.074341</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.083008</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>0.124695</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>0.101807</td>\n",
       "      <td>-0.030762</td>\n",
       "      <td>-0.225098</td>\n",
       "      <td>0.467041</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>0.070618</td>\n",
       "      <td>0.184814</td>\n",
       "      <td>-0.049103</td>\n",
       "      <td>-0.184082</td>\n",
       "      <td>0.024933</td>\n",
       "      <td>0.085571</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>-0.153687</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>0.092285</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>-0.215942</td>\n",
       "      <td>-0.115143</td>\n",
       "      <td>0.187378</td>\n",
       "      <td>0.262695</td>\n",
       "      <td>-0.063721</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.202637</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.166504</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.224609</td>\n",
       "      <td>-0.033386</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.225098</td>\n",
       "      <td>-0.079712</td>\n",
       "      <td>-0.113525</td>\n",
       "      <td>0.084961</td>\n",
       "      <td>-0.257324</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.320312</td>\n",
       "      <td>0.165771</td>\n",
       "      <td>-0.292480</td>\n",
       "      <td>-0.020752</td>\n",
       "      <td>0.072754</td>\n",
       "      <td>-0.084839</td>\n",
       "      <td>0.069092</td>\n",
       "      <td>0.035198</td>\n",
       "      <td>0.018799</td>\n",
       "      <td>0.104736</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>-0.112061</td>\n",
       "      <td>0.046021</td>\n",
       "      <td>-0.188477</td>\n",
       "      <td>0.086792</td>\n",
       "      <td>-0.155884</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.155029</td>\n",
       "      <td>0.203003</td>\n",
       "      <td>0.062073</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>0.068481</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.070190</td>\n",
       "      <td>-0.297363</td>\n",
       "      <td>0.20166</td>\n",
       "      <td>-0.021973</td>\n",
       "      <td>-0.176086</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>0.100830</td>\n",
       "      <td>0.077271</td>\n",
       "      <td>-0.143433</td>\n",
       "      <td>0.030304</td>\n",
       "      <td>-0.09021</td>\n",
       "      <td>-0.142334</td>\n",
       "      <td>-0.226318</td>\n",
       "      <td>-0.059570</td>\n",
       "      <td>0.246582</td>\n",
       "      <td>-0.049927</td>\n",
       "      <td>0.216553</td>\n",
       "      <td>-0.128418</td>\n",
       "      <td>-0.072021</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>-0.065308</td>\n",
       "      <td>-0.057007</td>\n",
       "      <td>0.020020</td>\n",
       "      <td>0.082642</td>\n",
       "      <td>-0.013641</td>\n",
       "      <td>-0.182129</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.152588</td>\n",
       "      <td>-0.097046</td>\n",
       "      <td>-0.043701</td>\n",
       "      <td>0.066650</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>0.190308</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>-0.068115</td>\n",
       "      <td>0.157715</td>\n",
       "      <td>-0.093140</td>\n",
       "      <td>-0.064453</td>\n",
       "      <td>-0.188721</td>\n",
       "      <td>0.280273</td>\n",
       "      <td>0.023621</td>\n",
       "      <td>-0.118164</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.078857</td>\n",
       "      <td>-0.029785</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>0.061401</td>\n",
       "      <td>0.183838</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.260254</td>\n",
       "      <td>-0.113647</td>\n",
       "      <td>0.161095</td>\n",
       "      <td>-0.115967</td>\n",
       "      <td>0.018799</td>\n",
       "      <td>-0.083252</td>\n",
       "      <td>-0.061646</td>\n",
       "      <td>0.234863</td>\n",
       "      <td>-0.187012</td>\n",
       "      <td>0.211304</td>\n",
       "      <td>-0.074341</td>\n",
       "      <td>-0.036133</td>\n",
       "      <td>-0.095703</td>\n",
       "      <td>-0.222900</td>\n",
       "      <td>-0.005318</td>\n",
       "      <td>0.234863</td>\n",
       "      <td>-0.114136</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.228271</td>\n",
       "      <td>0.125244</td>\n",
       "      <td>-0.066895</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>-0.027344</td>\n",
       "      <td>-0.074341</td>\n",
       "      <td>-0.140503</td>\n",
       "      <td>-0.073059</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.123291</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.136688</td>\n",
       "      <td>-0.030518</td>\n",
       "      <td>-0.094360</td>\n",
       "      <td>-0.106689</td>\n",
       "      <td>-0.097900</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>-0.137268</td>\n",
       "      <td>-0.020630</td>\n",
       "      <td>0.015869</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>25</td>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5175</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6         7         8         9        10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49        50        51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72        73        74        75        76        77        78        79        80        81        82        83        84        85        86        87        88        89        90        91        92        93        94        95        96        97        98  \\\n",
       "49649  0.000830  0.050684 -0.078711  0.069385 -0.120630  0.095508  0.036621  0.002893  0.096631  0.027344  0.134375 -0.150244 -0.100488  0.123291  0.020605  0.104590  0.079565  0.159546 -0.014038 -0.019336  0.005188 -0.091040  0.018677  0.007471 -0.058362 -0.175696  0.052246 -0.047949  0.057214 -0.001563 -0.074683 -0.089453 -0.136575 -0.035522 -0.044775 -0.084277 -0.087115  0.092236 -0.074023  0.171582  0.055505 -0.031738  0.079736  0.000439  0.062964 -0.156643  0.037891  0.053223 -0.184277  0.015015 -0.042213  0.110443 -0.159143  0.075684  0.050726 -0.124219 -0.053876  0.066498  0.046753 -0.142627  0.001953  0.062012  0.155615 -0.119690  0.023047  0.107422 -0.012073  0.052051 -0.038714  0.096625  0.032764 -0.105762  0.075488 -0.013354 -0.159668 -0.022070 -0.127051 -0.002856  0.020410  0.096093  0.051074 -0.160498 -0.044824  0.037259  0.032520  0.038721 -0.170763  0.044312 -0.043982 -0.170801  0.001929 -0.107178 -0.018710 -0.023686  0.083203 -0.066431  0.240039  0.015967  0.080176   \n",
       "49650  0.073608  0.090820 -0.069580 -0.088684 -0.126465 -0.135132 -0.096680 -0.334961  0.136230  0.150879  0.071289 -0.105713  0.033203  0.161499 -0.276367 -0.014648 -0.018066 -0.157227  0.057129 -0.074219  0.015472  0.008789  0.104340  0.011719  0.035583 -0.005859  0.050232 -0.231201  0.066406  0.112549  0.104164 -0.083252 -0.042847 -0.089844 -0.002022 -0.015259  0.102051 -0.054443  0.074585  0.190308  0.149658  0.069946  0.135742  0.164062  0.017212 -0.115723  0.014465  0.089844  0.134766  0.052979  0.127441  0.041626 -0.037476  0.186035 -0.058472  0.020264 -0.024414 -0.101318  0.059448 -0.038818 -0.018188  0.194824 -0.150024  0.104996 -0.084717 -0.017090 -0.228516 -0.140137 -0.166992 -0.025635  0.056763  0.053955 -0.024170  0.106445 -0.187988 -0.006149  0.095215 -0.068298  0.083374 -0.022217  0.172363 -0.237793 -0.091675  0.073486 -0.096924 -0.298096 -0.056396  0.033203 -0.009766 -0.056641  0.118530 -0.108398 -0.208496  0.016373  0.094238  0.097534  0.108337  0.149597  0.069336   \n",
       "\n",
       "             99       100       101       102       103       104       105       106       107       108       109       110       111       112       113       114       115       116       117       118       119       120       121       122       123       124       125       126       127       128       129       130       131       132       133       134       135       136       137       138       139       140       141       142       143       144       145       146       147       148       149       150       151       152       153       154       155       156       157       158       159       160       161       162       163       164       165       166       167       168       169       170       171       172       173       174       175       176       177       178       179       180       181       182       183       184       185       186       187       188       189       190       191       192       193       194       195       196       197  \\\n",
       "49649  0.083057  0.085913  0.160815 -0.024756  0.089258  0.134863 -0.183105  0.093701 -0.032422 -0.030859 -0.171924 -0.070312 -0.000861  0.073975 -0.145459  0.006323 -0.124481 -0.067749 -0.072168  0.152429  0.090430 -0.165698  0.082666 -0.073535 -0.021973 -0.027893 -0.214258  0.093848  0.023438  0.018594  0.048389  0.057397  0.013354 -0.235205 -0.159955 -0.205176 -0.246289 -0.039508  0.218457 -0.163428  0.123743  0.073145 -0.007524 -0.051318  0.064874  0.069727 -0.036743 -0.072217 -0.016357 -0.172266 -0.006573  0.173486 -0.005371 -0.003857  0.003418 -0.031494 -0.094775  0.090625 -0.196759 -0.032446 -0.116797  0.097949  0.073145  0.187024 -0.023914 -0.021729 -0.093010  0.054492  0.035657 -0.014432 -0.093689  0.002148 -0.012500 -0.125958 -0.115039 -0.030200 -0.080908  0.124854 -0.017725  0.061914  0.010327 -0.130652 -0.037939  0.100732 -0.101514  0.065918  0.064258 -0.105762  0.006689  0.008521  0.134717 -0.051904  0.046875  0.099512  0.149072 -0.068091 -0.006396  0.045746 -0.036450   \n",
       "49650  0.243896 -0.097191 -0.044495  0.030579 -0.127319  0.216476 -0.082153 -0.138916 -0.220703  0.077881 -0.020264  0.236877 -0.114258 -0.154053  0.232910 -0.101074 -0.201172  0.088623  0.144287 -0.002197  0.283691  0.052490  0.333496 -0.053711  0.044067  0.132812 -0.104004 -0.086182 -0.043671  0.077148  0.148682  0.055725 -0.035156  0.074707 -0.068359 -0.011810  0.112152  0.275391 -0.065193 -0.439453  0.074341 -0.019775 -0.073242 -0.083008  0.150391 -0.065430  0.124695 -0.078613  0.101807 -0.030762 -0.225098  0.467041 -0.142578  0.070618  0.184814 -0.049103 -0.184082  0.024933  0.085571  0.273438 -0.153687 -0.020996  0.092285  0.117310 -0.215942 -0.115143  0.187378  0.262695 -0.063721  0.234375  0.202637  0.008789  0.166504 -0.168945 -0.224609 -0.033386  0.132812  0.225098 -0.079712 -0.113525  0.084961 -0.257324 -0.015869 -0.133789 -0.320312  0.165771 -0.292480 -0.020752  0.072754 -0.084839  0.069092  0.035198  0.018799  0.104736  0.017578 -0.112061  0.046021 -0.188477  0.086792   \n",
       "\n",
       "            198       199       200       201       202       203       204       205       206       207      208       209       210       211       212       213       214       215      216       217       218       219       220       221       222       223       224       225       226       227       228       229       230       231       232       233       234       235       236       237       238       239       240       241       242       243       244       245       246       247       248       249       250       251       252       253       254       255       256       257       258       259       260       261       262       263       264       265       266       267       268       269       270       271       272       273       274       275       276       277       278       279       280       281       282       283       284       285       286       287       288       289       290       291       292       293       294       295       296  \\\n",
       "49649 -0.158325 -0.030737  0.049097  0.045508 -0.149609  0.038666  0.093359  0.014551 -0.016589 -0.057178  0.11460 -0.064404 -0.166699  0.026172 -0.048755 -0.047766 -0.108203 -0.067676  0.05293  0.105859 -0.162329 -0.151929  0.009521 -0.007690  0.120850 -0.032813 -0.003711 -0.147778  0.150977 -0.049023 -0.017871  0.169336  0.044727 -0.014014 -0.022339 -0.033347  0.001709 -0.017822 -0.022852  0.067737  0.113086 -0.056299  0.023663 -0.058374  0.073743 -0.199902  0.022217  0.091113 -0.015039  0.064966 -0.006104 -0.209302  0.096289  0.058203  0.109180 -0.001855 -0.074133 -0.156067 -0.010223  0.082349 -0.107092  0.155078 -0.066010  0.070361 -0.042505  0.131006  0.076392 -0.072461 -0.046094  0.154224 -0.020947 -0.030371  0.039795  0.204150  0.089087 -0.191699 -0.013916 -0.139600 -0.072852 -0.053857 -0.142529  0.057910  0.062061 -0.052393  0.167090  0.058887 -0.076538 -0.095724 -0.230847  0.036865  0.106544  0.180566  0.070203  0.108008 -0.031592 -0.192676 -0.093750  0.075034  0.023315   \n",
       "49650 -0.155884  0.007324  0.155029  0.203003  0.062073 -0.023438  0.068481  0.030701  0.070190 -0.297363  0.20166 -0.021973 -0.176086 -0.018555  0.100830  0.077271 -0.143433  0.030304 -0.09021 -0.142334 -0.226318 -0.059570  0.246582 -0.049927  0.216553 -0.128418 -0.072021 -0.058594 -0.065308 -0.057007  0.020020  0.082642 -0.013641 -0.182129  0.018066  0.152588 -0.097046 -0.043701  0.066650 -0.152222  0.190308 -0.142578 -0.068115  0.157715 -0.093140 -0.064453 -0.188721  0.280273  0.023621 -0.118164 -0.089844  0.000305  0.078857 -0.029785 -0.040039  0.061401  0.183838  0.067871  0.260254 -0.113647  0.161095 -0.115967  0.018799 -0.083252 -0.061646  0.234863 -0.187012  0.211304 -0.074341 -0.036133 -0.095703 -0.222900 -0.005318  0.234863 -0.114136 -0.187500  0.228271  0.125244 -0.066895  0.083984 -0.018311 -0.027344 -0.074341 -0.140503 -0.073059  0.069946 -0.080322 -0.133789 -0.123291  0.000488  0.012451  0.136688 -0.030518 -0.094360 -0.106689 -0.097900  0.036133 -0.075684 -0.137268   \n",
       "\n",
       "            297       298       299  Top       Date  Label  topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  topic_7  topic_8  topic_9  sentiment  \n",
       "49649 -0.095508  0.155450  0.075000   24 2008-08-08      0   0.0295   0.0222   0.0178   0.0149   0.0128   0.0113     0.01   0.1759    0.698      0.0       -0.1  \n",
       "49650 -0.020630  0.015869  0.000488   25 2008-08-08      0   0.5175   0.0141   0.0113   0.0000   0.1980   0.2274     0.00   0.0000    0.000      0.0        0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "#Importar los datasets\n",
    "url_embeddings_average_individual = zipfile.ZipFile('../Data/average_bigram_topics_sentiment.zip')\n",
    "\n",
    "embeddings_average_individual = pd.read_csv(url_embeddings_average_individual.open('average_bigram_topics_sentiment.csv'), index_col = 0)\n",
    "\n",
    "embeddings_average_individual['Date'] =  pd.to_datetime(embeddings_average_individual['Date'], format='%Y-%m-%d')\n",
    "\n",
    "embeddings_average_individual.reset_index(inplace=True)\n",
    "embeddings_average_individual.fillna(0, inplace=True)\n",
    "embeddings_average_individual.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAXOJcEcbmed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 91 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono la fecha para la cual hago el corte de train y test\n",
    "training_end = pd.to_datetime(\"2013-12-31\")\n",
    "num_training = len(embeddings_average_individual[(embeddings_average_individual[\"Date\"]) <= training_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418076004775169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Top</th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9443</th>\n",
       "      <td>-0.063354</td>\n",
       "      <td>0.015055</td>\n",
       "      <td>-0.127116</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>-0.07666</td>\n",
       "      <td>-0.111328</td>\n",
       "      <td>-0.004476</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.148763</td>\n",
       "      <td>0.015381</td>\n",
       "      <td>0.095835</td>\n",
       "      <td>-0.082113</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>-0.063232</td>\n",
       "      <td>0.176595</td>\n",
       "      <td>0.047791</td>\n",
       "      <td>0.068827</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.039714</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>-0.130697</td>\n",
       "      <td>-0.138672</td>\n",
       "      <td>0.013018</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>-0.103597</td>\n",
       "      <td>0.046224</td>\n",
       "      <td>0.167643</td>\n",
       "      <td>0.028768</td>\n",
       "      <td>-0.177083</td>\n",
       "      <td>0.079997</td>\n",
       "      <td>-0.026204</td>\n",
       "      <td>-0.10555</td>\n",
       "      <td>-0.044434</td>\n",
       "      <td>0.032288</td>\n",
       "      <td>-0.023071</td>\n",
       "      <td>0.133301</td>\n",
       "      <td>0.076497</td>\n",
       "      <td>0.03304</td>\n",
       "      <td>0.009018</td>\n",
       "      <td>-0.049886</td>\n",
       "      <td>0.048991</td>\n",
       "      <td>-0.166972</td>\n",
       "      <td>-0.024577</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>0.059896</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.14091</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.062581</td>\n",
       "      <td>-0.09082</td>\n",
       "      <td>0.042013</td>\n",
       "      <td>-0.231608</td>\n",
       "      <td>0.029012</td>\n",
       "      <td>-0.036865</td>\n",
       "      <td>0.06665</td>\n",
       "      <td>0.036418</td>\n",
       "      <td>0.096842</td>\n",
       "      <td>-0.008626</td>\n",
       "      <td>-0.129781</td>\n",
       "      <td>-0.075521</td>\n",
       "      <td>-0.066447</td>\n",
       "      <td>-0.120361</td>\n",
       "      <td>-0.009572</td>\n",
       "      <td>0.056966</td>\n",
       "      <td>0.158529</td>\n",
       "      <td>0.16154</td>\n",
       "      <td>-0.03658</td>\n",
       "      <td>-0.037516</td>\n",
       "      <td>-0.003642</td>\n",
       "      <td>-0.131917</td>\n",
       "      <td>-0.201009</td>\n",
       "      <td>-0.108358</td>\n",
       "      <td>-0.025553</td>\n",
       "      <td>0.015381</td>\n",
       "      <td>-0.040436</td>\n",
       "      <td>0.02002</td>\n",
       "      <td>-0.031291</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>0.06901</td>\n",
       "      <td>-0.016602</td>\n",
       "      <td>0.061971</td>\n",
       "      <td>-0.024089</td>\n",
       "      <td>-0.119954</td>\n",
       "      <td>0.102397</td>\n",
       "      <td>-0.027995</td>\n",
       "      <td>0.075684</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.162069</td>\n",
       "      <td>-0.056152</td>\n",
       "      <td>-0.052409</td>\n",
       "      <td>-0.097493</td>\n",
       "      <td>0.134684</td>\n",
       "      <td>0.032633</td>\n",
       "      <td>-0.05717</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.025065</td>\n",
       "      <td>-0.104533</td>\n",
       "      <td>0.061686</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>-0.01062</td>\n",
       "      <td>0.011882</td>\n",
       "      <td>0.077637</td>\n",
       "      <td>0.152445</td>\n",
       "      <td>-0.071828</td>\n",
       "      <td>-0.009847</td>\n",
       "      <td>0.015299</td>\n",
       "      <td>-0.055908</td>\n",
       "      <td>0.00061</td>\n",
       "      <td>-0.220062</td>\n",
       "      <td>-0.024801</td>\n",
       "      <td>0.030268</td>\n",
       "      <td>0.121989</td>\n",
       "      <td>0.059082</td>\n",
       "      <td>0.096293</td>\n",
       "      <td>-0.104167</td>\n",
       "      <td>-0.1014</td>\n",
       "      <td>-0.065104</td>\n",
       "      <td>0.127686</td>\n",
       "      <td>-0.031738</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.182292</td>\n",
       "      <td>-0.149495</td>\n",
       "      <td>0.057251</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.085836</td>\n",
       "      <td>-0.180339</td>\n",
       "      <td>-0.055257</td>\n",
       "      <td>0.009542</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>-0.265706</td>\n",
       "      <td>0.073486</td>\n",
       "      <td>-0.054606</td>\n",
       "      <td>-0.061198</td>\n",
       "      <td>0.127223</td>\n",
       "      <td>0.051229</td>\n",
       "      <td>-0.170736</td>\n",
       "      <td>-0.05245</td>\n",
       "      <td>0.066813</td>\n",
       "      <td>0.055216</td>\n",
       "      <td>0.15918</td>\n",
       "      <td>-0.208008</td>\n",
       "      <td>-0.182129</td>\n",
       "      <td>-0.025879</td>\n",
       "      <td>-0.073324</td>\n",
       "      <td>-0.01237</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.051432</td>\n",
       "      <td>0.126363</td>\n",
       "      <td>-0.091227</td>\n",
       "      <td>-0.164185</td>\n",
       "      <td>-0.067464</td>\n",
       "      <td>-0.015137</td>\n",
       "      <td>-0.085531</td>\n",
       "      <td>0.055094</td>\n",
       "      <td>-0.141439</td>\n",
       "      <td>0.014974</td>\n",
       "      <td>-0.056966</td>\n",
       "      <td>-0.028809</td>\n",
       "      <td>-0.027262</td>\n",
       "      <td>-0.048706</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>-0.163574</td>\n",
       "      <td>-0.007894</td>\n",
       "      <td>-0.114258</td>\n",
       "      <td>-0.158203</td>\n",
       "      <td>-0.058512</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>-0.056844</td>\n",
       "      <td>-0.053385</td>\n",
       "      <td>0.063354</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>-0.142253</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>-0.045736</td>\n",
       "      <td>-0.042033</td>\n",
       "      <td>0.046987</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>-0.062826</td>\n",
       "      <td>-0.066081</td>\n",
       "      <td>-0.04777</td>\n",
       "      <td>0.038656</td>\n",
       "      <td>0.033529</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.082642</td>\n",
       "      <td>-0.078125</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.036214</td>\n",
       "      <td>-0.152832</td>\n",
       "      <td>0.25472</td>\n",
       "      <td>-0.013672</td>\n",
       "      <td>-0.063721</td>\n",
       "      <td>-0.125651</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>-0.002991</td>\n",
       "      <td>0.078242</td>\n",
       "      <td>-0.183431</td>\n",
       "      <td>-0.183838</td>\n",
       "      <td>0.157064</td>\n",
       "      <td>0.077474</td>\n",
       "      <td>-0.017049</td>\n",
       "      <td>0.025187</td>\n",
       "      <td>-0.011759</td>\n",
       "      <td>-0.11084</td>\n",
       "      <td>-0.094686</td>\n",
       "      <td>0.106974</td>\n",
       "      <td>-0.17806</td>\n",
       "      <td>0.056348</td>\n",
       "      <td>-0.158773</td>\n",
       "      <td>0.05306</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>0.140951</td>\n",
       "      <td>-0.034424</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>-0.057048</td>\n",
       "      <td>-0.156576</td>\n",
       "      <td>-0.014974</td>\n",
       "      <td>0.191121</td>\n",
       "      <td>0.015574</td>\n",
       "      <td>-0.012594</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>-0.000814</td>\n",
       "      <td>-0.024333</td>\n",
       "      <td>0.123698</td>\n",
       "      <td>0.172526</td>\n",
       "      <td>-0.055766</td>\n",
       "      <td>-0.105957</td>\n",
       "      <td>-0.061605</td>\n",
       "      <td>0.05485</td>\n",
       "      <td>0.08846</td>\n",
       "      <td>0.043294</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>0.105347</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>-0.017853</td>\n",
       "      <td>-0.113118</td>\n",
       "      <td>-0.303304</td>\n",
       "      <td>-0.09789</td>\n",
       "      <td>0.066203</td>\n",
       "      <td>0.056315</td>\n",
       "      <td>-0.090515</td>\n",
       "      <td>-0.113118</td>\n",
       "      <td>0.047811</td>\n",
       "      <td>-0.075928</td>\n",
       "      <td>-0.015381</td>\n",
       "      <td>0.018717</td>\n",
       "      <td>0.018717</td>\n",
       "      <td>-0.019368</td>\n",
       "      <td>-0.086304</td>\n",
       "      <td>-0.051595</td>\n",
       "      <td>0.061526</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>-0.060476</td>\n",
       "      <td>0.118571</td>\n",
       "      <td>0.104329</td>\n",
       "      <td>-0.006226</td>\n",
       "      <td>0.249349</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.076986</td>\n",
       "      <td>0.111287</td>\n",
       "      <td>-0.207357</td>\n",
       "      <td>-0.124064</td>\n",
       "      <td>-0.05837</td>\n",
       "      <td>0.099202</td>\n",
       "      <td>0.141357</td>\n",
       "      <td>0.039876</td>\n",
       "      <td>0.09493</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>-0.138835</td>\n",
       "      <td>-0.245687</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.069092</td>\n",
       "      <td>0.081706</td>\n",
       "      <td>0.022705</td>\n",
       "      <td>-0.067546</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.088623</td>\n",
       "      <td>0.109782</td>\n",
       "      <td>-0.175354</td>\n",
       "      <td>0.104126</td>\n",
       "      <td>-0.074707</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.067464</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>-0.053955</td>\n",
       "      <td>0.056478</td>\n",
       "      <td>0.055176</td>\n",
       "      <td>-0.096842</td>\n",
       "      <td>-0.020833</td>\n",
       "      <td>0.071859</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.068522</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9113</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3        4         5         6         7         8         9        10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32       33        34        35        36        37        38       39        40        41        42        43        44        45        46        47       48      49        50        51        52       53        54        55        56        57       58        59        60        61        62        63        64        65        66        67        68       69       70        71        72        73        74        75        76        77        78       79        80        81       82        83        84        85        86        87        88        89        90        91        92        93        94        95        96        97       98        99  \\\n",
       "9443 -0.063354  0.015055 -0.127116  0.055664 -0.07666 -0.111328 -0.004476 -0.130859  0.148763  0.015381  0.095835 -0.082113  0.023112  0.041992 -0.063232  0.176595  0.047791  0.068827  0.000814  0.072917  0.039714  0.004801 -0.130697 -0.138672  0.013018  0.010295 -0.103597  0.046224  0.167643  0.028768 -0.177083  0.079997 -0.026204 -0.10555 -0.044434  0.032288 -0.023071  0.133301  0.076497  0.03304  0.009018 -0.049886  0.048991 -0.166972 -0.024577 -0.147461  0.059896  0.010742  0.14091  0.0271 -0.012817  0.015625  0.062581 -0.09082  0.042013 -0.231608  0.029012 -0.036865  0.06665  0.036418  0.096842 -0.008626 -0.129781 -0.075521 -0.066447 -0.120361 -0.009572  0.056966  0.158529  0.16154 -0.03658 -0.037516 -0.003642 -0.131917 -0.201009 -0.108358 -0.025553  0.015381 -0.040436  0.02002 -0.031291 -0.091309  0.06901 -0.016602  0.061971 -0.024089 -0.119954  0.102397 -0.027995  0.075684  0.003418 -0.054688 -0.162069 -0.056152 -0.052409 -0.097493  0.134684  0.032633 -0.05717  0.018066   \n",
       "\n",
       "           100       101       102       103       104      105       106       107       108       109       110       111       112      113       114       115       116       117       118       119       120     121       122       123       124       125       126       127       128       129       130       131       132       133       134       135       136       137       138       139       140       141      142       143       144      145       146       147       148       149      150       151       152       153       154       155       156       157       158       159       160       161       162       163       164       165       166       167       168       169       170       171       172       173       174       175       176       177       178       179       180       181       182       183       184       185      186       187       188       189       190       191       192       193       194      195       196       197       198       199  \\\n",
       "9443  0.025065 -0.104533  0.061686  0.041504 -0.013387 -0.01062  0.011882  0.077637  0.152445 -0.071828 -0.009847  0.015299 -0.055908  0.00061 -0.220062 -0.024801  0.030268  0.121989  0.059082  0.096293 -0.104167 -0.1014 -0.065104  0.127686 -0.031738 -0.002238 -0.182292 -0.149495  0.057251  0.097656  0.085836 -0.180339 -0.055257  0.009542  0.064941 -0.265706  0.073486 -0.054606 -0.061198  0.127223  0.051229 -0.170736 -0.05245  0.066813  0.055216  0.15918 -0.208008 -0.182129 -0.025879 -0.073324 -0.01237  0.035156 -0.051432  0.126363 -0.091227 -0.164185 -0.067464 -0.015137 -0.085531  0.055094 -0.141439  0.014974 -0.056966 -0.028809 -0.027262 -0.048706  0.064209 -0.163574 -0.007894 -0.114258 -0.158203 -0.058512  0.029785 -0.056844 -0.053385  0.063354  0.063477  0.014608 -0.142253  0.080078 -0.045736 -0.042033  0.046987 -0.000651 -0.062826 -0.066081 -0.04777  0.038656  0.033529  0.038778  0.082642 -0.078125  0.050781  0.036214 -0.152832  0.25472 -0.013672 -0.063721 -0.125651 -0.277344   \n",
       "\n",
       "           200       201       202       203       204       205       206       207       208      209       210       211      212       213       214      215       216       217       218       219       220       221       222       223       224       225       226       227       228       229       230       231       232       233      234      235       236       237       238       239     240       241       242       243      244       245       246       247       248       249       250       251       252       253       254       255       256       257       258       259       260       261       262       263       264       265       266       267       268      269       270       271       272      273       274       275       276       277       278       279       280       281       282       283       284       285       286       287       288       289       290       291       292       293       294       295       296       297       298       299  \\\n",
       "9443 -0.002991  0.078242 -0.183431 -0.183838  0.157064  0.077474 -0.017049  0.025187 -0.011759 -0.11084 -0.094686  0.106974 -0.17806  0.056348 -0.158773  0.05306  0.239258  0.140951 -0.034424 -0.002797 -0.057048 -0.156576 -0.014974  0.191121  0.015574 -0.012594 -0.008545 -0.000814 -0.024333  0.123698  0.172526 -0.055766 -0.105957 -0.061605  0.05485  0.08846  0.043294  0.020447  0.105347 -0.008301 -0.0118 -0.017853 -0.113118 -0.303304 -0.09789  0.066203  0.056315 -0.090515 -0.113118  0.047811 -0.075928 -0.015381  0.018717  0.018717 -0.019368 -0.086304 -0.051595  0.061526  0.036438 -0.060476  0.118571  0.104329 -0.006226  0.249349  0.008057  0.076986  0.111287 -0.207357 -0.124064 -0.05837  0.099202  0.141357  0.039876  0.09493 -0.094238  0.105469 -0.138835 -0.245687 -0.046631  0.069092  0.081706  0.022705 -0.067546  0.114583  0.088623  0.109782 -0.175354  0.104126 -0.074707  0.083984  0.067464  0.001546 -0.053955  0.056478  0.055176 -0.096842 -0.020833  0.071859  0.132812 -0.068522   \n",
       "\n",
       "      Top       Date  Label  topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  topic_7  topic_8  topic_9  sentiment  \n",
       "9443    1 2014-12-31      0   0.9113   0.0174   0.0139   0.0116      0.0      0.0      0.0      0.0      0.0      0.0        0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 484 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono el archivo con el que se corre el modelo\n",
    "data = embeddings_average_individual[embeddings_average_individual['Date']<='2014-12-31']\n",
    "print(data['Label'].mean())\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6294, 1, 311)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 143 ms\n"
     ]
    }
   ],
   "source": [
    "training = data[:num_training]\n",
    "testing = data[num_training:]\n",
    "\n",
    "# Se separa en train y test\n",
    "x_train = data.drop([\"Top\",\"Label\", \"Date\"], axis=1)[:num_training]\n",
    "x_test = data.drop([\"Top\",'Label', 'Date'], axis=1)[num_training:]\n",
    "y_train = data[\"Label\"].values[:num_training]\n",
    "y_test = data[\"Label\"].values[num_training:]\n",
    "\n",
    "\n",
    "x_train_array = x_train.to_numpy()\n",
    "reshape_x_train = x_train_array.reshape(len(x_train), 1, x_train.shape[1])\n",
    "reshape_x_train.shape\n",
    "\n",
    "x_test_array = x_test.to_numpy()\n",
    "reshape_x_test = x_test_array.reshape(len(x_test), 1, x_train.shape[1])\n",
    "reshape_x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir espacio de busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 57 ms\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'units1': hp.choice('units1', [8, 16, 32, 64, 128, 256, 512]),\n",
    "    'units2': hp.choice('units2', [8, 16, 32, 64, 128, 256, 512]),\n",
    "    'units3': hp.choice('units3', [8, 16, 32, 64, 128, 256, 512]),\n",
    "                 \n",
    "    'dropout1': hp.choice('dropout1', [0.1,0.2,0.3, 0.4]),\n",
    "    'dropout2': hp.choice('dropout2', [0.1,0.2,0.3, 0.4]),\n",
    "\n",
    "    'batch_size' : hp.choice('batch_size', [128,256,512]),\n",
    "    \n",
    "    'nb_epochs' : hp.choice('nb_epochs', [50]),\n",
    "\n",
    "    'optimizer':  hp.choice('optimizer', [ 'adam','adadelta']),   \n",
    "    'activation': hp.choice('activation', [ 'relu','sigmoid','softmax']), \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir busqueda bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "#Objective function that hyperopt will minimize\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def objective(params):\n",
    "    \n",
    "#     import ml_metrics\n",
    "\n",
    "    \n",
    "    start = timer()\n",
    "    print ('Params testing: ', params)\n",
    "    print ('\\n ')\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(CuDNNGRU(128, return_sequences = True)))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "    model.add(Bidirectional(CuDNNGRU(params['units1'])))\n",
    "    model.add(Dropout(params['dropout2']))\n",
    "    model.add(Dense(params['units2'], activation='relu'))\n",
    "    model.add(Dense(params['units3'], activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(params['optimizer'], loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    logdir = \"Resultados\\\\\" + exp_name +\"\\\\logs\\\\model\"\n",
    "\n",
    "    tensor_board = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, profile_batch = 100000000)\n",
    "\n",
    "    \n",
    "    #includes the call back object\n",
    "    model.fit(reshape_x_train, y_train, epochs=params['nb_epochs'], batch_size=params['batch_size'],\n",
    "                verbose = 0, validation_data=(reshape_x_test, y_test),callbacks=[tensor_board])\n",
    "     \n",
    "    #predict the test set \n",
    "    ypred = model.predict_proba(reshape_x_test)\n",
    "    testing_cp = testing.copy()\n",
    "    testing_cp['Prob'] = ypred\n",
    "    testing_cp['Prob_dia'] = testing_cp['Prob'].groupby(testing_cp['Date']).transform('mean')\n",
    "    testing_cp['Prediction'] = 0\n",
    "    testing_cp.loc[testing_cp['Prob_dia']>0.5, 'Prediction'] = 1\n",
    "    testing_cp.drop_duplicates(subset=['Date','Prediction','Label'], inplace=True)\n",
    "    \n",
    "    acc = accuracy_score(testing_cp.Label, testing_cp.Prediction)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([-acc, params, run_time])\n",
    "    of_connection.close()\n",
    "    \n",
    "    \n",
    "    print('Test accuracy:', acc)\n",
    " \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'train_time': run_time,}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenar resultados de cada iteracin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import tpe\n",
    "\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 143 ms\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from hyperopt import Trials\n",
    "\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# File to save first results\n",
    "out_file = folder + '/gbm_results.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanzar optimizacin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params testing:                                                                                                        \n",
      "{'activation': 'softmax', 'batch_size': 512, 'dropout1': 0.3, 'dropout2': 0.2, 'nb_epochs': 50, 'optimizer': 'adadelta', 'units1': 32, 'units2': 256, 'units3': 32}\n",
      "Test accuracy:                                                                                                         \n",
      "0.4901185770750988                                                                                                     \n",
      "Params testing:                                                                                                        \n",
      "{'activation': 'softmax', 'batch_size': 256, 'dropout1': 0.4, 'dropout2': 0.4, 'nb_epochs': 50, 'optimizer': 'adadelta', 'units1': 512, 'units2': 16, 'units3': 8}\n",
      "Test accuracy:                                                                                                         \n",
      "0.4901185770750988                                                                                                     \n",
      "100%|| 2/2 [07:51<00:00, 235.63s/trial, best loss: -0.4901185770750988]\n",
      "time: 7min 51s\n"
     ]
    }
   ],
   "source": [
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = 2, trials = bayes_trials,\n",
    "            verbose = 1, rstate= np.random.RandomState(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar bayesiana, por si quisiera retomar donde queda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 52 ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(bayes_trials, open(folder + '/trials.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer mejores parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': -0.4901185770750988,\n",
       "  'status': 'ok',\n",
       "  'train_time': 101.01375379999999},\n",
       " {'loss': -0.4901185770750988, 'status': 'ok', 'train_time': 370.1786396}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 440 ms\n"
     ]
    }
   ],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "bayes_trials_results  = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results [:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.490119</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 512, '...</td>\n",
       "      <td>101.013754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.490119</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 256, '...</td>\n",
       "      <td>370.178640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                             params        time\n",
       "0 -0.490119  {'activation': 'softmax', 'batch_size': 512, '...  101.013754\n",
       "1 -0.490119  {'activation': 'softmax', 'batch_size': 256, '...  370.178640"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 402 ms\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(folder + '/gbm_results.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'softmax',\n",
       " 'batch_size': 512,\n",
       " 'dropout1': 0.3,\n",
       " 'dropout2': 0.2,\n",
       " 'nb_epochs': 50,\n",
       " 'optimizer': 'adadelta',\n",
       " 'units1': 32,\n",
       " 'units2': 256,\n",
       " 'units3': 32}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 430 ms\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'softmax',\n",
       " 'batch_size': 512,\n",
       " 'dropout1': 0.3,\n",
       " 'dropout2': 0.2,\n",
       " 'nb_epochs': 50,\n",
       " 'optimizer': 'adadelta',\n",
       " 'units1': 32,\n",
       " 'units2': 256,\n",
       " 'units3': 32}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 467 ms\n"
     ]
    }
   ],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "best_bayes_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir datasets de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 403 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono la fecha para la cual hago el corte de train y test\n",
    "training_end = pd.to_datetime(\"2014-12-31\")\n",
    "num_training = len(embeddings_average_individual[(embeddings_average_individual[\"Date\"]) <= training_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Top</th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.02655</td>\n",
       "      <td>-0.005437</td>\n",
       "      <td>-0.049437</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>-0.08665</td>\n",
       "      <td>0.110962</td>\n",
       "      <td>0.086507</td>\n",
       "      <td>0.01122</td>\n",
       "      <td>-0.123851</td>\n",
       "      <td>-0.069445</td>\n",
       "      <td>0.046326</td>\n",
       "      <td>-0.092684</td>\n",
       "      <td>0.105031</td>\n",
       "      <td>0.078355</td>\n",
       "      <td>0.042023</td>\n",
       "      <td>-0.022827</td>\n",
       "      <td>-0.063538</td>\n",
       "      <td>-0.094391</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>-0.017698</td>\n",
       "      <td>-0.034032</td>\n",
       "      <td>0.052694</td>\n",
       "      <td>-0.043579</td>\n",
       "      <td>-0.059591</td>\n",
       "      <td>0.028366</td>\n",
       "      <td>0.14961</td>\n",
       "      <td>-0.076497</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>-0.050603</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.04453</td>\n",
       "      <td>-0.023275</td>\n",
       "      <td>-0.015015</td>\n",
       "      <td>0.078247</td>\n",
       "      <td>-0.093613</td>\n",
       "      <td>-0.037659</td>\n",
       "      <td>0.036929</td>\n",
       "      <td>0.054586</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>0.146851</td>\n",
       "      <td>-0.059499</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>-0.150101</td>\n",
       "      <td>0.038727</td>\n",
       "      <td>0.11497</td>\n",
       "      <td>-0.060374</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>-0.03419</td>\n",
       "      <td>-0.04878</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>0.031077</td>\n",
       "      <td>0.054706</td>\n",
       "      <td>-0.023376</td>\n",
       "      <td>-0.059611</td>\n",
       "      <td>-0.116048</td>\n",
       "      <td>0.019796</td>\n",
       "      <td>-0.092779</td>\n",
       "      <td>0.069397</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>-0.044083</td>\n",
       "      <td>-0.059352</td>\n",
       "      <td>0.062612</td>\n",
       "      <td>-0.028783</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.037933</td>\n",
       "      <td>-0.076304</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.004979</td>\n",
       "      <td>0.091044</td>\n",
       "      <td>0.03831</td>\n",
       "      <td>0.056834</td>\n",
       "      <td>-0.062567</td>\n",
       "      <td>-0.011286</td>\n",
       "      <td>0.048192</td>\n",
       "      <td>0.126668</td>\n",
       "      <td>0.06871</td>\n",
       "      <td>0.169627</td>\n",
       "      <td>0.064963</td>\n",
       "      <td>-0.069956</td>\n",
       "      <td>-0.055331</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>-0.10978</td>\n",
       "      <td>-0.008942</td>\n",
       "      <td>-0.049391</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0.043355</td>\n",
       "      <td>0.040724</td>\n",
       "      <td>0.062861</td>\n",
       "      <td>-0.090495</td>\n",
       "      <td>-0.065338</td>\n",
       "      <td>-0.054422</td>\n",
       "      <td>0.01357</td>\n",
       "      <td>0.06425</td>\n",
       "      <td>0.084117</td>\n",
       "      <td>0.069021</td>\n",
       "      <td>-0.048922</td>\n",
       "      <td>-0.04539</td>\n",
       "      <td>-0.105464</td>\n",
       "      <td>0.040438</td>\n",
       "      <td>-0.05098</td>\n",
       "      <td>-0.042369</td>\n",
       "      <td>-0.259115</td>\n",
       "      <td>-0.031316</td>\n",
       "      <td>-0.110067</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>-0.033813</td>\n",
       "      <td>0.068558</td>\n",
       "      <td>0.008415</td>\n",
       "      <td>0.065531</td>\n",
       "      <td>-0.009593</td>\n",
       "      <td>-0.06676</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>-0.025653</td>\n",
       "      <td>-0.080393</td>\n",
       "      <td>0.055687</td>\n",
       "      <td>0.082654</td>\n",
       "      <td>-0.020147</td>\n",
       "      <td>-0.07518</td>\n",
       "      <td>-0.099518</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>0.040599</td>\n",
       "      <td>0.012319</td>\n",
       "      <td>0.06191</td>\n",
       "      <td>-0.037506</td>\n",
       "      <td>0.02595</td>\n",
       "      <td>0.158768</td>\n",
       "      <td>0.070648</td>\n",
       "      <td>-0.050069</td>\n",
       "      <td>-0.090535</td>\n",
       "      <td>-0.075063</td>\n",
       "      <td>-0.069895</td>\n",
       "      <td>-0.022481</td>\n",
       "      <td>-0.012672</td>\n",
       "      <td>-0.089986</td>\n",
       "      <td>-0.050354</td>\n",
       "      <td>0.162383</td>\n",
       "      <td>-0.022855</td>\n",
       "      <td>-0.096497</td>\n",
       "      <td>-0.021907</td>\n",
       "      <td>0.029973</td>\n",
       "      <td>-0.092219</td>\n",
       "      <td>0.045115</td>\n",
       "      <td>-0.038025</td>\n",
       "      <td>-0.053009</td>\n",
       "      <td>-0.085876</td>\n",
       "      <td>-0.159342</td>\n",
       "      <td>0.082438</td>\n",
       "      <td>0.062836</td>\n",
       "      <td>-0.054595</td>\n",
       "      <td>-0.007467</td>\n",
       "      <td>0.022705</td>\n",
       "      <td>-0.07308</td>\n",
       "      <td>-0.130718</td>\n",
       "      <td>-0.04321</td>\n",
       "      <td>-0.057373</td>\n",
       "      <td>-0.01974</td>\n",
       "      <td>-0.043142</td>\n",
       "      <td>0.01186</td>\n",
       "      <td>0.069222</td>\n",
       "      <td>-0.02478</td>\n",
       "      <td>-0.002594</td>\n",
       "      <td>-0.140828</td>\n",
       "      <td>0.052887</td>\n",
       "      <td>-0.054372</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.089589</td>\n",
       "      <td>-0.046539</td>\n",
       "      <td>-0.08475</td>\n",
       "      <td>-0.069346</td>\n",
       "      <td>-0.071147</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>-0.05452</td>\n",
       "      <td>0.116689</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>-0.027476</td>\n",
       "      <td>-0.040019</td>\n",
       "      <td>-0.104167</td>\n",
       "      <td>-0.114581</td>\n",
       "      <td>0.024998</td>\n",
       "      <td>0.053182</td>\n",
       "      <td>-0.035671</td>\n",
       "      <td>0.055969</td>\n",
       "      <td>-0.023046</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.02793</td>\n",
       "      <td>0.093552</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.043393</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.069979</td>\n",
       "      <td>0.029744</td>\n",
       "      <td>0.053935</td>\n",
       "      <td>0.099854</td>\n",
       "      <td>-0.062459</td>\n",
       "      <td>-0.086884</td>\n",
       "      <td>-0.032506</td>\n",
       "      <td>-0.041692</td>\n",
       "      <td>0.032838</td>\n",
       "      <td>-0.032617</td>\n",
       "      <td>0.01623</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>-0.093465</td>\n",
       "      <td>-0.01297</td>\n",
       "      <td>0.085303</td>\n",
       "      <td>0.149264</td>\n",
       "      <td>-0.11233</td>\n",
       "      <td>-0.01181</td>\n",
       "      <td>0.118187</td>\n",
       "      <td>-0.024419</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>-0.171748</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>0.078379</td>\n",
       "      <td>-0.029582</td>\n",
       "      <td>-0.053752</td>\n",
       "      <td>0.04306</td>\n",
       "      <td>-0.01981</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.044607</td>\n",
       "      <td>-0.041189</td>\n",
       "      <td>0.126353</td>\n",
       "      <td>-0.083649</td>\n",
       "      <td>0.070443</td>\n",
       "      <td>0.065664</td>\n",
       "      <td>-0.031071</td>\n",
       "      <td>-0.076167</td>\n",
       "      <td>0.022043</td>\n",
       "      <td>-0.056544</td>\n",
       "      <td>-0.033414</td>\n",
       "      <td>-0.07841</td>\n",
       "      <td>-0.007584</td>\n",
       "      <td>0.028048</td>\n",
       "      <td>0.047563</td>\n",
       "      <td>-0.03553</td>\n",
       "      <td>0.097061</td>\n",
       "      <td>-0.034648</td>\n",
       "      <td>0.016682</td>\n",
       "      <td>-0.11674</td>\n",
       "      <td>-0.016691</td>\n",
       "      <td>-0.115661</td>\n",
       "      <td>-0.014822</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>-0.002602</td>\n",
       "      <td>0.033976</td>\n",
       "      <td>0.067755</td>\n",
       "      <td>-0.045471</td>\n",
       "      <td>0.069234</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>-0.015391</td>\n",
       "      <td>0.020459</td>\n",
       "      <td>0.020137</td>\n",
       "      <td>0.049369</td>\n",
       "      <td>0.018361</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>-0.077982</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>-0.023763</td>\n",
       "      <td>-0.021281</td>\n",
       "      <td>-0.099264</td>\n",
       "      <td>-0.009176</td>\n",
       "      <td>0.163818</td>\n",
       "      <td>0.149923</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>0.049657</td>\n",
       "      <td>-0.090281</td>\n",
       "      <td>-0.018667</td>\n",
       "      <td>0.099538</td>\n",
       "      <td>0.063007</td>\n",
       "      <td>0.127879</td>\n",
       "      <td>0.02595</td>\n",
       "      <td>0.134786</td>\n",
       "      <td>-0.101334</td>\n",
       "      <td>-0.123652</td>\n",
       "      <td>-0.076009</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>-0.029872</td>\n",
       "      <td>0.062391</td>\n",
       "      <td>-0.020111</td>\n",
       "      <td>0.026784</td>\n",
       "      <td>0.067586</td>\n",
       "      <td>-0.012716</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>-0.00868</td>\n",
       "      <td>0.096343</td>\n",
       "      <td>0.052673</td>\n",
       "      <td>0.040049</td>\n",
       "      <td>-0.090737</td>\n",
       "      <td>-0.015834</td>\n",
       "      <td>0.056478</td>\n",
       "      <td>-0.03123</td>\n",
       "      <td>-0.096171</td>\n",
       "      <td>-0.045821</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>0.038754</td>\n",
       "      <td>0.031172</td>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.287754</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>0.490658</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6        7         8         9       10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27       28        29        30        31       32       33        34        35        36        37        38        39        40      41        42        43      44        45        46       47        48        49       50       51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68     69        70        71       72        73        74        75        76        77       78        79        80        81        82        83       84        85        86        87      88        89        90        91        92        93        94       95       96        97        98        99      100       101  \\\n",
       "0  0.02655 -0.005437 -0.049437  0.075195  0.007438  0.003312  0.160296 -0.08665  0.110962  0.086507  0.01122 -0.123851 -0.069445  0.046326 -0.092684  0.105031  0.078355  0.042023 -0.022827 -0.063538 -0.094391 -0.001472 -0.017698 -0.034032  0.052694 -0.043579 -0.059591  0.028366  0.14961 -0.076497 -0.001678 -0.050603 -0.16626 -0.04453 -0.023275 -0.015015  0.078247 -0.093613 -0.037659  0.036929  0.054586 -0.0861  0.146851 -0.059499  0.0556 -0.150101  0.038727  0.11497 -0.060374  0.033613 -0.03419 -0.04878 -0.002706  0.031077  0.054706 -0.023376 -0.059611 -0.116048  0.019796 -0.092779  0.069397  0.028481 -0.044083 -0.059352  0.062612 -0.028783  0.004842  0.037933 -0.076304  0.046 -0.004979  0.091044  0.03831  0.056834 -0.062567 -0.011286  0.048192  0.126668  0.06871  0.169627  0.064963 -0.069956 -0.055331  0.010803 -0.10978 -0.008942 -0.049391  0.071823  0.0896  0.043355  0.040724  0.062861 -0.090495 -0.065338 -0.054422  0.01357  0.06425  0.084117  0.069021 -0.048922 -0.04539 -0.105464   \n",
       "\n",
       "        102      103       104       105       106       107       108       109       110       111       112       113      114       115       116       117       118       119       120      121       122       123       124       125      126       127      128       129       130       131       132       133       134       135       136       137       138       139       140       141       142       143       144       145       146       147       148       149       150       151       152       153       154      155       156      157       158      159       160      161       162      163       164       165       166       167       168       169       170      171       172       173       174      175       176       177       178       179       180       181       182       183       184       185       186       187      188       189       190       191       192       193       194       195       196       197       198       199       200       201  \\\n",
       "0  0.040438 -0.05098 -0.042369 -0.259115 -0.031316 -0.110067  0.004466 -0.033813  0.068558  0.008415  0.065531 -0.009593 -0.06676  0.004476 -0.025653 -0.080393  0.055687  0.082654 -0.020147 -0.07518 -0.099518  0.007182  0.040599  0.012319  0.06191 -0.037506  0.02595  0.158768  0.070648 -0.050069 -0.090535 -0.075063 -0.069895 -0.022481 -0.012672 -0.089986 -0.050354  0.162383 -0.022855 -0.096497 -0.021907  0.029973 -0.092219  0.045115 -0.038025 -0.053009 -0.085876 -0.159342  0.082438  0.062836 -0.054595 -0.007467  0.022705 -0.07308 -0.130718 -0.04321 -0.057373 -0.01974 -0.043142  0.01186  0.069222 -0.02478 -0.002594 -0.140828  0.052887 -0.054372  0.000005 -0.089589 -0.046539 -0.08475 -0.069346 -0.071147 -0.012772 -0.05452  0.116689  0.066132 -0.027476 -0.040019 -0.104167 -0.114581  0.024998  0.053182 -0.035671  0.055969 -0.023046 -0.008371  0.02793  0.093552  0.104046  0.043393  0.020592  0.069979  0.029744  0.053935  0.099854 -0.062459 -0.086884 -0.032506 -0.041692  0.032838   \n",
       "\n",
       "        202      203       204       205      206       207       208      209      210       211       212       213       214       215       216       217       218      219      220       221       222       223       224       225       226       227       228       229       230       231       232      233       234       235       236      237       238       239       240      241       242       243       244       245       246       247       248       249       250       251       252       253       254       255       256       257       258       259       260       261       262       263       264       265       266       267       268       269       270       271       272      273       274       275       276       277       278       279       280       281       282       283       284       285       286       287      288       289       290       291       292       293       294      295       296       297       298       299  Top       Date  Label  \\\n",
       "0 -0.032617  0.01623  0.090942 -0.093465 -0.01297  0.085303  0.149264 -0.11233 -0.01181  0.118187 -0.024419  0.003143 -0.171748  0.052714  0.078379 -0.029582 -0.053752  0.04306 -0.01981 -0.013031  0.044607 -0.041189  0.126353 -0.083649  0.070443  0.065664 -0.031071 -0.076167  0.022043 -0.056544 -0.033414 -0.07841 -0.007584  0.028048  0.047563 -0.03553  0.097061 -0.034648  0.016682 -0.11674 -0.016691 -0.115661 -0.014822  0.038366 -0.002602  0.033976  0.067755 -0.045471  0.069234  0.088664 -0.015391  0.020459  0.020137  0.049369  0.018361  0.014715 -0.077982  0.004481 -0.023763 -0.021281 -0.099264 -0.009176  0.163818  0.149923  0.010976  0.049657 -0.090281 -0.018667  0.099538  0.063007  0.127879  0.02595  0.134786 -0.101334 -0.123652 -0.076009  0.004648 -0.029872  0.062391 -0.020111  0.026784  0.067586 -0.012716  0.014837  0.008809  0.018158 -0.00868  0.096343  0.052673  0.040049 -0.090737 -0.015834  0.056478 -0.03123 -0.096171 -0.045821  0.025391  0.006185    1 2016-07-01      1   \n",
       "\n",
       "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5  topic_6   topic_7   topic_8   topic_9  sentiment  \n",
       "0  0.051395  0.038754  0.031172  0.026116  0.022466  0.019712  0.01756  0.287754  0.014414  0.490658   0.033333  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 573 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono el archivo con el que se corre el modelo\n",
    "data = embeddings_average_individual\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9443, 1, 311)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 340 ms\n"
     ]
    }
   ],
   "source": [
    "training = data[:num_training]\n",
    "testing = data[num_training:]\n",
    "\n",
    "# Se separa en train y test\n",
    "x_train = data.drop([\"Top\",\"Label\", \"Date\"], axis=1)[:num_training]\n",
    "x_test = data.drop([\"Top\",'Label', 'Date'], axis=1)[num_training:]\n",
    "y_train = data[\"Label\"].values[:num_training]\n",
    "y_test = data[\"Label\"].values[num_training:]\n",
    "\n",
    "\n",
    "x_train_array = x_train.to_numpy()\n",
    "reshape_x_train = x_train_array.reshape(len(x_train), 1, x_train.shape[1])\n",
    "reshape_x_train.shape\n",
    "\n",
    "x_test_array = x_test.to_numpy()\n",
    "reshape_x_test = x_test_array.reshape(len(x_test), 1, x_train.shape[1])\n",
    "reshape_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIKq7z8tnIWl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 355 ms\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNGRU(128, return_sequences = True)))\n",
    "model.add(Dropout(best_bayes_params['dropout1']))\n",
    "model.add(Bidirectional(CuDNNGRU(best_bayes_params['units1'])))\n",
    "model.add(Dropout(best_bayes_params['dropout2']))\n",
    "model.add(Dense(best_bayes_params['units2'], activation='relu'))\n",
    "model.add(Dense(best_bayes_params['units3'], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=best_bayes_params['optimizer'], loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "# define the checkpoint\n",
    "filepath= ch_folder + \"/word2vec-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 434 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logdir = \"Resultados\\\\\" + exp_name +\"\\\\logs\\\\model\"\n",
    "\n",
    "\n",
    "tensor_board = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, profile_batch = 100000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1589755592715,
     "user": {
      "displayName": "Melina D'Alessandro",
      "photoUrl": "https://lh4.googleusercontent.com/-AU_sxBOTu8w/AAAAAAAAAAI/AAAAAAAAAR8/nO0zS5J_9Wo/s64/photo.jpg",
      "userId": "09190509655785270416"
     },
     "user_tz": 180
    },
    "id": "JsHgNLFnnTLN",
    "outputId": "4c22910d-c7b2-4dff-eb32-15c2574174ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40208 samples\n",
      "Epoch 1/50\n",
      "40208/40208 [==============================] - 12s 291us/sample - loss: 0.6924 - accuracy: 0.5342\n",
      "Epoch 2/50\n",
      "40208/40208 [==============================] - 8s 210us/sample - loss: 0.6924 - accuracy: 0.5345\n",
      "Epoch 3/50\n",
      "40208/40208 [==============================] - 8s 205us/sample - loss: 0.6923 - accuracy: 0.5356\n",
      "Epoch 4/50\n",
      "40208/40208 [==============================] - 8s 203us/sample - loss: 0.6922 - accuracy: 0.5375\n",
      "Epoch 5/50\n",
      "40208/40208 [==============================] - 8s 208us/sample - loss: 0.6921 - accuracy: 0.5378\n",
      "Epoch 6/50\n",
      "40208/40208 [==============================] - 9s 230us/sample - loss: 0.6920 - accuracy: 0.5383\n",
      "Epoch 7/50\n",
      "40208/40208 [==============================] - 8s 194us/sample - loss: 0.6920 - accuracy: 0.5385\n",
      "Epoch 8/50\n",
      "40208/40208 [==============================] - 7s 182us/sample - loss: 0.6919 - accuracy: 0.5392\n",
      "Epoch 9/50\n",
      "40208/40208 [==============================] - 8s 190us/sample - loss: 0.6918 - accuracy: 0.5387\n",
      "Epoch 10/50\n",
      "40208/40208 [==============================] - 7s 186us/sample - loss: 0.6917 - accuracy: 0.5388\n",
      "Epoch 11/50\n",
      "40208/40208 [==============================] - 7s 179us/sample - loss: 0.6916 - accuracy: 0.5386\n",
      "Epoch 12/50\n",
      "40208/40208 [==============================] - 8s 193us/sample - loss: 0.6916 - accuracy: 0.5389\n",
      "Epoch 13/50\n",
      "40208/40208 [==============================] - 7s 186us/sample - loss: 0.6915 - accuracy: 0.5390\n",
      "Epoch 14/50\n",
      "40208/40208 [==============================] - 7s 178us/sample - loss: 0.6914 - accuracy: 0.5390\n",
      "Epoch 15/50\n",
      "40208/40208 [==============================] - 8s 187us/sample - loss: 0.6914 - accuracy: 0.5390\n",
      "Epoch 16/50\n",
      "40208/40208 [==============================] - 9s 229us/sample - loss: 0.6914 - accuracy: 0.5390\n",
      "Epoch 17/50\n",
      "40208/40208 [==============================] - 8s 207us/sample - loss: 0.6913 - accuracy: 0.5391\n",
      "Epoch 18/50\n",
      "40208/40208 [==============================] - 8s 203us/sample - loss: 0.6912 - accuracy: 0.5391\n",
      "Epoch 19/50\n",
      "40208/40208 [==============================] - 8s 187us/sample - loss: 0.6912 - accuracy: 0.5391 - loss: 0.6912 - accuracy: 0.53\n",
      "Epoch 20/50\n",
      "40208/40208 [==============================] - 8s 200us/sample - loss: 0.6911 - accuracy: 0.5391\n",
      "Epoch 21/50\n",
      "40208/40208 [==============================] - 8s 211us/sample - loss: 0.6911 - accuracy: 0.5391\n",
      "Epoch 22/50\n",
      "40208/40208 [==============================] - 8s 196us/sample - loss: 0.6910 - accuracy: 0.5391\n",
      "Epoch 23/50\n",
      "40208/40208 [==============================] - 8s 193us/sample - loss: 0.6909 - accuracy: 0.5391\n",
      "Epoch 24/50\n",
      "40208/40208 [==============================] - 7s 182us/sample - loss: 0.6909 - accuracy: 0.5391\n",
      "Epoch 25/50\n",
      "40208/40208 [==============================] - 7s 182us/sample - loss: 0.6909 - accuracy: 0.5391\n",
      "Epoch 26/50\n",
      "40208/40208 [==============================] - 7s 185us/sample - loss: 0.6908 - accuracy: 0.5391\n",
      "Epoch 27/50\n",
      "40208/40208 [==============================] - 8s 189us/sample - loss: 0.6907 - accuracy: 0.5391 - loss: 0.6907 - accuracy: \n",
      "Epoch 28/50\n",
      "40208/40208 [==============================] - 8s 188us/sample - loss: 0.6907 - accuracy: 0.5391\n",
      "Epoch 29/50\n",
      "40208/40208 [==============================] - 8s 204us/sample - loss: 0.6907 - accuracy: 0.5391 - loss: 0.6908 - ac\n",
      "Epoch 30/50\n",
      "40208/40208 [==============================] - 8s 209us/sample - loss: 0.6906 - accuracy: 0.5391\n",
      "Epoch 31/50\n",
      "40208/40208 [==============================] - 9s 215us/sample - loss: 0.6906 - accuracy: 0.5391\n",
      "Epoch 32/50\n",
      "40208/40208 [==============================] - 8s 211us/sample - loss: 0.6906 - accuracy: 0.5391 - loss: 0.6906 \n",
      "Epoch 33/50\n",
      "40208/40208 [==============================] - 9s 212us/sample - loss: 0.6906 - accuracy: 0.5391\n",
      "Epoch 34/50\n",
      "40208/40208 [==============================] - 9s 212us/sample - loss: 0.6905 - accuracy: 0.5391\n",
      "Epoch 35/50\n",
      "40208/40208 [==============================] - 8s 203us/sample - loss: 0.6905 - accuracy: 0.5391\n",
      "Epoch 36/50\n",
      "40208/40208 [==============================] - 8s 208us/sample - loss: 0.6905 - accuracy: 0.5391 - l\n",
      "Epoch 37/50\n",
      "40208/40208 [==============================] - 8s 209us/sample - loss: 0.6905 - accuracy: 0.5391\n",
      "Epoch 38/50\n",
      "40208/40208 [==============================] - 8s 208us/sample - loss: 0.6904 - accuracy: 0.5391\n",
      "Epoch 39/50\n",
      "40208/40208 [==============================] - 8s 205us/sample - loss: 0.6904 - accuracy: 0.5391\n",
      "Epoch 40/50\n",
      "40208/40208 [==============================] - 9s 213us/sample - loss: 0.6904 - accuracy: 0.5391\n",
      "Epoch 41/50\n",
      "40208/40208 [==============================] - 8s 201us/sample - loss: 0.6904 - accuracy: 0.53912s - l\n",
      "Epoch 42/50\n",
      "40208/40208 [==============================] - 8s 211us/sample - loss: 0.6904 - accuracy: 0.5391\n",
      "Epoch 43/50\n",
      "40208/40208 [==============================] - 8s 210us/sample - loss: 0.6904 - accuracy: 0.5391\n",
      "Epoch 44/50\n",
      "40208/40208 [==============================] - 8s 207us/sample - loss: 0.6903 - accuracy: 0.5391\n",
      "Epoch 45/50\n",
      "40208/40208 [==============================] - 8s 208us/sample - loss: 0.6903 - accuracy: 0.5391\n",
      "Epoch 46/50\n",
      "40208/40208 [==============================] - 9s 221us/sample - loss: 0.6903 - accuracy: 0.5391\n",
      "Epoch 47/50\n",
      "40208/40208 [==============================] - 9s 227us/sample - loss: 0.6903 - accuracy: 0.5391\n",
      "Epoch 48/50\n",
      "40208/40208 [==============================] - 9s 235us/sample - loss: 0.6903 - accuracy: 0.5391\n",
      "Epoch 49/50\n",
      "40208/40208 [==============================] - 8s 210us/sample - loss: 0.6903 - accuracy: 0.5391\n",
      "Epoch 50/50\n",
      "40208/40208 [==============================] - 9s 214us/sample - loss: 0.6902 - accuracy: 0.5391\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection multiple                  338688    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection multiple                  55680     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  16640     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  8224      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  33        \n",
      "=================================================================\n",
      "Total params: 419,265\n",
      "Trainable params: 419,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 6min 54s\n"
     ]
    }
   ],
   "source": [
    "# # fit the model\n",
    "# model.fit(reshape_x_train, y_train,\n",
    "#           epochs=best_bayes_params['nb_epochs'], \n",
    "#           batch_size=best_bayes_params['batch_size'], callbacks=[tensor_board])\n",
    "\n",
    "# fit the model\n",
    "model.fit(reshape_x_train, y_train,\n",
    "          epochs=50,\n",
    "          batch_size=100, callbacks=[tensor_board])\n",
    "\n",
    "\n",
    "model.save(folder + '/keras_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5391\n",
      "Testing Accuracy:  0.5195\n",
      "time: 12 s\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(reshape_x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(reshape_x_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1165,
     "status": "ok",
     "timestamp": 1589755595730,
     "user": {
      "displayName": "Melina D'Alessandro",
      "photoUrl": "https://lh4.googleusercontent.com/-AU_sxBOTu8w/AAAAAAAAAAI/AAAAAAAAAR8/nO0zS5J_9Wo/s64/photo.jpg",
      "userId": "09190509655785270416"
     },
     "user_tz": 180
    },
    "id": "mdsR4Qngv5Q1",
    "outputId": "433bbcf6-d6eb-44c1-a1bb-afee72ac0ffb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5271095 ],\n",
       "       [0.53398144],\n",
       "       [0.52964747],\n",
       "       ...,\n",
       "       [0.5279911 ],\n",
       "       [0.52568513],\n",
       "       [0.5369585 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "ypred = model.predict_proba(reshape_x_test)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de das en testing:  379\n",
      "Numero de registros en testing:  379\n",
      "time: 70 ms\n"
     ]
    }
   ],
   "source": [
    "testing_cp = testing.copy()\n",
    "testing_cp['Prob'] = ypred\n",
    "testing_cp['Prob_dia'] = testing_cp['Prob'].groupby(testing_cp['Date']).transform('mean')\n",
    "testing_cp['Prediction'] = 0\n",
    "testing_cp.loc[testing_cp['Prob_dia']> 0.5, 'Prediction'] = 1\n",
    "testing_cp.drop_duplicates(subset=['Date','Prediction','Label'], inplace=True)\n",
    "testing_cp.head(1)\n",
    "\n",
    "print('Numero de das en testing: ', testing['Date'].nunique())\n",
    "print('Numero de registros en testing: ', testing_cp['Date'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 364 ms\n"
     ]
    }
   ],
   "source": [
    "testing_cp['Label'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    197\n",
       "0    182\n",
       "Name: Label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    379\n",
       "Name: Prediction, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "display(testing_cp['Label'].value_counts(),\n",
    "        testing_cp['Prediction'].value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.00      0.00      0.00       182\n",
      "     class 1       0.52      1.00      0.68       197\n",
      "\n",
      "    accuracy                           0.52       379\n",
      "   macro avg       0.26      0.50      0.34       379\n",
      "weighted avg       0.27      0.52      0.36       379\n",
      "\n",
      "time: 405 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(testing_cp.Label, testing_cp.Prediction, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creates a confusion matrix\n",
    "cm = confusion_matrix(testing_cp.Label, testing_cp.Prediction) \n",
    "\n",
    "# Transform to df for easier plotting\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['Sube','Baja'], \n",
    "                     columns = ['Sube','Baja'])\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cm_df, annot=True, cmap=\"Greens\", fmt='g')\n",
    "plt.title('Neuronal Network \\nAccuracy:{0:.3f}'.format(accuracy_score(testing_cp.Label, testing_cp.Prediction)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.save_checkpoint()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resultados/7/RNN_Model_Base.ipynb'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 274 ms\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "copyfile('RNN_Model_Base_GPU_m3.ipynb', folder + '/RNN_Model_Base.ipynb' )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhtKTbzPEwz7PyaEe0FkIO",
   "collapsed_sections": [],
   "name": "RNN - Meli.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
