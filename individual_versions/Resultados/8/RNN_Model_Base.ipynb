{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4dLP23xZmpC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "# Importar librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "from datetime import date\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n",
      "time: 329 ms\n"
     ]
    }
   ],
   "source": [
    "exp_name = '8'\n",
    "folder = 'Resultados/' + exp_name\n",
    "my_file = Path(folder)\n",
    "if os.path.exists(my_file):\n",
    "    print('already exists')\n",
    "else:\n",
    "    os.makedirs(folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n",
      "time: 171 ms\n"
     ]
    }
   ],
   "source": [
    "ch_folder = folder + '/Checkpoints'\n",
    "my_file = Path(ch_folder)\n",
    "if os.path.exists(my_file):\n",
    "    print('already exists')\n",
    "else:\n",
    "    os.makedirs(ch_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nzv66BqFbl92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49649</th>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.050684</td>\n",
       "      <td>-0.078711</td>\n",
       "      <td>0.069385</td>\n",
       "      <td>-0.120630</td>\n",
       "      <td>0.095508</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.096631</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1759</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49650</th>\n",
       "      <td>0.073608</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>-0.069580</td>\n",
       "      <td>-0.088684</td>\n",
       "      <td>-0.126465</td>\n",
       "      <td>-0.135132</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>-0.334961</td>\n",
       "      <td>0.136230</td>\n",
       "      <td>0.150879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 314 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "49649  0.000830  0.050684 -0.078711  0.069385 -0.120630  0.095508  0.036621   \n",
       "49650  0.073608  0.090820 -0.069580 -0.088684 -0.126465 -0.135132 -0.096680   \n",
       "\n",
       "              7         8         9  ...  topic_1  topic_2  topic_3  topic_4  \\\n",
       "49649  0.002893  0.096631  0.027344  ...   0.0222   0.0178   0.0149   0.0128   \n",
       "49650 -0.334961  0.136230  0.150879  ...   0.0141   0.0113   0.0000   0.1980   \n",
       "\n",
       "       topic_5  topic_6  topic_7  topic_8  topic_9  sentiment  \n",
       "49649   0.0113     0.01   0.1759    0.698      0.0       -0.1  \n",
       "49650   0.2274     0.00   0.0000    0.000      0.0        0.0  \n",
       "\n",
       "[2 rows x 314 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.03 s\n"
     ]
    }
   ],
   "source": [
    "#Importar los datasets\n",
    "url_embeddings_average_individual = zipfile.ZipFile('../Data/average_bigram_topics_sentiment.zip')\n",
    "\n",
    "embeddings_average_individual = pd.read_csv(url_embeddings_average_individual.open('average_bigram_topics_sentiment.csv'), index_col = 0)\n",
    "\n",
    "embeddings_average_individual['Date'] =  pd.to_datetime(embeddings_average_individual['Date'], format='%Y-%m-%d')\n",
    "# embeddings_average_individual.sort_values('Date', inplace=True)\n",
    "# embeddings_average_individual.reset_index(drop = True , inplace=True)\n",
    "embeddings_average_individual.reset_index(inplace=True)\n",
    "embeddings_average_individual.fillna(0, inplace=True)\n",
    "embeddings_average_individual.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAXOJcEcbmed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono la fecha para la cual hago el corte de train y test\n",
    "training_end = pd.to_datetime(\"2013-12-31\")\n",
    "num_training = len(embeddings_average_individual[(embeddings_average_individual[\"Date\"]) <= training_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 247 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono el archivo con el que se corre el modelo\n",
    "data = embeddings_average_individual[embeddings_average_individual['Date']<='2014-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6294, 1, 311)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 286 ms\n"
     ]
    }
   ],
   "source": [
    "# Se separa en train y test\n",
    "x_train = data.drop([\"Top\",\"Label\", \"Date\"], axis=1)[:num_training]\n",
    "x_test = data.drop([\"Top\",'Label', 'Date'], axis=1)[num_training:]\n",
    "y_train = data[\"Label\"].values[:num_training]\n",
    "y_test = data[\"Label\"].values[num_training:]\n",
    "\n",
    "\n",
    "x_train_array = x_train.to_numpy()\n",
    "reshape_x_train = x_train_array.reshape(len(x_train), 1, x_train.shape[1])\n",
    "reshape_x_train.shape\n",
    "\n",
    "x_test_array = x_test.to_numpy()\n",
    "reshape_x_test = x_test_array.reshape(len(x_test), 1, x_train.shape[1])\n",
    "reshape_x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir espacio de busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 182 ms\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'units1': hp.choice('units1', [10, 64, 128, 256, 512]),\n",
    "    'units2': hp.choice('units2', [10, 64, 128, 256, 512]),\n",
    "                 \n",
    "    'dropout1': hp.choice('dropout1', [0.2,0.3,0.1]),\n",
    "    \n",
    "    'batch_size' : hp.choice('batch_size', [128,256,512]),\n",
    "    'nb_epochs' : hp.choice('nb_epochs', [50]),\n",
    "\n",
    "    'optimizer':  hp.choice('optimizer', [ 'adam','adadelta']),   \n",
    "    'activation': 'relu'    \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir busqueda bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 145 ms\n"
     ]
    }
   ],
   "source": [
    "#Objective function that hyperopt will minimize\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def objective(params):\n",
    "    \n",
    "#     import ml_metrics\n",
    "\n",
    "    \n",
    "    start = timer()\n",
    "    print ('Params testing: ', params)\n",
    "    print ('\\n ')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['units1'], input_shape=(1,x_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "    model.add(LSTM(params['units2'], return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # compile the model\n",
    "    model.compile(optimizer=params['optimizer'], loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    logdir = \"Resultados\\\\\" + exp_name +\"\\\\logs\\\\model\"\n",
    "\n",
    "    tensor_board = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, profile_batch = 100000000)\n",
    "\n",
    "    \n",
    "    #includes the call back object\n",
    "    model.fit(reshape_x_train, y_train, epochs=params['nb_epochs'], batch_size=params['batch_size'],\n",
    "              verbose = 0, validation_data=(reshape_x_test, y_test),callbacks=[tensor_board])\n",
    "     \n",
    "    #predict the test set \n",
    "    score, acc = model.evaluate(reshape_x_test, y_test, verbose=0)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([-acc, params, score, run_time])\n",
    "    of_connection.close()\n",
    "    \n",
    "    \n",
    "    print('Test accuracy:', acc)\n",
    " \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'train_time': run_time,}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenar resultados de cada iteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import tpe\n",
    "\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from hyperopt import Trials\n",
    "\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# File to save first results\n",
    "out_file = folder + '/gbm_results.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'score','time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanzar optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 512, 'dropout1': 0.3, 'nb_epochs': 50, 'optimizer': 'adadelta', 'units1': 128, 'units2': 128}                                     \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.4914204                                                                                                                                                              \n",
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 256, 'dropout1': 0.3, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 64, 'units2': 10}                                           \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.48601842                                                                                                                                                             \n",
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 512, 'dropout1': 0.3, 'nb_epochs': 50, 'optimizer': 'adadelta', 'units1': 512, 'units2': 10}                                      \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.49189705                                                                                                                                                             \n",
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 256, 'dropout1': 0.3, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 10, 'units2': 512}                                          \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.49491578                                                                                                                                                             \n",
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 256, 'dropout1': 0.3, 'nb_epochs': 50, 'optimizer': 'adadelta', 'units1': 128, 'units2': 128}                                     \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.4915793                                                                                                                                                              \n",
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 256, 'dropout1': 0.2, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 256, 'units2': 128}                                         \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.4855418                                                                                                                                                              \n",
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 128, 'dropout1': 0.3, 'nb_epochs': 50, 'optimizer': 'adadelta', 'units1': 256, 'units2': 256}                                     \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.4915793                                                                                                                                                              \n",
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 128, 'dropout1': 0.2, 'nb_epochs': 50, 'optimizer': 'adadelta', 'units1': 64, 'units2': 256}                                      \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.4915793                                                                                                                                                              \n",
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 256, 'dropout1': 0.3, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 512, 'units2': 10}                                          \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.48490626                                                                                                                                                             \n",
      "Params testing:                                                                                                                                                        \n",
      "{'activation': 'relu', 'batch_size': 256, 'dropout1': 0.2, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 512, 'units2': 64}                                          \n",
      "Test accuracy:                                                                                                                                                         \n",
      "0.48347634                                                                                                                                                             \n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [30:27<00:00, 182.71s/trial, best loss: -0.49491578340530396]\n",
      "time: 30min 27s\n"
     ]
    }
   ],
   "source": [
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = 10, trials = bayes_trials,\n",
    "            verbose = 1, rstate= np.random.RandomState(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar bayesiana, por si quisiera retomar donde queda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(bayes_trials, open(folder + '/trials.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer mejores parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': -0.49491578340530396,\n",
       "  'status': 'ok',\n",
       "  'train_time': 183.40774599999997},\n",
       " {'loss': -0.4918970465660095, 'status': 'ok', 'train_time': 166.3682008}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "bayes_trials_results  = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results [:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.494916</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 256, 'dro...</td>\n",
       "      <td>0.761239</td>\n",
       "      <td>183.407746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.491897</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 512, 'dro...</td>\n",
       "      <td>0.693182</td>\n",
       "      <td>166.368201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.491579</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 256, 'dro...</td>\n",
       "      <td>0.693573</td>\n",
       "      <td>125.723013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.491579</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 128, 'dro...</td>\n",
       "      <td>0.694035</td>\n",
       "      <td>295.828129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.491579</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 128, 'dro...</td>\n",
       "      <td>0.693995</td>\n",
       "      <td>217.515321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                             params     score  \\\n",
       "0 -0.494916  {'activation': 'relu', 'batch_size': 256, 'dro...  0.761239   \n",
       "1 -0.491897  {'activation': 'relu', 'batch_size': 512, 'dro...  0.693182   \n",
       "2 -0.491579  {'activation': 'relu', 'batch_size': 256, 'dro...  0.693573   \n",
       "3 -0.491579  {'activation': 'relu', 'batch_size': 128, 'dro...  0.694035   \n",
       "4 -0.491579  {'activation': 'relu', 'batch_size': 128, 'dro...  0.693995   \n",
       "\n",
       "         time  \n",
       "0  183.407746  \n",
       "1  166.368201  \n",
       "2  125.723013  \n",
       "3  295.828129  \n",
       "4  217.515321  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(folder + '/gbm_results.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 256,\n",
       " 'dropout1': 0.3,\n",
       " 'nb_epochs': 50,\n",
       " 'optimizer': 'adam',\n",
       " 'units1': 10,\n",
       " 'units2': 512}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 256,\n",
       " 'dropout1': 0.3,\n",
       " 'nb_epochs': 50,\n",
       " 'optimizer': 'adam',\n",
       " 'units1': 10,\n",
       " 'units2': 512}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 137 ms\n"
     ]
    }
   ],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "best_bayes_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir datasets de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 278 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono la fecha para la cual hago el corte de train y test\n",
    "training_end = pd.to_datetime(\"2014-12-31\")\n",
    "num_training = len(embeddings_average_individual[(embeddings_average_individual[\"Date\"]) <= training_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono el archivo con el que se corre el modelo\n",
    "data = embeddings_average_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9443, 1, 311)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 308 ms\n"
     ]
    }
   ],
   "source": [
    "# Se separa en train y test\n",
    "x_train = data.drop([\"Top\",\"Label\", \"Date\"], axis=1)[:num_training]\n",
    "x_test = data.drop([\"Top\",'Label', 'Date'], axis=1)[num_training:]\n",
    "y_train = data[\"Label\"].values[:num_training]\n",
    "y_test = data[\"Label\"].values[num_training:]\n",
    "\n",
    "\n",
    "x_train_array = x_train.to_numpy()\n",
    "reshape_x_train = x_train_array.reshape(len(x_train), 1, x_train.shape[1])\n",
    "reshape_x_train.shape\n",
    "\n",
    "x_test_array = x_test.to_numpy()\n",
    "reshape_x_test = x_test_array.reshape(len(x_test), 1, x_train.shape[1])\n",
    "reshape_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIKq7z8tnIWl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(best_bayes_params['units1'], input_shape=(1,x_train.shape[1]), return_sequences=True))\n",
    "model.add(Dropout(best_bayes_params['dropout1']))\n",
    "model.add(LSTM(best_bayes_params['units2'], return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# define the checkpoint\n",
    "filepath= ch_folder + \"/word2vec-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logdir = \"Resultados\\\\\" + exp_name +\"\\\\logs\\\\model\"\n",
    "\n",
    "\n",
    "tensor_board = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, profile_batch = 100000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1589755592715,
     "user": {
      "displayName": "Melina D'Alessandro",
      "photoUrl": "https://lh4.googleusercontent.com/-AU_sxBOTu8w/AAAAAAAAAAI/AAAAAAAAAR8/nO0zS5J_9Wo/s64/photo.jpg",
      "userId": "09190509655785270416"
     },
     "user_tz": 180
    },
    "id": "JsHgNLFnnTLN",
    "outputId": "4c22910d-c7b2-4dff-eb32-15c2574174ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40208 samples\n",
      "Epoch 1/50\n",
      "40208/40208 [==============================] - 8s 199us/sample - loss: 0.6904 - accuracy: 0.5383\n",
      "Epoch 2/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6901 - accuracy: 0.5391\n",
      "Epoch 3/50\n",
      "40208/40208 [==============================] - 4s 91us/sample - loss: 0.6898 - accuracy: 0.5391\n",
      "Epoch 4/50\n",
      "40208/40208 [==============================] - 4s 95us/sample - loss: 0.6892 - accuracy: 0.5396\n",
      "Epoch 5/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6887 - accuracy: 0.5404\n",
      "Epoch 6/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6884 - accuracy: 0.5386\n",
      "Epoch 7/50\n",
      "40208/40208 [==============================] - 4s 99us/sample - loss: 0.6880 - accuracy: 0.5412\n",
      "Epoch 8/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6875 - accuracy: 0.5439\n",
      "Epoch 9/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6869 - accuracy: 0.5430\n",
      "Epoch 10/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6862 - accuracy: 0.5447\n",
      "Epoch 11/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6856 - accuracy: 0.5471\n",
      "Epoch 12/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6849 - accuracy: 0.5499\n",
      "Epoch 13/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6846 - accuracy: 0.5477\n",
      "Epoch 14/50\n",
      "40208/40208 [==============================] - 4s 91us/sample - loss: 0.6835 - accuracy: 0.5526\n",
      "Epoch 15/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6830 - accuracy: 0.5527\n",
      "Epoch 16/50\n",
      "40208/40208 [==============================] - 4s 91us/sample - loss: 0.6825 - accuracy: 0.5566\n",
      "Epoch 17/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6813 - accuracy: 0.5571\n",
      "Epoch 18/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6805 - accuracy: 0.5563\n",
      "Epoch 19/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6793 - accuracy: 0.5582\n",
      "Epoch 20/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6788 - accuracy: 0.5592\n",
      "Epoch 21/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6775 - accuracy: 0.5606\n",
      "Epoch 22/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6777 - accuracy: 0.5610\n",
      "Epoch 23/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6758 - accuracy: 0.5673\n",
      "Epoch 24/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6745 - accuracy: 0.5659\n",
      "Epoch 25/50\n",
      "40208/40208 [==============================] - 4s 95us/sample - loss: 0.6742 - accuracy: 0.5678\n",
      "Epoch 26/50\n",
      "40208/40208 [==============================] - 4s 94us/sample - loss: 0.6738 - accuracy: 0.5677\n",
      "Epoch 27/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6724 - accuracy: 0.5685\n",
      "Epoch 28/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6720 - accuracy: 0.5705\n",
      "Epoch 29/50\n",
      "40208/40208 [==============================] - 4s 95us/sample - loss: 0.6706 - accuracy: 0.5748\n",
      "Epoch 30/50\n",
      "40208/40208 [==============================] - 4s 109us/sample - loss: 0.6689 - accuracy: 0.5713\n",
      "Epoch 31/50\n",
      "40208/40208 [==============================] - 4s 108us/sample - loss: 0.6697 - accuracy: 0.5741\n",
      "Epoch 32/50\n",
      "40208/40208 [==============================] - 4s 109us/sample - loss: 0.6678 - accuracy: 0.5754\n",
      "Epoch 33/50\n",
      "40208/40208 [==============================] - 4s 106us/sample - loss: 0.6676 - accuracy: 0.5753\n",
      "Epoch 34/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6672 - accuracy: 0.5774\n",
      "Epoch 35/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6660 - accuracy: 0.5801\n",
      "Epoch 36/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6655 - accuracy: 0.5791\n",
      "Epoch 37/50\n",
      "40208/40208 [==============================] - 4s 99us/sample - loss: 0.6660 - accuracy: 0.5789\n",
      "Epoch 38/50\n",
      "40208/40208 [==============================] - 4s 92us/sample - loss: 0.6646 - accuracy: 0.5808\n",
      "Epoch 39/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6634 - accuracy: 0.5796\n",
      "Epoch 40/50\n",
      "40208/40208 [==============================] - 4s 99us/sample - loss: 0.6638 - accuracy: 0.5793\n",
      "Epoch 41/50\n",
      "40208/40208 [==============================] - 4s 93us/sample - loss: 0.6623 - accuracy: 0.5841\n",
      "Epoch 42/50\n",
      "40208/40208 [==============================] - 4s 95us/sample - loss: 0.6611 - accuracy: 0.5820\n",
      "Epoch 43/50\n",
      "40208/40208 [==============================] - 4s 97us/sample - loss: 0.6606 - accuracy: 0.5830\n",
      "Epoch 44/50\n",
      "40208/40208 [==============================] - 4s 94us/sample - loss: 0.6612 - accuracy: 0.5796\n",
      "Epoch 45/50\n",
      "40208/40208 [==============================] - 4s 94us/sample - loss: 0.6599 - accuracy: 0.5859\n",
      "Epoch 46/50\n",
      "40208/40208 [==============================] - 4s 95us/sample - loss: 0.6589 - accuracy: 0.5853\n",
      "Epoch 47/50\n",
      "40208/40208 [==============================] - 4s 95us/sample - loss: 0.6579 - accuracy: 0.5870\n",
      "Epoch 48/50\n",
      "40208/40208 [==============================] - 4s 95us/sample - loss: 0.6576 - accuracy: 0.5900\n",
      "Epoch 49/50\n",
      "40208/40208 [==============================] - 4s 95us/sample - loss: 0.6577 - accuracy: 0.5877\n",
      "Epoch 50/50\n",
      "40208/40208 [==============================] - 4s 102us/sample - loss: 0.6569 - accuracy: 0.5867\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 1, 10)             12880     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1, 10)             0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 512)               1071104   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,084,497\n",
      "Trainable params: 1,084,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(reshape_x_train, y_train,\n",
    "          epochs=best_bayes_params['nb_epochs'], \n",
    "          batch_size=best_bayes_params['batch_size'], callbacks=[tensor_board])\n",
    "\n",
    "\n",
    "model.save(folder + '/keras_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1165,
     "status": "ok",
     "timestamp": 1589755595730,
     "user": {
      "displayName": "Melina D'Alessandro",
      "photoUrl": "https://lh4.googleusercontent.com/-AU_sxBOTu8w/AAAAAAAAAAI/AAAAAAAAAR8/nO0zS5J_9Wo/s64/photo.jpg",
      "userId": "09190509655785270416"
     },
     "user_tz": 180
    },
    "id": "mdsR4Qngv5Q1",
    "outputId": "433bbcf6-d6eb-44c1-a1bb-afee72ac0ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.502383\n",
      "time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(reshape_x_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "copyfile('RNN_Model_Base_GPU.ipynb', folder + '/RNN_Model_Base.ipynb' )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhtKTbzPEwz7PyaEe0FkIO",
   "collapsed_sections": [],
   "name": "RNN - Meli.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
