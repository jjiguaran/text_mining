{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4dLP23xZmpC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ed\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "# Importar librerias\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "from datetime import date\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, GlobalMaxPool1D, Conv1D, Flatten,  MaxPooling1D, Activation, GlobalMaxPooling1D, Bidirectional, GRU\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n",
      "time: 214 ms\n"
     ]
    }
   ],
   "source": [
    "exp_name = '7'\n",
    "folder = 'Resultados/' + exp_name\n",
    "my_file = Path(folder)\n",
    "if os.path.exists(my_file):\n",
    "    print('already exists')\n",
    "else:\n",
    "    os.makedirs(folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n",
      "time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "ch_folder = folder + '/Checkpoints'\n",
    "my_file = Path(ch_folder)\n",
    "if os.path.exists(my_file):\n",
    "    print('already exists')\n",
    "else:\n",
    "    os.makedirs(ch_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nzv66BqFbl92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Top</th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49649</th>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.050684</td>\n",
       "      <td>-0.078711</td>\n",
       "      <td>0.069385</td>\n",
       "      <td>-0.120630</td>\n",
       "      <td>0.095508</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.096631</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.134375</td>\n",
       "      <td>-0.150244</td>\n",
       "      <td>-0.100488</td>\n",
       "      <td>0.123291</td>\n",
       "      <td>0.020605</td>\n",
       "      <td>0.104590</td>\n",
       "      <td>0.079565</td>\n",
       "      <td>0.159546</td>\n",
       "      <td>-0.014038</td>\n",
       "      <td>-0.019336</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>-0.091040</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>-0.058362</td>\n",
       "      <td>-0.175696</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>-0.047949</td>\n",
       "      <td>0.057214</td>\n",
       "      <td>-0.001563</td>\n",
       "      <td>-0.074683</td>\n",
       "      <td>-0.089453</td>\n",
       "      <td>-0.136575</td>\n",
       "      <td>-0.035522</td>\n",
       "      <td>-0.044775</td>\n",
       "      <td>-0.084277</td>\n",
       "      <td>-0.087115</td>\n",
       "      <td>0.092236</td>\n",
       "      <td>-0.074023</td>\n",
       "      <td>0.171582</td>\n",
       "      <td>0.055505</td>\n",
       "      <td>-0.031738</td>\n",
       "      <td>0.079736</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.062964</td>\n",
       "      <td>-0.156643</td>\n",
       "      <td>0.037891</td>\n",
       "      <td>0.053223</td>\n",
       "      <td>-0.184277</td>\n",
       "      <td>0.015015</td>\n",
       "      <td>-0.042213</td>\n",
       "      <td>0.110443</td>\n",
       "      <td>-0.159143</td>\n",
       "      <td>0.075684</td>\n",
       "      <td>0.050726</td>\n",
       "      <td>-0.124219</td>\n",
       "      <td>-0.053876</td>\n",
       "      <td>0.066498</td>\n",
       "      <td>0.046753</td>\n",
       "      <td>-0.142627</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>0.155615</td>\n",
       "      <td>-0.119690</td>\n",
       "      <td>0.023047</td>\n",
       "      <td>0.107422</td>\n",
       "      <td>-0.012073</td>\n",
       "      <td>0.052051</td>\n",
       "      <td>-0.038714</td>\n",
       "      <td>0.096625</td>\n",
       "      <td>0.032764</td>\n",
       "      <td>-0.105762</td>\n",
       "      <td>0.075488</td>\n",
       "      <td>-0.013354</td>\n",
       "      <td>-0.159668</td>\n",
       "      <td>-0.022070</td>\n",
       "      <td>-0.127051</td>\n",
       "      <td>-0.002856</td>\n",
       "      <td>0.020410</td>\n",
       "      <td>0.096093</td>\n",
       "      <td>0.051074</td>\n",
       "      <td>-0.160498</td>\n",
       "      <td>-0.044824</td>\n",
       "      <td>0.037259</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>-0.170763</td>\n",
       "      <td>0.044312</td>\n",
       "      <td>-0.043982</td>\n",
       "      <td>-0.170801</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>-0.107178</td>\n",
       "      <td>-0.018710</td>\n",
       "      <td>-0.023686</td>\n",
       "      <td>0.083203</td>\n",
       "      <td>-0.066431</td>\n",
       "      <td>0.240039</td>\n",
       "      <td>0.015967</td>\n",
       "      <td>0.080176</td>\n",
       "      <td>0.083057</td>\n",
       "      <td>0.085913</td>\n",
       "      <td>0.160815</td>\n",
       "      <td>-0.024756</td>\n",
       "      <td>0.089258</td>\n",
       "      <td>0.134863</td>\n",
       "      <td>-0.183105</td>\n",
       "      <td>0.093701</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>-0.030859</td>\n",
       "      <td>-0.171924</td>\n",
       "      <td>-0.070312</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>0.073975</td>\n",
       "      <td>-0.145459</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>-0.124481</td>\n",
       "      <td>-0.067749</td>\n",
       "      <td>-0.072168</td>\n",
       "      <td>0.152429</td>\n",
       "      <td>0.090430</td>\n",
       "      <td>-0.165698</td>\n",
       "      <td>0.082666</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>-0.021973</td>\n",
       "      <td>-0.027893</td>\n",
       "      <td>-0.214258</td>\n",
       "      <td>0.093848</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.018594</td>\n",
       "      <td>0.048389</td>\n",
       "      <td>0.057397</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>-0.235205</td>\n",
       "      <td>-0.159955</td>\n",
       "      <td>-0.205176</td>\n",
       "      <td>-0.246289</td>\n",
       "      <td>-0.039508</td>\n",
       "      <td>0.218457</td>\n",
       "      <td>-0.163428</td>\n",
       "      <td>0.123743</td>\n",
       "      <td>0.073145</td>\n",
       "      <td>-0.007524</td>\n",
       "      <td>-0.051318</td>\n",
       "      <td>0.064874</td>\n",
       "      <td>0.069727</td>\n",
       "      <td>-0.036743</td>\n",
       "      <td>-0.072217</td>\n",
       "      <td>-0.016357</td>\n",
       "      <td>-0.172266</td>\n",
       "      <td>-0.006573</td>\n",
       "      <td>0.173486</td>\n",
       "      <td>-0.005371</td>\n",
       "      <td>-0.003857</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>-0.031494</td>\n",
       "      <td>-0.094775</td>\n",
       "      <td>0.090625</td>\n",
       "      <td>-0.196759</td>\n",
       "      <td>-0.032446</td>\n",
       "      <td>-0.116797</td>\n",
       "      <td>0.097949</td>\n",
       "      <td>0.073145</td>\n",
       "      <td>0.187024</td>\n",
       "      <td>-0.023914</td>\n",
       "      <td>-0.021729</td>\n",
       "      <td>-0.093010</td>\n",
       "      <td>0.054492</td>\n",
       "      <td>0.035657</td>\n",
       "      <td>-0.014432</td>\n",
       "      <td>-0.093689</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>-0.125958</td>\n",
       "      <td>-0.115039</td>\n",
       "      <td>-0.030200</td>\n",
       "      <td>-0.080908</td>\n",
       "      <td>0.124854</td>\n",
       "      <td>-0.017725</td>\n",
       "      <td>0.061914</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>-0.130652</td>\n",
       "      <td>-0.037939</td>\n",
       "      <td>0.100732</td>\n",
       "      <td>-0.101514</td>\n",
       "      <td>0.065918</td>\n",
       "      <td>0.064258</td>\n",
       "      <td>-0.105762</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>0.134717</td>\n",
       "      <td>-0.051904</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.099512</td>\n",
       "      <td>0.149072</td>\n",
       "      <td>-0.068091</td>\n",
       "      <td>-0.006396</td>\n",
       "      <td>0.045746</td>\n",
       "      <td>-0.036450</td>\n",
       "      <td>-0.158325</td>\n",
       "      <td>-0.030737</td>\n",
       "      <td>0.049097</td>\n",
       "      <td>0.045508</td>\n",
       "      <td>-0.149609</td>\n",
       "      <td>0.038666</td>\n",
       "      <td>0.093359</td>\n",
       "      <td>0.014551</td>\n",
       "      <td>-0.016589</td>\n",
       "      <td>-0.057178</td>\n",
       "      <td>0.11460</td>\n",
       "      <td>-0.064404</td>\n",
       "      <td>-0.166699</td>\n",
       "      <td>0.026172</td>\n",
       "      <td>-0.048755</td>\n",
       "      <td>-0.047766</td>\n",
       "      <td>-0.108203</td>\n",
       "      <td>-0.067676</td>\n",
       "      <td>0.05293</td>\n",
       "      <td>0.105859</td>\n",
       "      <td>-0.162329</td>\n",
       "      <td>-0.151929</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>0.120850</td>\n",
       "      <td>-0.032813</td>\n",
       "      <td>-0.003711</td>\n",
       "      <td>-0.147778</td>\n",
       "      <td>0.150977</td>\n",
       "      <td>-0.049023</td>\n",
       "      <td>-0.017871</td>\n",
       "      <td>0.169336</td>\n",
       "      <td>0.044727</td>\n",
       "      <td>-0.014014</td>\n",
       "      <td>-0.022339</td>\n",
       "      <td>-0.033347</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>-0.017822</td>\n",
       "      <td>-0.022852</td>\n",
       "      <td>0.067737</td>\n",
       "      <td>0.113086</td>\n",
       "      <td>-0.056299</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>-0.058374</td>\n",
       "      <td>0.073743</td>\n",
       "      <td>-0.199902</td>\n",
       "      <td>0.022217</td>\n",
       "      <td>0.091113</td>\n",
       "      <td>-0.015039</td>\n",
       "      <td>0.064966</td>\n",
       "      <td>-0.006104</td>\n",
       "      <td>-0.209302</td>\n",
       "      <td>0.096289</td>\n",
       "      <td>0.058203</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>-0.074133</td>\n",
       "      <td>-0.156067</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>0.082349</td>\n",
       "      <td>-0.107092</td>\n",
       "      <td>0.155078</td>\n",
       "      <td>-0.066010</td>\n",
       "      <td>0.070361</td>\n",
       "      <td>-0.042505</td>\n",
       "      <td>0.131006</td>\n",
       "      <td>0.076392</td>\n",
       "      <td>-0.072461</td>\n",
       "      <td>-0.046094</td>\n",
       "      <td>0.154224</td>\n",
       "      <td>-0.020947</td>\n",
       "      <td>-0.030371</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.204150</td>\n",
       "      <td>0.089087</td>\n",
       "      <td>-0.191699</td>\n",
       "      <td>-0.013916</td>\n",
       "      <td>-0.139600</td>\n",
       "      <td>-0.072852</td>\n",
       "      <td>-0.053857</td>\n",
       "      <td>-0.142529</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.062061</td>\n",
       "      <td>-0.052393</td>\n",
       "      <td>0.167090</td>\n",
       "      <td>0.058887</td>\n",
       "      <td>-0.076538</td>\n",
       "      <td>-0.095724</td>\n",
       "      <td>-0.230847</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.106544</td>\n",
       "      <td>0.180566</td>\n",
       "      <td>0.070203</td>\n",
       "      <td>0.108008</td>\n",
       "      <td>-0.031592</td>\n",
       "      <td>-0.192676</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.075034</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>-0.095508</td>\n",
       "      <td>0.155450</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>24</td>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1759</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49650</th>\n",
       "      <td>0.073608</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>-0.069580</td>\n",
       "      <td>-0.088684</td>\n",
       "      <td>-0.126465</td>\n",
       "      <td>-0.135132</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>-0.334961</td>\n",
       "      <td>0.136230</td>\n",
       "      <td>0.150879</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.105713</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.161499</td>\n",
       "      <td>-0.276367</td>\n",
       "      <td>-0.014648</td>\n",
       "      <td>-0.018066</td>\n",
       "      <td>-0.157227</td>\n",
       "      <td>0.057129</td>\n",
       "      <td>-0.074219</td>\n",
       "      <td>0.015472</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.035583</td>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.050232</td>\n",
       "      <td>-0.231201</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.112549</td>\n",
       "      <td>0.104164</td>\n",
       "      <td>-0.083252</td>\n",
       "      <td>-0.042847</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>-0.002022</td>\n",
       "      <td>-0.015259</td>\n",
       "      <td>0.102051</td>\n",
       "      <td>-0.054443</td>\n",
       "      <td>0.074585</td>\n",
       "      <td>0.190308</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.017212</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>0.052979</td>\n",
       "      <td>0.127441</td>\n",
       "      <td>0.041626</td>\n",
       "      <td>-0.037476</td>\n",
       "      <td>0.186035</td>\n",
       "      <td>-0.058472</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>-0.024414</td>\n",
       "      <td>-0.101318</td>\n",
       "      <td>0.059448</td>\n",
       "      <td>-0.038818</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>0.194824</td>\n",
       "      <td>-0.150024</td>\n",
       "      <td>0.104996</td>\n",
       "      <td>-0.084717</td>\n",
       "      <td>-0.017090</td>\n",
       "      <td>-0.228516</td>\n",
       "      <td>-0.140137</td>\n",
       "      <td>-0.166992</td>\n",
       "      <td>-0.025635</td>\n",
       "      <td>0.056763</td>\n",
       "      <td>0.053955</td>\n",
       "      <td>-0.024170</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>-0.187988</td>\n",
       "      <td>-0.006149</td>\n",
       "      <td>0.095215</td>\n",
       "      <td>-0.068298</td>\n",
       "      <td>0.083374</td>\n",
       "      <td>-0.022217</td>\n",
       "      <td>0.172363</td>\n",
       "      <td>-0.237793</td>\n",
       "      <td>-0.091675</td>\n",
       "      <td>0.073486</td>\n",
       "      <td>-0.096924</td>\n",
       "      <td>-0.298096</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>-0.009766</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>0.118530</td>\n",
       "      <td>-0.108398</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>0.016373</td>\n",
       "      <td>0.094238</td>\n",
       "      <td>0.097534</td>\n",
       "      <td>0.108337</td>\n",
       "      <td>0.149597</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.243896</td>\n",
       "      <td>-0.097191</td>\n",
       "      <td>-0.044495</td>\n",
       "      <td>0.030579</td>\n",
       "      <td>-0.127319</td>\n",
       "      <td>0.216476</td>\n",
       "      <td>-0.082153</td>\n",
       "      <td>-0.138916</td>\n",
       "      <td>-0.220703</td>\n",
       "      <td>0.077881</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>0.236877</td>\n",
       "      <td>-0.114258</td>\n",
       "      <td>-0.154053</td>\n",
       "      <td>0.232910</td>\n",
       "      <td>-0.101074</td>\n",
       "      <td>-0.201172</td>\n",
       "      <td>0.088623</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>-0.002197</td>\n",
       "      <td>0.283691</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>0.333496</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>0.044067</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>-0.086182</td>\n",
       "      <td>-0.043671</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.148682</td>\n",
       "      <td>0.055725</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>-0.068359</td>\n",
       "      <td>-0.011810</td>\n",
       "      <td>0.112152</td>\n",
       "      <td>0.275391</td>\n",
       "      <td>-0.065193</td>\n",
       "      <td>-0.439453</td>\n",
       "      <td>0.074341</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.083008</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>0.124695</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>0.101807</td>\n",
       "      <td>-0.030762</td>\n",
       "      <td>-0.225098</td>\n",
       "      <td>0.467041</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>0.070618</td>\n",
       "      <td>0.184814</td>\n",
       "      <td>-0.049103</td>\n",
       "      <td>-0.184082</td>\n",
       "      <td>0.024933</td>\n",
       "      <td>0.085571</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>-0.153687</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>0.092285</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>-0.215942</td>\n",
       "      <td>-0.115143</td>\n",
       "      <td>0.187378</td>\n",
       "      <td>0.262695</td>\n",
       "      <td>-0.063721</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.202637</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.166504</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.224609</td>\n",
       "      <td>-0.033386</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.225098</td>\n",
       "      <td>-0.079712</td>\n",
       "      <td>-0.113525</td>\n",
       "      <td>0.084961</td>\n",
       "      <td>-0.257324</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.320312</td>\n",
       "      <td>0.165771</td>\n",
       "      <td>-0.292480</td>\n",
       "      <td>-0.020752</td>\n",
       "      <td>0.072754</td>\n",
       "      <td>-0.084839</td>\n",
       "      <td>0.069092</td>\n",
       "      <td>0.035198</td>\n",
       "      <td>0.018799</td>\n",
       "      <td>0.104736</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>-0.112061</td>\n",
       "      <td>0.046021</td>\n",
       "      <td>-0.188477</td>\n",
       "      <td>0.086792</td>\n",
       "      <td>-0.155884</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.155029</td>\n",
       "      <td>0.203003</td>\n",
       "      <td>0.062073</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>0.068481</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.070190</td>\n",
       "      <td>-0.297363</td>\n",
       "      <td>0.20166</td>\n",
       "      <td>-0.021973</td>\n",
       "      <td>-0.176086</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>0.100830</td>\n",
       "      <td>0.077271</td>\n",
       "      <td>-0.143433</td>\n",
       "      <td>0.030304</td>\n",
       "      <td>-0.09021</td>\n",
       "      <td>-0.142334</td>\n",
       "      <td>-0.226318</td>\n",
       "      <td>-0.059570</td>\n",
       "      <td>0.246582</td>\n",
       "      <td>-0.049927</td>\n",
       "      <td>0.216553</td>\n",
       "      <td>-0.128418</td>\n",
       "      <td>-0.072021</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>-0.065308</td>\n",
       "      <td>-0.057007</td>\n",
       "      <td>0.020020</td>\n",
       "      <td>0.082642</td>\n",
       "      <td>-0.013641</td>\n",
       "      <td>-0.182129</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.152588</td>\n",
       "      <td>-0.097046</td>\n",
       "      <td>-0.043701</td>\n",
       "      <td>0.066650</td>\n",
       "      <td>-0.152222</td>\n",
       "      <td>0.190308</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>-0.068115</td>\n",
       "      <td>0.157715</td>\n",
       "      <td>-0.093140</td>\n",
       "      <td>-0.064453</td>\n",
       "      <td>-0.188721</td>\n",
       "      <td>0.280273</td>\n",
       "      <td>0.023621</td>\n",
       "      <td>-0.118164</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.078857</td>\n",
       "      <td>-0.029785</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>0.061401</td>\n",
       "      <td>0.183838</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.260254</td>\n",
       "      <td>-0.113647</td>\n",
       "      <td>0.161095</td>\n",
       "      <td>-0.115967</td>\n",
       "      <td>0.018799</td>\n",
       "      <td>-0.083252</td>\n",
       "      <td>-0.061646</td>\n",
       "      <td>0.234863</td>\n",
       "      <td>-0.187012</td>\n",
       "      <td>0.211304</td>\n",
       "      <td>-0.074341</td>\n",
       "      <td>-0.036133</td>\n",
       "      <td>-0.095703</td>\n",
       "      <td>-0.222900</td>\n",
       "      <td>-0.005318</td>\n",
       "      <td>0.234863</td>\n",
       "      <td>-0.114136</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.228271</td>\n",
       "      <td>0.125244</td>\n",
       "      <td>-0.066895</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>-0.027344</td>\n",
       "      <td>-0.074341</td>\n",
       "      <td>-0.140503</td>\n",
       "      <td>-0.073059</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.123291</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.136688</td>\n",
       "      <td>-0.030518</td>\n",
       "      <td>-0.094360</td>\n",
       "      <td>-0.106689</td>\n",
       "      <td>-0.097900</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>-0.137268</td>\n",
       "      <td>-0.020630</td>\n",
       "      <td>0.015869</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>25</td>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5175</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6         7         8         9        10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49        50        51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72        73        74        75        76        77        78        79        80        81        82        83        84        85        86        87        88        89        90        91        92        93        94        95        96        97        98  \\\n",
       "49649  0.000830  0.050684 -0.078711  0.069385 -0.120630  0.095508  0.036621  0.002893  0.096631  0.027344  0.134375 -0.150244 -0.100488  0.123291  0.020605  0.104590  0.079565  0.159546 -0.014038 -0.019336  0.005188 -0.091040  0.018677  0.007471 -0.058362 -0.175696  0.052246 -0.047949  0.057214 -0.001563 -0.074683 -0.089453 -0.136575 -0.035522 -0.044775 -0.084277 -0.087115  0.092236 -0.074023  0.171582  0.055505 -0.031738  0.079736  0.000439  0.062964 -0.156643  0.037891  0.053223 -0.184277  0.015015 -0.042213  0.110443 -0.159143  0.075684  0.050726 -0.124219 -0.053876  0.066498  0.046753 -0.142627  0.001953  0.062012  0.155615 -0.119690  0.023047  0.107422 -0.012073  0.052051 -0.038714  0.096625  0.032764 -0.105762  0.075488 -0.013354 -0.159668 -0.022070 -0.127051 -0.002856  0.020410  0.096093  0.051074 -0.160498 -0.044824  0.037259  0.032520  0.038721 -0.170763  0.044312 -0.043982 -0.170801  0.001929 -0.107178 -0.018710 -0.023686  0.083203 -0.066431  0.240039  0.015967  0.080176   \n",
       "49650  0.073608  0.090820 -0.069580 -0.088684 -0.126465 -0.135132 -0.096680 -0.334961  0.136230  0.150879  0.071289 -0.105713  0.033203  0.161499 -0.276367 -0.014648 -0.018066 -0.157227  0.057129 -0.074219  0.015472  0.008789  0.104340  0.011719  0.035583 -0.005859  0.050232 -0.231201  0.066406  0.112549  0.104164 -0.083252 -0.042847 -0.089844 -0.002022 -0.015259  0.102051 -0.054443  0.074585  0.190308  0.149658  0.069946  0.135742  0.164062  0.017212 -0.115723  0.014465  0.089844  0.134766  0.052979  0.127441  0.041626 -0.037476  0.186035 -0.058472  0.020264 -0.024414 -0.101318  0.059448 -0.038818 -0.018188  0.194824 -0.150024  0.104996 -0.084717 -0.017090 -0.228516 -0.140137 -0.166992 -0.025635  0.056763  0.053955 -0.024170  0.106445 -0.187988 -0.006149  0.095215 -0.068298  0.083374 -0.022217  0.172363 -0.237793 -0.091675  0.073486 -0.096924 -0.298096 -0.056396  0.033203 -0.009766 -0.056641  0.118530 -0.108398 -0.208496  0.016373  0.094238  0.097534  0.108337  0.149597  0.069336   \n",
       "\n",
       "             99       100       101       102       103       104       105       106       107       108       109       110       111       112       113       114       115       116       117       118       119       120       121       122       123       124       125       126       127       128       129       130       131       132       133       134       135       136       137       138       139       140       141       142       143       144       145       146       147       148       149       150       151       152       153       154       155       156       157       158       159       160       161       162       163       164       165       166       167       168       169       170       171       172       173       174       175       176       177       178       179       180       181       182       183       184       185       186       187       188       189       190       191       192       193       194       195       196       197  \\\n",
       "49649  0.083057  0.085913  0.160815 -0.024756  0.089258  0.134863 -0.183105  0.093701 -0.032422 -0.030859 -0.171924 -0.070312 -0.000861  0.073975 -0.145459  0.006323 -0.124481 -0.067749 -0.072168  0.152429  0.090430 -0.165698  0.082666 -0.073535 -0.021973 -0.027893 -0.214258  0.093848  0.023438  0.018594  0.048389  0.057397  0.013354 -0.235205 -0.159955 -0.205176 -0.246289 -0.039508  0.218457 -0.163428  0.123743  0.073145 -0.007524 -0.051318  0.064874  0.069727 -0.036743 -0.072217 -0.016357 -0.172266 -0.006573  0.173486 -0.005371 -0.003857  0.003418 -0.031494 -0.094775  0.090625 -0.196759 -0.032446 -0.116797  0.097949  0.073145  0.187024 -0.023914 -0.021729 -0.093010  0.054492  0.035657 -0.014432 -0.093689  0.002148 -0.012500 -0.125958 -0.115039 -0.030200 -0.080908  0.124854 -0.017725  0.061914  0.010327 -0.130652 -0.037939  0.100732 -0.101514  0.065918  0.064258 -0.105762  0.006689  0.008521  0.134717 -0.051904  0.046875  0.099512  0.149072 -0.068091 -0.006396  0.045746 -0.036450   \n",
       "49650  0.243896 -0.097191 -0.044495  0.030579 -0.127319  0.216476 -0.082153 -0.138916 -0.220703  0.077881 -0.020264  0.236877 -0.114258 -0.154053  0.232910 -0.101074 -0.201172  0.088623  0.144287 -0.002197  0.283691  0.052490  0.333496 -0.053711  0.044067  0.132812 -0.104004 -0.086182 -0.043671  0.077148  0.148682  0.055725 -0.035156  0.074707 -0.068359 -0.011810  0.112152  0.275391 -0.065193 -0.439453  0.074341 -0.019775 -0.073242 -0.083008  0.150391 -0.065430  0.124695 -0.078613  0.101807 -0.030762 -0.225098  0.467041 -0.142578  0.070618  0.184814 -0.049103 -0.184082  0.024933  0.085571  0.273438 -0.153687 -0.020996  0.092285  0.117310 -0.215942 -0.115143  0.187378  0.262695 -0.063721  0.234375  0.202637  0.008789  0.166504 -0.168945 -0.224609 -0.033386  0.132812  0.225098 -0.079712 -0.113525  0.084961 -0.257324 -0.015869 -0.133789 -0.320312  0.165771 -0.292480 -0.020752  0.072754 -0.084839  0.069092  0.035198  0.018799  0.104736  0.017578 -0.112061  0.046021 -0.188477  0.086792   \n",
       "\n",
       "            198       199       200       201       202       203       204       205       206       207      208       209       210       211       212       213       214       215      216       217       218       219       220       221       222       223       224       225       226       227       228       229       230       231       232       233       234       235       236       237       238       239       240       241       242       243       244       245       246       247       248       249       250       251       252       253       254       255       256       257       258       259       260       261       262       263       264       265       266       267       268       269       270       271       272       273       274       275       276       277       278       279       280       281       282       283       284       285       286       287       288       289       290       291       292       293       294       295       296  \\\n",
       "49649 -0.158325 -0.030737  0.049097  0.045508 -0.149609  0.038666  0.093359  0.014551 -0.016589 -0.057178  0.11460 -0.064404 -0.166699  0.026172 -0.048755 -0.047766 -0.108203 -0.067676  0.05293  0.105859 -0.162329 -0.151929  0.009521 -0.007690  0.120850 -0.032813 -0.003711 -0.147778  0.150977 -0.049023 -0.017871  0.169336  0.044727 -0.014014 -0.022339 -0.033347  0.001709 -0.017822 -0.022852  0.067737  0.113086 -0.056299  0.023663 -0.058374  0.073743 -0.199902  0.022217  0.091113 -0.015039  0.064966 -0.006104 -0.209302  0.096289  0.058203  0.109180 -0.001855 -0.074133 -0.156067 -0.010223  0.082349 -0.107092  0.155078 -0.066010  0.070361 -0.042505  0.131006  0.076392 -0.072461 -0.046094  0.154224 -0.020947 -0.030371  0.039795  0.204150  0.089087 -0.191699 -0.013916 -0.139600 -0.072852 -0.053857 -0.142529  0.057910  0.062061 -0.052393  0.167090  0.058887 -0.076538 -0.095724 -0.230847  0.036865  0.106544  0.180566  0.070203  0.108008 -0.031592 -0.192676 -0.093750  0.075034  0.023315   \n",
       "49650 -0.155884  0.007324  0.155029  0.203003  0.062073 -0.023438  0.068481  0.030701  0.070190 -0.297363  0.20166 -0.021973 -0.176086 -0.018555  0.100830  0.077271 -0.143433  0.030304 -0.09021 -0.142334 -0.226318 -0.059570  0.246582 -0.049927  0.216553 -0.128418 -0.072021 -0.058594 -0.065308 -0.057007  0.020020  0.082642 -0.013641 -0.182129  0.018066  0.152588 -0.097046 -0.043701  0.066650 -0.152222  0.190308 -0.142578 -0.068115  0.157715 -0.093140 -0.064453 -0.188721  0.280273  0.023621 -0.118164 -0.089844  0.000305  0.078857 -0.029785 -0.040039  0.061401  0.183838  0.067871  0.260254 -0.113647  0.161095 -0.115967  0.018799 -0.083252 -0.061646  0.234863 -0.187012  0.211304 -0.074341 -0.036133 -0.095703 -0.222900 -0.005318  0.234863 -0.114136 -0.187500  0.228271  0.125244 -0.066895  0.083984 -0.018311 -0.027344 -0.074341 -0.140503 -0.073059  0.069946 -0.080322 -0.133789 -0.123291  0.000488  0.012451  0.136688 -0.030518 -0.094360 -0.106689 -0.097900  0.036133 -0.075684 -0.137268   \n",
       "\n",
       "            297       298       299  Top       Date  Label  topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  topic_7  topic_8  topic_9  sentiment  \n",
       "49649 -0.095508  0.155450  0.075000   24 2008-08-08      0   0.0295   0.0222   0.0178   0.0149   0.0128   0.0113     0.01   0.1759    0.698      0.0       -0.1  \n",
       "49650 -0.020630  0.015869  0.000488   25 2008-08-08      0   0.5175   0.0141   0.0113   0.0000   0.1980   0.2274     0.00   0.0000    0.000      0.0        0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.38 s\n"
     ]
    }
   ],
   "source": [
    "#Importar los datasets\n",
    "url_embeddings_average_individual = zipfile.ZipFile('../Data/average_bigram_topics_sentiment.zip')\n",
    "\n",
    "embeddings_average_individual = pd.read_csv(url_embeddings_average_individual.open('average_bigram_topics_sentiment.csv'), index_col = 0)\n",
    "\n",
    "embeddings_average_individual['Date'] =  pd.to_datetime(embeddings_average_individual['Date'], format='%Y-%m-%d')\n",
    "\n",
    "embeddings_average_individual.reset_index(inplace=True)\n",
    "embeddings_average_individual.fillna(0, inplace=True)\n",
    "embeddings_average_individual.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAXOJcEcbmed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 98 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono la fecha para la cual hago el corte de train y test\n",
    "training_end = pd.to_datetime(\"2013-12-31\")\n",
    "num_training = len(embeddings_average_individual[(embeddings_average_individual[\"Date\"]) <= training_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418076004775169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Top</th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9443</th>\n",
       "      <td>-0.063354</td>\n",
       "      <td>0.015055</td>\n",
       "      <td>-0.127116</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>-0.07666</td>\n",
       "      <td>-0.111328</td>\n",
       "      <td>-0.004476</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.148763</td>\n",
       "      <td>0.015381</td>\n",
       "      <td>0.095835</td>\n",
       "      <td>-0.082113</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>-0.063232</td>\n",
       "      <td>0.176595</td>\n",
       "      <td>0.047791</td>\n",
       "      <td>0.068827</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.039714</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>-0.130697</td>\n",
       "      <td>-0.138672</td>\n",
       "      <td>0.013018</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>-0.103597</td>\n",
       "      <td>0.046224</td>\n",
       "      <td>0.167643</td>\n",
       "      <td>0.028768</td>\n",
       "      <td>-0.177083</td>\n",
       "      <td>0.079997</td>\n",
       "      <td>-0.026204</td>\n",
       "      <td>-0.10555</td>\n",
       "      <td>-0.044434</td>\n",
       "      <td>0.032288</td>\n",
       "      <td>-0.023071</td>\n",
       "      <td>0.133301</td>\n",
       "      <td>0.076497</td>\n",
       "      <td>0.03304</td>\n",
       "      <td>0.009018</td>\n",
       "      <td>-0.049886</td>\n",
       "      <td>0.048991</td>\n",
       "      <td>-0.166972</td>\n",
       "      <td>-0.024577</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>0.059896</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.14091</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.062581</td>\n",
       "      <td>-0.09082</td>\n",
       "      <td>0.042013</td>\n",
       "      <td>-0.231608</td>\n",
       "      <td>0.029012</td>\n",
       "      <td>-0.036865</td>\n",
       "      <td>0.06665</td>\n",
       "      <td>0.036418</td>\n",
       "      <td>0.096842</td>\n",
       "      <td>-0.008626</td>\n",
       "      <td>-0.129781</td>\n",
       "      <td>-0.075521</td>\n",
       "      <td>-0.066447</td>\n",
       "      <td>-0.120361</td>\n",
       "      <td>-0.009572</td>\n",
       "      <td>0.056966</td>\n",
       "      <td>0.158529</td>\n",
       "      <td>0.16154</td>\n",
       "      <td>-0.03658</td>\n",
       "      <td>-0.037516</td>\n",
       "      <td>-0.003642</td>\n",
       "      <td>-0.131917</td>\n",
       "      <td>-0.201009</td>\n",
       "      <td>-0.108358</td>\n",
       "      <td>-0.025553</td>\n",
       "      <td>0.015381</td>\n",
       "      <td>-0.040436</td>\n",
       "      <td>0.02002</td>\n",
       "      <td>-0.031291</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>0.06901</td>\n",
       "      <td>-0.016602</td>\n",
       "      <td>0.061971</td>\n",
       "      <td>-0.024089</td>\n",
       "      <td>-0.119954</td>\n",
       "      <td>0.102397</td>\n",
       "      <td>-0.027995</td>\n",
       "      <td>0.075684</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>-0.162069</td>\n",
       "      <td>-0.056152</td>\n",
       "      <td>-0.052409</td>\n",
       "      <td>-0.097493</td>\n",
       "      <td>0.134684</td>\n",
       "      <td>0.032633</td>\n",
       "      <td>-0.05717</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.025065</td>\n",
       "      <td>-0.104533</td>\n",
       "      <td>0.061686</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>-0.01062</td>\n",
       "      <td>0.011882</td>\n",
       "      <td>0.077637</td>\n",
       "      <td>0.152445</td>\n",
       "      <td>-0.071828</td>\n",
       "      <td>-0.009847</td>\n",
       "      <td>0.015299</td>\n",
       "      <td>-0.055908</td>\n",
       "      <td>0.00061</td>\n",
       "      <td>-0.220062</td>\n",
       "      <td>-0.024801</td>\n",
       "      <td>0.030268</td>\n",
       "      <td>0.121989</td>\n",
       "      <td>0.059082</td>\n",
       "      <td>0.096293</td>\n",
       "      <td>-0.104167</td>\n",
       "      <td>-0.1014</td>\n",
       "      <td>-0.065104</td>\n",
       "      <td>0.127686</td>\n",
       "      <td>-0.031738</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.182292</td>\n",
       "      <td>-0.149495</td>\n",
       "      <td>0.057251</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.085836</td>\n",
       "      <td>-0.180339</td>\n",
       "      <td>-0.055257</td>\n",
       "      <td>0.009542</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>-0.265706</td>\n",
       "      <td>0.073486</td>\n",
       "      <td>-0.054606</td>\n",
       "      <td>-0.061198</td>\n",
       "      <td>0.127223</td>\n",
       "      <td>0.051229</td>\n",
       "      <td>-0.170736</td>\n",
       "      <td>-0.05245</td>\n",
       "      <td>0.066813</td>\n",
       "      <td>0.055216</td>\n",
       "      <td>0.15918</td>\n",
       "      <td>-0.208008</td>\n",
       "      <td>-0.182129</td>\n",
       "      <td>-0.025879</td>\n",
       "      <td>-0.073324</td>\n",
       "      <td>-0.01237</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.051432</td>\n",
       "      <td>0.126363</td>\n",
       "      <td>-0.091227</td>\n",
       "      <td>-0.164185</td>\n",
       "      <td>-0.067464</td>\n",
       "      <td>-0.015137</td>\n",
       "      <td>-0.085531</td>\n",
       "      <td>0.055094</td>\n",
       "      <td>-0.141439</td>\n",
       "      <td>0.014974</td>\n",
       "      <td>-0.056966</td>\n",
       "      <td>-0.028809</td>\n",
       "      <td>-0.027262</td>\n",
       "      <td>-0.048706</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>-0.163574</td>\n",
       "      <td>-0.007894</td>\n",
       "      <td>-0.114258</td>\n",
       "      <td>-0.158203</td>\n",
       "      <td>-0.058512</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>-0.056844</td>\n",
       "      <td>-0.053385</td>\n",
       "      <td>0.063354</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>-0.142253</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>-0.045736</td>\n",
       "      <td>-0.042033</td>\n",
       "      <td>0.046987</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>-0.062826</td>\n",
       "      <td>-0.066081</td>\n",
       "      <td>-0.04777</td>\n",
       "      <td>0.038656</td>\n",
       "      <td>0.033529</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.082642</td>\n",
       "      <td>-0.078125</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.036214</td>\n",
       "      <td>-0.152832</td>\n",
       "      <td>0.25472</td>\n",
       "      <td>-0.013672</td>\n",
       "      <td>-0.063721</td>\n",
       "      <td>-0.125651</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>-0.002991</td>\n",
       "      <td>0.078242</td>\n",
       "      <td>-0.183431</td>\n",
       "      <td>-0.183838</td>\n",
       "      <td>0.157064</td>\n",
       "      <td>0.077474</td>\n",
       "      <td>-0.017049</td>\n",
       "      <td>0.025187</td>\n",
       "      <td>-0.011759</td>\n",
       "      <td>-0.11084</td>\n",
       "      <td>-0.094686</td>\n",
       "      <td>0.106974</td>\n",
       "      <td>-0.17806</td>\n",
       "      <td>0.056348</td>\n",
       "      <td>-0.158773</td>\n",
       "      <td>0.05306</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>0.140951</td>\n",
       "      <td>-0.034424</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>-0.057048</td>\n",
       "      <td>-0.156576</td>\n",
       "      <td>-0.014974</td>\n",
       "      <td>0.191121</td>\n",
       "      <td>0.015574</td>\n",
       "      <td>-0.012594</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>-0.000814</td>\n",
       "      <td>-0.024333</td>\n",
       "      <td>0.123698</td>\n",
       "      <td>0.172526</td>\n",
       "      <td>-0.055766</td>\n",
       "      <td>-0.105957</td>\n",
       "      <td>-0.061605</td>\n",
       "      <td>0.05485</td>\n",
       "      <td>0.08846</td>\n",
       "      <td>0.043294</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>0.105347</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>-0.017853</td>\n",
       "      <td>-0.113118</td>\n",
       "      <td>-0.303304</td>\n",
       "      <td>-0.09789</td>\n",
       "      <td>0.066203</td>\n",
       "      <td>0.056315</td>\n",
       "      <td>-0.090515</td>\n",
       "      <td>-0.113118</td>\n",
       "      <td>0.047811</td>\n",
       "      <td>-0.075928</td>\n",
       "      <td>-0.015381</td>\n",
       "      <td>0.018717</td>\n",
       "      <td>0.018717</td>\n",
       "      <td>-0.019368</td>\n",
       "      <td>-0.086304</td>\n",
       "      <td>-0.051595</td>\n",
       "      <td>0.061526</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>-0.060476</td>\n",
       "      <td>0.118571</td>\n",
       "      <td>0.104329</td>\n",
       "      <td>-0.006226</td>\n",
       "      <td>0.249349</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.076986</td>\n",
       "      <td>0.111287</td>\n",
       "      <td>-0.207357</td>\n",
       "      <td>-0.124064</td>\n",
       "      <td>-0.05837</td>\n",
       "      <td>0.099202</td>\n",
       "      <td>0.141357</td>\n",
       "      <td>0.039876</td>\n",
       "      <td>0.09493</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>-0.138835</td>\n",
       "      <td>-0.245687</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.069092</td>\n",
       "      <td>0.081706</td>\n",
       "      <td>0.022705</td>\n",
       "      <td>-0.067546</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.088623</td>\n",
       "      <td>0.109782</td>\n",
       "      <td>-0.175354</td>\n",
       "      <td>0.104126</td>\n",
       "      <td>-0.074707</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.067464</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>-0.053955</td>\n",
       "      <td>0.056478</td>\n",
       "      <td>0.055176</td>\n",
       "      <td>-0.096842</td>\n",
       "      <td>-0.020833</td>\n",
       "      <td>0.071859</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.068522</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9113</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3        4         5         6         7         8         9        10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32       33        34        35        36        37        38       39        40        41        42        43        44        45        46        47       48      49        50        51        52       53        54        55        56        57       58        59        60        61        62        63        64        65        66        67        68       69       70        71        72        73        74        75        76        77        78       79        80        81       82        83        84        85        86        87        88        89        90        91        92        93        94        95        96        97       98        99  \\\n",
       "9443 -0.063354  0.015055 -0.127116  0.055664 -0.07666 -0.111328 -0.004476 -0.130859  0.148763  0.015381  0.095835 -0.082113  0.023112  0.041992 -0.063232  0.176595  0.047791  0.068827  0.000814  0.072917  0.039714  0.004801 -0.130697 -0.138672  0.013018  0.010295 -0.103597  0.046224  0.167643  0.028768 -0.177083  0.079997 -0.026204 -0.10555 -0.044434  0.032288 -0.023071  0.133301  0.076497  0.03304  0.009018 -0.049886  0.048991 -0.166972 -0.024577 -0.147461  0.059896  0.010742  0.14091  0.0271 -0.012817  0.015625  0.062581 -0.09082  0.042013 -0.231608  0.029012 -0.036865  0.06665  0.036418  0.096842 -0.008626 -0.129781 -0.075521 -0.066447 -0.120361 -0.009572  0.056966  0.158529  0.16154 -0.03658 -0.037516 -0.003642 -0.131917 -0.201009 -0.108358 -0.025553  0.015381 -0.040436  0.02002 -0.031291 -0.091309  0.06901 -0.016602  0.061971 -0.024089 -0.119954  0.102397 -0.027995  0.075684  0.003418 -0.054688 -0.162069 -0.056152 -0.052409 -0.097493  0.134684  0.032633 -0.05717  0.018066   \n",
       "\n",
       "           100       101       102       103       104      105       106       107       108       109       110       111       112      113       114       115       116       117       118       119       120     121       122       123       124       125       126       127       128       129       130       131       132       133       134       135       136       137       138       139       140       141      142       143       144      145       146       147       148       149      150       151       152       153       154       155       156       157       158       159       160       161       162       163       164       165       166       167       168       169       170       171       172       173       174       175       176       177       178       179       180       181       182       183       184       185      186       187       188       189       190       191       192       193       194      195       196       197       198       199  \\\n",
       "9443  0.025065 -0.104533  0.061686  0.041504 -0.013387 -0.01062  0.011882  0.077637  0.152445 -0.071828 -0.009847  0.015299 -0.055908  0.00061 -0.220062 -0.024801  0.030268  0.121989  0.059082  0.096293 -0.104167 -0.1014 -0.065104  0.127686 -0.031738 -0.002238 -0.182292 -0.149495  0.057251  0.097656  0.085836 -0.180339 -0.055257  0.009542  0.064941 -0.265706  0.073486 -0.054606 -0.061198  0.127223  0.051229 -0.170736 -0.05245  0.066813  0.055216  0.15918 -0.208008 -0.182129 -0.025879 -0.073324 -0.01237  0.035156 -0.051432  0.126363 -0.091227 -0.164185 -0.067464 -0.015137 -0.085531  0.055094 -0.141439  0.014974 -0.056966 -0.028809 -0.027262 -0.048706  0.064209 -0.163574 -0.007894 -0.114258 -0.158203 -0.058512  0.029785 -0.056844 -0.053385  0.063354  0.063477  0.014608 -0.142253  0.080078 -0.045736 -0.042033  0.046987 -0.000651 -0.062826 -0.066081 -0.04777  0.038656  0.033529  0.038778  0.082642 -0.078125  0.050781  0.036214 -0.152832  0.25472 -0.013672 -0.063721 -0.125651 -0.277344   \n",
       "\n",
       "           200       201       202       203       204       205       206       207       208      209       210       211      212       213       214      215       216       217       218       219       220       221       222       223       224       225       226       227       228       229       230       231       232       233      234      235       236       237       238       239     240       241       242       243      244       245       246       247       248       249       250       251       252       253       254       255       256       257       258       259       260       261       262       263       264       265       266       267       268      269       270       271       272      273       274       275       276       277       278       279       280       281       282       283       284       285       286       287       288       289       290       291       292       293       294       295       296       297       298       299  \\\n",
       "9443 -0.002991  0.078242 -0.183431 -0.183838  0.157064  0.077474 -0.017049  0.025187 -0.011759 -0.11084 -0.094686  0.106974 -0.17806  0.056348 -0.158773  0.05306  0.239258  0.140951 -0.034424 -0.002797 -0.057048 -0.156576 -0.014974  0.191121  0.015574 -0.012594 -0.008545 -0.000814 -0.024333  0.123698  0.172526 -0.055766 -0.105957 -0.061605  0.05485  0.08846  0.043294  0.020447  0.105347 -0.008301 -0.0118 -0.017853 -0.113118 -0.303304 -0.09789  0.066203  0.056315 -0.090515 -0.113118  0.047811 -0.075928 -0.015381  0.018717  0.018717 -0.019368 -0.086304 -0.051595  0.061526  0.036438 -0.060476  0.118571  0.104329 -0.006226  0.249349  0.008057  0.076986  0.111287 -0.207357 -0.124064 -0.05837  0.099202  0.141357  0.039876  0.09493 -0.094238  0.105469 -0.138835 -0.245687 -0.046631  0.069092  0.081706  0.022705 -0.067546  0.114583  0.088623  0.109782 -0.175354  0.104126 -0.074707  0.083984  0.067464  0.001546 -0.053955  0.056478  0.055176 -0.096842 -0.020833  0.071859  0.132812 -0.068522   \n",
       "\n",
       "      Top       Date  Label  topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  topic_7  topic_8  topic_9  sentiment  \n",
       "9443    1 2014-12-31      0   0.9113   0.0174   0.0139   0.0116      0.0      0.0      0.0      0.0      0.0      0.0        0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 535 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono el archivo con el que se corre el modelo\n",
    "data = embeddings_average_individual[embeddings_average_individual['Date']<='2014-12-31']\n",
    "print(data['Label'].mean())\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6294, 1, 311)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "training = data[:num_training]\n",
    "testing = data[num_training:]\n",
    "\n",
    "# Se separa en train y test\n",
    "x_train = data.drop([\"Top\",\"Label\", \"Date\"], axis=1)[:num_training]\n",
    "x_test = data.drop([\"Top\",'Label', 'Date'], axis=1)[num_training:]\n",
    "y_train = data[\"Label\"].values[:num_training]\n",
    "y_test = data[\"Label\"].values[num_training:]\n",
    "\n",
    "\n",
    "x_train_array = x_train.to_numpy()\n",
    "reshape_x_train = x_train_array.reshape(len(x_train), 1, x_train.shape[1])\n",
    "reshape_x_train.shape\n",
    "\n",
    "x_test_array = x_test.to_numpy()\n",
    "reshape_x_test = x_test_array.reshape(len(x_test), 1, x_train.shape[1])\n",
    "reshape_x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir espacio de busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'units1': hp.choice('units1', [8, 16, 32, 64, 128, 256, 512]),\n",
    "    'units2': hp.choice('units2', [8, 16, 32, 64, 128, 256, 512]),\n",
    "                 \n",
    "    'dropout1': hp.choice('dropout1', [0.1,0.2,0.3, 0.4]),\n",
    "    \n",
    "    'batch_size' : hp.choice('batch_size', [16,64,128,256]),\n",
    "    \n",
    "    'nb_epochs' : hp.choice('nb_epochs', [50]),\n",
    "\n",
    "    'optimizer':  hp.choice('optimizer', [ 'adam','adadelta']),   \n",
    "    'activation': hp.choice('activation', [ 'relu','sigmoid','softmax']), \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir busqueda bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 274 ms\n"
     ]
    }
   ],
   "source": [
    "#Objective function that hyperopt will minimize\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def objective(params):\n",
    "    \n",
    "#     import ml_metrics\n",
    "\n",
    "    \n",
    "    start = timer()\n",
    "    print ('Params testing: ', params)\n",
    "    print ('\\n ')\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['units1'], input_shape=(1,x_train.shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "    model.add(LSTM(params['units2'], return_sequences=False))\n",
    "    model.add(Dense(1,  activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer=params['optimizer'], loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    logdir = \"Resultados\\\\\" + exp_name +\"\\\\logs\\\\model\"\n",
    "\n",
    "    tensor_board = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, profile_batch = 100000000)\n",
    "\n",
    "    \n",
    "    #includes the call back object\n",
    "    model.fit(reshape_x_train, y_train, epochs=params['nb_epochs'], batch_size=params['batch_size'],\n",
    "               verbose = 0, validation_data=(reshape_x_test, y_test),callbacks=[tensor_board])\n",
    "     \n",
    "    #predict the test set \n",
    "    ypred = model.predict_proba(reshape_x_test)\n",
    "    testing_cp = testing.copy()\n",
    "    testing_cp['Prob'] = ypred\n",
    "    testing_cp['Prob_dia'] = testing_cp['Prob'].groupby(testing_cp['Date']).transform('mean')\n",
    "    testing_cp['Prediction'] = 0\n",
    "    testing_cp.loc[testing_cp['Prob_dia']>0.5, 'Prediction'] = 1\n",
    "    testing_cp.drop_duplicates(subset=['Date','Prediction','Label'], inplace=True)\n",
    "    \n",
    "    acc = accuracy_score(testing_cp.Label, testing_cp.Prediction)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([-acc, params, run_time])\n",
    "    of_connection.close()\n",
    "    \n",
    "    \n",
    "    print('Test accuracy:', acc)\n",
    " \n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'train_time': run_time,}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenar resultados de cada iteracin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import tpe\n",
    "\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from hyperopt import Trials\n",
    "\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# File to save first results\n",
    "out_file = folder + '/gbm_results.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanzar optimizacin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params testing:                                                                                                        \n",
      "{'activation': 'softmax', 'batch_size': 64, 'dropout1': 0.2, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 256, 'units2': 32}\n",
      "Test accuracy:                                                                                                         \n",
      "0.4901185770750988                                                                                                     \n",
      "Params testing:                                                                                                        \n",
      "{'activation': 'sigmoid', 'batch_size': 256, 'dropout1': 0.2, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 16, 'units2': 8}\n",
      "Test accuracy:                                                                                                         \n",
      "0.4743083003952569                                                                                                     \n",
      "Params testing:                                                                                                        \n",
      "{'activation': 'softmax', 'batch_size': 64, 'dropout1': 0.4, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 512, 'units2': 8}\n",
      "Test accuracy:                                                                                                         \n",
      "0.4901185770750988                                                                                                     \n",
      "Params testing:                                                                                                        \n",
      "{'activation': 'softmax', 'batch_size': 64, 'dropout1': 0.2, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 8, 'units2': 128}\n",
      "Test accuracy:                                                                                                         \n",
      "0.4901185770750988                                                                                                     \n",
      "Params testing:                                                                                                        \n",
      "{'activation': 'sigmoid', 'batch_size': 64, 'dropout1': 0.3, 'nb_epochs': 50, 'optimizer': 'adam', 'units1': 256, 'units2': 256}\n",
      "Test accuracy:                                                                                                         \n",
      "0.44664031620553357                                                                                                    \n",
      "100%|| 5/5 [36:59<00:00, 443.83s/trial, best loss: -0.4901185770750988]\n",
      "time: 36min 59s\n"
     ]
    }
   ],
   "source": [
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = 5, trials = bayes_trials,\n",
    "            verbose = 1, rstate= np.random.RandomState(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar bayesiana, por si quisiera retomar donde queda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 149 ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(bayes_trials, open(folder + '/trials.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer mejores parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': -0.4901185770750988, 'status': 'ok', 'train_time': 475.05156},\n",
       " {'loss': -0.4901185770750988,\n",
       "  'status': 'ok',\n",
       "  'train_time': 666.5135124000001}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 263 ms\n"
     ]
    }
   ],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "bayes_trials_results  = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results [:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.490119</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 64, 'd...</td>\n",
       "      <td>475.051560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.490119</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 64, 'd...</td>\n",
       "      <td>666.513512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.490119</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 64, 'd...</td>\n",
       "      <td>449.323307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.474308</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 256, '...</td>\n",
       "      <td>126.620175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.446640</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 64, 'd...</td>\n",
       "      <td>501.471922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                             params        time\n",
       "0 -0.490119  {'activation': 'softmax', 'batch_size': 64, 'd...  475.051560\n",
       "1 -0.490119  {'activation': 'softmax', 'batch_size': 64, 'd...  666.513512\n",
       "2 -0.490119  {'activation': 'softmax', 'batch_size': 64, 'd...  449.323307\n",
       "3 -0.474308  {'activation': 'sigmoid', 'batch_size': 256, '...  126.620175\n",
       "4 -0.446640  {'activation': 'sigmoid', 'batch_size': 64, 'd...  501.471922"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 293 ms\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(folder + '/gbm_results.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'softmax',\n",
       " 'batch_size': 64,\n",
       " 'dropout1': 0.2,\n",
       " 'nb_epochs': 50,\n",
       " 'optimizer': 'adam',\n",
       " 'units1': 256,\n",
       " 'units2': 32}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 51 ms\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'softmax',\n",
       " 'batch_size': 64,\n",
       " 'dropout1': 0.2,\n",
       " 'nb_epochs': 50,\n",
       " 'optimizer': 'adam',\n",
       " 'units1': 256,\n",
       " 'units2': 32}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 286 ms\n"
     ]
    }
   ],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "best_bayes_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir datasets de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 476 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono la fecha para la cual hago el corte de train y test\n",
    "training_end = pd.to_datetime(\"2014-12-31\")\n",
    "num_training = len(embeddings_average_individual[(embeddings_average_individual[\"Date\"]) <= training_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Top</th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.02655</td>\n",
       "      <td>-0.005437</td>\n",
       "      <td>-0.049437</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>-0.08665</td>\n",
       "      <td>0.110962</td>\n",
       "      <td>0.086507</td>\n",
       "      <td>0.01122</td>\n",
       "      <td>-0.123851</td>\n",
       "      <td>-0.069445</td>\n",
       "      <td>0.046326</td>\n",
       "      <td>-0.092684</td>\n",
       "      <td>0.105031</td>\n",
       "      <td>0.078355</td>\n",
       "      <td>0.042023</td>\n",
       "      <td>-0.022827</td>\n",
       "      <td>-0.063538</td>\n",
       "      <td>-0.094391</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>-0.017698</td>\n",
       "      <td>-0.034032</td>\n",
       "      <td>0.052694</td>\n",
       "      <td>-0.043579</td>\n",
       "      <td>-0.059591</td>\n",
       "      <td>0.028366</td>\n",
       "      <td>0.14961</td>\n",
       "      <td>-0.076497</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>-0.050603</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.04453</td>\n",
       "      <td>-0.023275</td>\n",
       "      <td>-0.015015</td>\n",
       "      <td>0.078247</td>\n",
       "      <td>-0.093613</td>\n",
       "      <td>-0.037659</td>\n",
       "      <td>0.036929</td>\n",
       "      <td>0.054586</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>0.146851</td>\n",
       "      <td>-0.059499</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>-0.150101</td>\n",
       "      <td>0.038727</td>\n",
       "      <td>0.11497</td>\n",
       "      <td>-0.060374</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>-0.03419</td>\n",
       "      <td>-0.04878</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>0.031077</td>\n",
       "      <td>0.054706</td>\n",
       "      <td>-0.023376</td>\n",
       "      <td>-0.059611</td>\n",
       "      <td>-0.116048</td>\n",
       "      <td>0.019796</td>\n",
       "      <td>-0.092779</td>\n",
       "      <td>0.069397</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>-0.044083</td>\n",
       "      <td>-0.059352</td>\n",
       "      <td>0.062612</td>\n",
       "      <td>-0.028783</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.037933</td>\n",
       "      <td>-0.076304</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.004979</td>\n",
       "      <td>0.091044</td>\n",
       "      <td>0.03831</td>\n",
       "      <td>0.056834</td>\n",
       "      <td>-0.062567</td>\n",
       "      <td>-0.011286</td>\n",
       "      <td>0.048192</td>\n",
       "      <td>0.126668</td>\n",
       "      <td>0.06871</td>\n",
       "      <td>0.169627</td>\n",
       "      <td>0.064963</td>\n",
       "      <td>-0.069956</td>\n",
       "      <td>-0.055331</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>-0.10978</td>\n",
       "      <td>-0.008942</td>\n",
       "      <td>-0.049391</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0.043355</td>\n",
       "      <td>0.040724</td>\n",
       "      <td>0.062861</td>\n",
       "      <td>-0.090495</td>\n",
       "      <td>-0.065338</td>\n",
       "      <td>-0.054422</td>\n",
       "      <td>0.01357</td>\n",
       "      <td>0.06425</td>\n",
       "      <td>0.084117</td>\n",
       "      <td>0.069021</td>\n",
       "      <td>-0.048922</td>\n",
       "      <td>-0.04539</td>\n",
       "      <td>-0.105464</td>\n",
       "      <td>0.040438</td>\n",
       "      <td>-0.05098</td>\n",
       "      <td>-0.042369</td>\n",
       "      <td>-0.259115</td>\n",
       "      <td>-0.031316</td>\n",
       "      <td>-0.110067</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>-0.033813</td>\n",
       "      <td>0.068558</td>\n",
       "      <td>0.008415</td>\n",
       "      <td>0.065531</td>\n",
       "      <td>-0.009593</td>\n",
       "      <td>-0.06676</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>-0.025653</td>\n",
       "      <td>-0.080393</td>\n",
       "      <td>0.055687</td>\n",
       "      <td>0.082654</td>\n",
       "      <td>-0.020147</td>\n",
       "      <td>-0.07518</td>\n",
       "      <td>-0.099518</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>0.040599</td>\n",
       "      <td>0.012319</td>\n",
       "      <td>0.06191</td>\n",
       "      <td>-0.037506</td>\n",
       "      <td>0.02595</td>\n",
       "      <td>0.158768</td>\n",
       "      <td>0.070648</td>\n",
       "      <td>-0.050069</td>\n",
       "      <td>-0.090535</td>\n",
       "      <td>-0.075063</td>\n",
       "      <td>-0.069895</td>\n",
       "      <td>-0.022481</td>\n",
       "      <td>-0.012672</td>\n",
       "      <td>-0.089986</td>\n",
       "      <td>-0.050354</td>\n",
       "      <td>0.162383</td>\n",
       "      <td>-0.022855</td>\n",
       "      <td>-0.096497</td>\n",
       "      <td>-0.021907</td>\n",
       "      <td>0.029973</td>\n",
       "      <td>-0.092219</td>\n",
       "      <td>0.045115</td>\n",
       "      <td>-0.038025</td>\n",
       "      <td>-0.053009</td>\n",
       "      <td>-0.085876</td>\n",
       "      <td>-0.159342</td>\n",
       "      <td>0.082438</td>\n",
       "      <td>0.062836</td>\n",
       "      <td>-0.054595</td>\n",
       "      <td>-0.007467</td>\n",
       "      <td>0.022705</td>\n",
       "      <td>-0.07308</td>\n",
       "      <td>-0.130718</td>\n",
       "      <td>-0.04321</td>\n",
       "      <td>-0.057373</td>\n",
       "      <td>-0.01974</td>\n",
       "      <td>-0.043142</td>\n",
       "      <td>0.01186</td>\n",
       "      <td>0.069222</td>\n",
       "      <td>-0.02478</td>\n",
       "      <td>-0.002594</td>\n",
       "      <td>-0.140828</td>\n",
       "      <td>0.052887</td>\n",
       "      <td>-0.054372</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.089589</td>\n",
       "      <td>-0.046539</td>\n",
       "      <td>-0.08475</td>\n",
       "      <td>-0.069346</td>\n",
       "      <td>-0.071147</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>-0.05452</td>\n",
       "      <td>0.116689</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>-0.027476</td>\n",
       "      <td>-0.040019</td>\n",
       "      <td>-0.104167</td>\n",
       "      <td>-0.114581</td>\n",
       "      <td>0.024998</td>\n",
       "      <td>0.053182</td>\n",
       "      <td>-0.035671</td>\n",
       "      <td>0.055969</td>\n",
       "      <td>-0.023046</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.02793</td>\n",
       "      <td>0.093552</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.043393</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.069979</td>\n",
       "      <td>0.029744</td>\n",
       "      <td>0.053935</td>\n",
       "      <td>0.099854</td>\n",
       "      <td>-0.062459</td>\n",
       "      <td>-0.086884</td>\n",
       "      <td>-0.032506</td>\n",
       "      <td>-0.041692</td>\n",
       "      <td>0.032838</td>\n",
       "      <td>-0.032617</td>\n",
       "      <td>0.01623</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>-0.093465</td>\n",
       "      <td>-0.01297</td>\n",
       "      <td>0.085303</td>\n",
       "      <td>0.149264</td>\n",
       "      <td>-0.11233</td>\n",
       "      <td>-0.01181</td>\n",
       "      <td>0.118187</td>\n",
       "      <td>-0.024419</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>-0.171748</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>0.078379</td>\n",
       "      <td>-0.029582</td>\n",
       "      <td>-0.053752</td>\n",
       "      <td>0.04306</td>\n",
       "      <td>-0.01981</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.044607</td>\n",
       "      <td>-0.041189</td>\n",
       "      <td>0.126353</td>\n",
       "      <td>-0.083649</td>\n",
       "      <td>0.070443</td>\n",
       "      <td>0.065664</td>\n",
       "      <td>-0.031071</td>\n",
       "      <td>-0.076167</td>\n",
       "      <td>0.022043</td>\n",
       "      <td>-0.056544</td>\n",
       "      <td>-0.033414</td>\n",
       "      <td>-0.07841</td>\n",
       "      <td>-0.007584</td>\n",
       "      <td>0.028048</td>\n",
       "      <td>0.047563</td>\n",
       "      <td>-0.03553</td>\n",
       "      <td>0.097061</td>\n",
       "      <td>-0.034648</td>\n",
       "      <td>0.016682</td>\n",
       "      <td>-0.11674</td>\n",
       "      <td>-0.016691</td>\n",
       "      <td>-0.115661</td>\n",
       "      <td>-0.014822</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>-0.002602</td>\n",
       "      <td>0.033976</td>\n",
       "      <td>0.067755</td>\n",
       "      <td>-0.045471</td>\n",
       "      <td>0.069234</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>-0.015391</td>\n",
       "      <td>0.020459</td>\n",
       "      <td>0.020137</td>\n",
       "      <td>0.049369</td>\n",
       "      <td>0.018361</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>-0.077982</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>-0.023763</td>\n",
       "      <td>-0.021281</td>\n",
       "      <td>-0.099264</td>\n",
       "      <td>-0.009176</td>\n",
       "      <td>0.163818</td>\n",
       "      <td>0.149923</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>0.049657</td>\n",
       "      <td>-0.090281</td>\n",
       "      <td>-0.018667</td>\n",
       "      <td>0.099538</td>\n",
       "      <td>0.063007</td>\n",
       "      <td>0.127879</td>\n",
       "      <td>0.02595</td>\n",
       "      <td>0.134786</td>\n",
       "      <td>-0.101334</td>\n",
       "      <td>-0.123652</td>\n",
       "      <td>-0.076009</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>-0.029872</td>\n",
       "      <td>0.062391</td>\n",
       "      <td>-0.020111</td>\n",
       "      <td>0.026784</td>\n",
       "      <td>0.067586</td>\n",
       "      <td>-0.012716</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>-0.00868</td>\n",
       "      <td>0.096343</td>\n",
       "      <td>0.052673</td>\n",
       "      <td>0.040049</td>\n",
       "      <td>-0.090737</td>\n",
       "      <td>-0.015834</td>\n",
       "      <td>0.056478</td>\n",
       "      <td>-0.03123</td>\n",
       "      <td>-0.096171</td>\n",
       "      <td>-0.045821</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>0.038754</td>\n",
       "      <td>0.031172</td>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.287754</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>0.490658</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6        7         8         9       10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27       28        29        30        31       32       33        34        35        36        37        38        39        40      41        42        43      44        45        46       47        48        49       50       51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68     69        70        71       72        73        74        75        76        77       78        79        80        81        82        83       84        85        86        87      88        89        90        91        92        93        94       95       96        97        98        99      100       101  \\\n",
       "0  0.02655 -0.005437 -0.049437  0.075195  0.007438  0.003312  0.160296 -0.08665  0.110962  0.086507  0.01122 -0.123851 -0.069445  0.046326 -0.092684  0.105031  0.078355  0.042023 -0.022827 -0.063538 -0.094391 -0.001472 -0.017698 -0.034032  0.052694 -0.043579 -0.059591  0.028366  0.14961 -0.076497 -0.001678 -0.050603 -0.16626 -0.04453 -0.023275 -0.015015  0.078247 -0.093613 -0.037659  0.036929  0.054586 -0.0861  0.146851 -0.059499  0.0556 -0.150101  0.038727  0.11497 -0.060374  0.033613 -0.03419 -0.04878 -0.002706  0.031077  0.054706 -0.023376 -0.059611 -0.116048  0.019796 -0.092779  0.069397  0.028481 -0.044083 -0.059352  0.062612 -0.028783  0.004842  0.037933 -0.076304  0.046 -0.004979  0.091044  0.03831  0.056834 -0.062567 -0.011286  0.048192  0.126668  0.06871  0.169627  0.064963 -0.069956 -0.055331  0.010803 -0.10978 -0.008942 -0.049391  0.071823  0.0896  0.043355  0.040724  0.062861 -0.090495 -0.065338 -0.054422  0.01357  0.06425  0.084117  0.069021 -0.048922 -0.04539 -0.105464   \n",
       "\n",
       "        102      103       104       105       106       107       108       109       110       111       112       113      114       115       116       117       118       119       120      121       122       123       124       125      126       127      128       129       130       131       132       133       134       135       136       137       138       139       140       141       142       143       144       145       146       147       148       149       150       151       152       153       154      155       156      157       158      159       160      161       162      163       164       165       166       167       168       169       170      171       172       173       174      175       176       177       178       179       180       181       182       183       184       185       186       187      188       189       190       191       192       193       194       195       196       197       198       199       200       201  \\\n",
       "0  0.040438 -0.05098 -0.042369 -0.259115 -0.031316 -0.110067  0.004466 -0.033813  0.068558  0.008415  0.065531 -0.009593 -0.06676  0.004476 -0.025653 -0.080393  0.055687  0.082654 -0.020147 -0.07518 -0.099518  0.007182  0.040599  0.012319  0.06191 -0.037506  0.02595  0.158768  0.070648 -0.050069 -0.090535 -0.075063 -0.069895 -0.022481 -0.012672 -0.089986 -0.050354  0.162383 -0.022855 -0.096497 -0.021907  0.029973 -0.092219  0.045115 -0.038025 -0.053009 -0.085876 -0.159342  0.082438  0.062836 -0.054595 -0.007467  0.022705 -0.07308 -0.130718 -0.04321 -0.057373 -0.01974 -0.043142  0.01186  0.069222 -0.02478 -0.002594 -0.140828  0.052887 -0.054372  0.000005 -0.089589 -0.046539 -0.08475 -0.069346 -0.071147 -0.012772 -0.05452  0.116689  0.066132 -0.027476 -0.040019 -0.104167 -0.114581  0.024998  0.053182 -0.035671  0.055969 -0.023046 -0.008371  0.02793  0.093552  0.104046  0.043393  0.020592  0.069979  0.029744  0.053935  0.099854 -0.062459 -0.086884 -0.032506 -0.041692  0.032838   \n",
       "\n",
       "        202      203       204       205      206       207       208      209      210       211       212       213       214       215       216       217       218      219      220       221       222       223       224       225       226       227       228       229       230       231       232      233       234       235       236      237       238       239       240      241       242       243       244       245       246       247       248       249       250       251       252       253       254       255       256       257       258       259       260       261       262       263       264       265       266       267       268       269       270       271       272      273       274       275       276       277       278       279       280       281       282       283       284       285       286       287      288       289       290       291       292       293       294      295       296       297       298       299  Top       Date  Label  \\\n",
       "0 -0.032617  0.01623  0.090942 -0.093465 -0.01297  0.085303  0.149264 -0.11233 -0.01181  0.118187 -0.024419  0.003143 -0.171748  0.052714  0.078379 -0.029582 -0.053752  0.04306 -0.01981 -0.013031  0.044607 -0.041189  0.126353 -0.083649  0.070443  0.065664 -0.031071 -0.076167  0.022043 -0.056544 -0.033414 -0.07841 -0.007584  0.028048  0.047563 -0.03553  0.097061 -0.034648  0.016682 -0.11674 -0.016691 -0.115661 -0.014822  0.038366 -0.002602  0.033976  0.067755 -0.045471  0.069234  0.088664 -0.015391  0.020459  0.020137  0.049369  0.018361  0.014715 -0.077982  0.004481 -0.023763 -0.021281 -0.099264 -0.009176  0.163818  0.149923  0.010976  0.049657 -0.090281 -0.018667  0.099538  0.063007  0.127879  0.02595  0.134786 -0.101334 -0.123652 -0.076009  0.004648 -0.029872  0.062391 -0.020111  0.026784  0.067586 -0.012716  0.014837  0.008809  0.018158 -0.00868  0.096343  0.052673  0.040049 -0.090737 -0.015834  0.056478 -0.03123 -0.096171 -0.045821  0.025391  0.006185    1 2016-07-01      1   \n",
       "\n",
       "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5  topic_6   topic_7   topic_8   topic_9  sentiment  \n",
       "0  0.051395  0.038754  0.031172  0.026116  0.022466  0.019712  0.01756  0.287754  0.014414  0.490658   0.033333  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 448 ms\n"
     ]
    }
   ],
   "source": [
    "# Selecciono el archivo con el que se corre el modelo\n",
    "data = embeddings_average_individual\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9443, 1, 311)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 267 ms\n"
     ]
    }
   ],
   "source": [
    "training = data[:num_training]\n",
    "testing = data[num_training:]\n",
    "\n",
    "# Se separa en train y test\n",
    "x_train = data.drop([\"Top\",\"Label\", \"Date\"], axis=1)[:num_training]\n",
    "x_test = data.drop([\"Top\",'Label', 'Date'], axis=1)[num_training:]\n",
    "y_train = data[\"Label\"].values[:num_training]\n",
    "y_test = data[\"Label\"].values[num_training:]\n",
    "\n",
    "\n",
    "x_train_array = x_train.to_numpy()\n",
    "reshape_x_train = x_train_array.reshape(len(x_train), 1, x_train.shape[1])\n",
    "reshape_x_train.shape\n",
    "\n",
    "x_test_array = x_test.to_numpy()\n",
    "reshape_x_test = x_test_array.reshape(len(x_test), 1, x_train.shape[1])\n",
    "reshape_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIKq7z8tnIWl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(best_bayes_params['units1'], input_shape=(1,x_train.shape[1]), return_sequences=True))\n",
    "model.add(Dropout(best_bayes_params['dropout1']))\n",
    "model.add(LSTM(best_bayes_params['units2'], return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# define the checkpoint\n",
    "filepath= ch_folder + \"/word2vec-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 999 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logdir = \"Resultados\\\\\" + exp_name +\"\\\\logs\\\\model\"\n",
    "\n",
    "\n",
    "tensor_board = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, profile_batch = 100000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1589755592715,
     "user": {
      "displayName": "Melina D'Alessandro",
      "photoUrl": "https://lh4.googleusercontent.com/-AU_sxBOTu8w/AAAAAAAAAAI/AAAAAAAAAR8/nO0zS5J_9Wo/s64/photo.jpg",
      "userId": "09190509655785270416"
     },
     "user_tz": 180
    },
    "id": "JsHgNLFnnTLN",
    "outputId": "4c22910d-c7b2-4dff-eb32-15c2574174ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40208 samples\n",
      "Epoch 1/50\n",
      "40208/40208 [==============================] - 15s 384us/sample - loss: 0.6904 - accuracy: 0.53883 - - ETA: 8s - - ETA: 2s - loss: 0 - ETA: 1s\n",
      "Epoch 2/50\n",
      "40208/40208 [==============================] - 11s 262us/sample - loss: 0.6897 - accuracy: 0.5386\n",
      "Epoch 3/50\n",
      "40208/40208 [==============================] - 11s 262us/sample - loss: 0.6891 - accuracy: 0.5405\n",
      "Epoch 4/50\n",
      "40208/40208 [==============================] - 11s 272us/sample - loss: 0.6883 - accuracy: 0.5408- loss: 0.6882 - accuracy: 0.\n",
      "Epoch 5/50\n",
      "40208/40208 [==============================] - 11s 265us/sample - loss: 0.6877 - accuracy: 0.5423\n",
      "Epoch 6/50\n",
      "40208/40208 [==============================] - 11s 264us/sample - loss: 0.6864 - accuracy: 0.5437\n",
      "Epoch 7/50\n",
      "40208/40208 [==============================] - 11s 275us/sample - loss: 0.6847 - accuracy: 0.5483\n",
      "Epoch 8/50\n",
      "40208/40208 [==============================] - 12s 290us/sample - loss: 0.6829 - accuracy: 0.5495\n",
      "Epoch 9/50\n",
      "40208/40208 [==============================] - 11s 277us/sample - loss: 0.6804 - accuracy: 0.5554\n",
      "Epoch 10/50\n",
      "40208/40208 [==============================] - 11s 266us/sample - loss: 0.6772 - accuracy: 0.5587\n",
      "Epoch 11/50\n",
      "40208/40208 [==============================] - 11s 264us/sample - loss: 0.6730 - accuracy: 0.5637\n",
      "Epoch 12/50\n",
      "40208/40208 [==============================] - 11s 271us/sample - loss: 0.6676 - accuracy: 0.5730- loss: 0.667\n",
      "Epoch 13/50\n",
      "40208/40208 [==============================] - 11s 279us/sample - loss: 0.6620 - accuracy: 0.5782\n",
      "Epoch 14/50\n",
      "40208/40208 [==============================] - 11s 285us/sample - loss: 0.6560 - accuracy: 0.5865\n",
      "Epoch 15/50\n",
      "40208/40208 [==============================] - 13s 320us/sample - loss: 0.6482 - accuracy: 0.5980\n",
      "Epoch 16/50\n",
      "40208/40208 [==============================] - 11s 265us/sample - loss: 0.6386 - accuracy: 0.6086\n",
      "Epoch 17/50\n",
      "40208/40208 [==============================] - 11s 275us/sample - loss: 0.6298 - accuracy: 0.6207\n",
      "Epoch 18/50\n",
      "40208/40208 [==============================] - 11s 269us/sample - loss: 0.6202 - accuracy: 0.6304- los\n",
      "Epoch 19/50\n",
      "40208/40208 [==============================] - 11s 264us/sample - loss: 0.6090 - accuracy: 0.6427\n",
      "Epoch 20/50\n",
      "40208/40208 [==============================] - 11s 272us/sample - loss: 0.5989 - accuracy: 0.6501\n",
      "Epoch 21/50\n",
      "40208/40208 [==============================] - 11s 264us/sample - loss: 0.5873 - accuracy: 0.6611\n",
      "Epoch 22/50\n",
      "40208/40208 [==============================] - 11s 267us/sample - loss: 0.5771 - accuracy: 0.6715\n",
      "Epoch 23/50\n",
      "40208/40208 [==============================] - 11s 272us/sample - loss: 0.5658 - accuracy: 0.6794\n",
      "Epoch 24/50\n",
      "40208/40208 [==============================] - 11s 263us/sample - loss: 0.5537 - accuracy: 0.6919\n",
      "Epoch 25/50\n",
      "40208/40208 [==============================] - 11s 270us/sample - loss: 0.5450 - accuracy: 0.6989- loss: 0.5449 - accuracy\n",
      "Epoch 26/50\n",
      "40208/40208 [==============================] - 13s 330us/sample - loss: 0.5339 - accuracy: 0.7072\n",
      "Epoch 27/50\n",
      "40208/40208 [==============================] - 11s 285us/sample - loss: 0.5212 - accuracy: 0.7184- loss: 0\n",
      "Epoch 28/50\n",
      "40208/40208 [==============================] - 11s 276us/sample - loss: 0.5113 - accuracy: 0.7234- loss:\n",
      "Epoch 29/50\n",
      "40208/40208 [==============================] - 11s 264us/sample - loss: 0.4988 - accuracy: 0.7362\n",
      "Epoch 30/50\n",
      "40208/40208 [==============================] - 11s 266us/sample - loss: 0.4887 - accuracy: 0.7401- loss: - ETA: 2s - los\n",
      "Epoch 31/50\n",
      "40208/40208 [==============================] - 11s 272us/sample - loss: 0.4797 - accuracy: 0.7495\n",
      "Epoch 32/50\n",
      "40208/40208 [==============================] - 11s 266us/sample - loss: 0.4662 - accuracy: 0.7585- loss: 0.4664 - accuracy: \n",
      "Epoch 33/50\n",
      "40208/40208 [==============================] - 11s 271us/sample - loss: 0.4564 - accuracy: 0.7626- loss: 0.4559 - \n",
      "Epoch 34/50\n",
      "40208/40208 [==============================] - 11s 276us/sample - loss: 0.4483 - accuracy: 0.7693\n",
      "Epoch 35/50\n",
      "40208/40208 [==============================] - 11s 266us/sample - loss: 0.4389 - accuracy: 0.7749\n",
      "Epoch 36/50\n",
      "40208/40208 [==============================] - 11s 270us/sample - loss: 0.4267 - accuracy: 0.7853\n",
      "Epoch 37/50\n",
      "40208/40208 [==============================] - 11s 268us/sample - loss: 0.4170 - accuracy: 0.7901\n",
      "Epoch 38/50\n",
      "40208/40208 [==============================] - 12s 290us/sample - loss: 0.4074 - accuracy: 0.7948\n",
      "Epoch 39/50\n",
      "40208/40208 [==============================] - 11s 276us/sample - loss: 0.3990 - accuracy: 0.8018\n",
      "Epoch 40/50\n",
      "40208/40208 [==============================] - 12s 291us/sample - loss: 0.3924 - accuracy: 0.8052\n",
      "Epoch 41/50\n",
      "40208/40208 [==============================] - 12s 305us/sample - loss: 0.3841 - accuracy: 0.8128\n",
      "Epoch 42/50\n",
      "40208/40208 [==============================] - 10s 252us/sample - loss: 0.3748 - accuracy: 0.8185\n",
      "Epoch 43/50\n",
      "40208/40208 [==============================] - 10s 242us/sample - loss: 0.3643 - accuracy: 0.8234\n",
      "Epoch 44/50\n",
      "40208/40208 [==============================] - 10s 252us/sample - loss: 0.3578 - accuracy: 0.8275- loss: 0.3582 \n",
      "Epoch 45/50\n",
      "40208/40208 [==============================] - 10s 241us/sample - loss: 0.3502 - accuracy: 0.8311\n",
      "Epoch 46/50\n",
      "40208/40208 [==============================] - 9s 233us/sample - loss: 0.3439 - accuracy: 0.8367\n",
      "Epoch 47/50\n",
      "40208/40208 [==============================] - 9s 230us/sample - loss: 0.3348 - accuracy: 0.8412 - loss: 0.327 - ETA: 4s - l - ETA\n",
      "Epoch 48/50\n",
      "40208/40208 [==============================] - 10s 237us/sample - loss: 0.3265 - accuracy: 0.8451\n",
      "Epoch 49/50\n",
      "40208/40208 [==============================] - 10s 252us/sample - loss: 0.3203 - accuracy: 0.8510\n",
      "Epoch 50/50\n",
      "40208/40208 [==============================] - 11s 264us/sample - loss: 0.3153 - accuracy: 0.8542- loss: 0.3151 - accuracy: \n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 1, 256)            581632    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 32)                36992     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 618,657\n",
      "Trainable params: 618,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 9min 11s\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(reshape_x_train, y_train,\n",
    "          epochs=best_bayes_params['nb_epochs'], \n",
    "          batch_size=best_bayes_params['batch_size'], callbacks=[tensor_board])\n",
    "\n",
    "\n",
    "model.save(folder + '/keras_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9112\n",
      "Testing Accuracy:  0.4944\n",
      "time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(reshape_x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(reshape_x_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1165,
     "status": "ok",
     "timestamp": 1589755595730,
     "user": {
      "displayName": "Melina D'Alessandro",
      "photoUrl": "https://lh4.googleusercontent.com/-AU_sxBOTu8w/AAAAAAAAAAI/AAAAAAAAAR8/nO0zS5J_9Wo/s64/photo.jpg",
      "userId": "09190509655785270416"
     },
     "user_tz": 180
    },
    "id": "mdsR4Qngv5Q1",
    "outputId": "433bbcf6-d6eb-44c1-a1bb-afee72ac0ffb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.6996182e-01],\n",
       "       [9.9030650e-01],\n",
       "       [8.9112395e-01],\n",
       "       ...,\n",
       "       [9.9999976e-01],\n",
       "       [4.9201171e-03],\n",
       "       [4.3578405e-04]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.42 s\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "ypred = model.predict_proba(reshape_x_test)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de das en testing:  379\n",
      "Numero de registros en testing:  379\n",
      "time: 76 ms\n"
     ]
    }
   ],
   "source": [
    "testing_cp = testing.copy()\n",
    "testing_cp['Prob'] = ypred\n",
    "testing_cp['Prob_dia'] = testing_cp['Prob'].groupby(testing_cp['Date']).transform('mean')\n",
    "testing_cp['Prediction'] = 0\n",
    "testing_cp.loc[testing_cp['Prob_dia']> 0.5, 'Prediction'] = 1\n",
    "testing_cp.drop_duplicates(subset=['Date','Prediction','Label'], inplace=True)\n",
    "testing_cp.head(1)\n",
    "\n",
    "print('Numero de das en testing: ', testing['Date'].nunique())\n",
    "print('Numero de registros en testing: ', testing_cp['Date'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 64 ms\n"
     ]
    }
   ],
   "source": [
    "testing_cp['Label'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    197\n",
       "0    182\n",
       "Name: Label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    280\n",
       "0     99\n",
       "Name: Prediction, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "display(testing_cp['Label'].value_counts(),\n",
    "        testing_cp['Prediction'].value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.41      0.23      0.29       182\n",
      "     class 1       0.50      0.71      0.58       197\n",
      "\n",
      "    accuracy                           0.47       379\n",
      "   macro avg       0.46      0.47      0.44       379\n",
      "weighted avg       0.46      0.47      0.44       379\n",
      "\n",
      "time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(testing_cp.Label, testing_cp.Prediction, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creates a confusion matrix\n",
    "cm = confusion_matrix(testing_cp.Label, testing_cp.Prediction) \n",
    "\n",
    "# Transform to df for easier plotting\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['Sube','Baja'], \n",
    "                     columns = ['Sube','Baja'])\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cm_df, annot=True, cmap=\"Greens\", fmt='g')\n",
    "plt.title('Neuronal Network \\nAccuracy:{0:.3f}'.format(accuracy_score(testing_cp.Label, testing_cp.Prediction)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.save_checkpoint()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resultados/7/RNN_Model_Base.ipynb'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 329 ms\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "copyfile('RNN_Model_Base_GPU.ipynb', folder + '/RNN_Model_Base.ipynb' )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhtKTbzPEwz7PyaEe0FkIO",
   "collapsed_sections": [],
   "name": "RNN - Meli.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
