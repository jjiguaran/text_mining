{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_train = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/individual_versions/Ed/Output_files/train.csv')\n",
    "topics_test = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/individual_versions/Ed/Output_files/test.csv')\n",
    "individual_sum = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/Data/embeddings_sum_individual.zip')\n",
    "individual_averages = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/Data/embeddings_average_individual.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_train.columns = list(topics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [topics_train, topics_test]\n",
    "topics_full = pd.concat(topics, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RedditNews = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/Data/RedditNews.csv')\n",
    "combined = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/Data/Combined_News_DJIA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RedditNews = RedditNews[RedditNews['Date'].isin(combined['Date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54798</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"South Korea's parliament votes to legalize c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54804</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"The German economy is rapidly improving, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54819</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"BBC News - Russian spy suspect missing in Cy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54821</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"Iraq inquiry: secret documents showing Tony ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54822</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b\"Apartheid loves apartheid: Israel's secret r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                               News\n",
       "54798  2010-06-30  b\"South Korea's parliament votes to legalize c...\n",
       "54804  2010-06-30  b\"The German economy is rapidly improving, wit...\n",
       "54819  2010-06-30  b\"BBC News - Russian spy suspect missing in Cy...\n",
       "54821  2010-06-30  b\"Iraq inquiry: secret documents showing Tony ...\n",
       "54822  2010-06-30  b\"Apartheid loves apartheid: Israel's secret r..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54799</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'Pope rebukes cardinal who exposed abuse: \\nP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54800</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'This depression is similar to the Great Pani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54801</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'The Niger Delta has experienced oil spills o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54802</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'G20 Toronto - So Black Block get green light...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54803</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>b'Half of Afghanistans 476 women prisoners wer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                               News\n",
       "54799  2010-06-30  b'Pope rebukes cardinal who exposed abuse: \\nP...\n",
       "54800  2010-06-30  b'This depression is similar to the Great Pani...\n",
       "54801  2010-06-30  b'The Niger Delta has experienced oil spills o...\n",
       "54802  2010-06-30  b'G20 Toronto - So Black Block get green light...\n",
       "54803  2010-06-30  b'Half of Afghanistans 476 women prisoners wer..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Hay error en la codificación de caracteres especiales, encontré ese, pero hay que ver que otros surgen\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"year.old\", \" year old \", regex=True)\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"\\d year\", \" year \", regex=True)\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"year old\", \"years old\", regex=True)\n",
    "\n",
    "\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"years.old\", \" years old \", regex=True)\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"\\d years\", \" years \", regex=True)\n",
    "## Hay error en la codificación de caracteres especiales, encontré ese, pero hay que ver que otros surgen\n",
    "index_review = RedditNews[(RedditNews['News'].str.startswith('b\"')) |\n",
    "                         (RedditNews['News'].str.startswith(\"b'\"))].index\n",
    "\n",
    "display(RedditNews[RedditNews['News'].str.startswith('b\"')].head(),\n",
    "        RedditNews[RedditNews['News'].str.startswith(\"b'\")].head())\n",
    "\n",
    "\n",
    "RedditNews['News'] = RedditNews['News'].str.replace('^b\\\"', \" \", regex=True)\n",
    "RedditNews['News'] = RedditNews['News'].str.replace(\"^b\\'\", \" \", regex=True)\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words(\"english\")\n",
    "REPLACE_BY_SPACE_RE = re.compile('[(){}\\[\\]\\|@,;-]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z ^#\\+_]')\n",
    "SEP_NUMBER = re.compile('(?<=\\d)\\,|\\.(?=\\d)')\n",
    "USA_ABREV = re.compile('U\\.S|u\\.s\\|u\\.s\\.a\\.|US')\n",
    "DOT_ABREV = re.compile('\\.(?![a-zA-Z]{2})')\n",
    "STARTING_B = re.compile(\"^\\\"b' |^b \")\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    \n",
    "    text = USA_ABREV.sub(' usa ', text) # replace U.S U.S. u.s US for usa\n",
    "    text = text.lower() # lowercase text\n",
    "    text = text.replace(\"al-qaeda\", \"alqaeda\")\n",
    "    text = text.replace(\"al-qa'eda\", \"alqaeda\")\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&', '')    \n",
    "\n",
    "    text = DOT_ABREV.sub('', text) # removes abrevetion dot, ej: L.G.B.T  = LGBT\n",
    "    text = SEP_NUMBER.sub('', text) # removes . and , seprating numbers\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub(' ', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = STARTING_B.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    text = text.strip()\n",
    "    return text\n",
    "    \n",
    "RedditNews['News'] = RedditNews['News'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15303</th>\n",
       "      <td>2014-10-27</td>\n",
       "      <td>4 banks including jpmorgan fined europe cartel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15299</th>\n",
       "      <td>2014-10-28</td>\n",
       "      <td>4 banks including jpmorgan fined europe cartel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20850</th>\n",
       "      <td>2014-03-19</td>\n",
       "      <td>900 workers already died building qatars world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>2014-03-14</td>\n",
       "      <td>900 workers already died building qatars world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27847</th>\n",
       "      <td>2013-06-13</td>\n",
       "      <td>afghan parliament upholds right marry children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20311</th>\n",
       "      <td>2014-04-10</td>\n",
       "      <td>usa troops may sent eastern europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>2016-04-26</td>\n",
       "      <td>watchdog says press freedom decline new era pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>watchdog says press freedom decline new era pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50278</th>\n",
       "      <td>2010-12-28</td>\n",
       "      <td>wikileaks swedish government hid anti terror o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50602</th>\n",
       "      <td>2010-12-15</td>\n",
       "      <td>wikileaks swedish government hid anti terror o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                               News\n",
       "15303  2014-10-27  4 banks including jpmorgan fined europe cartel...\n",
       "15299  2014-10-28  4 banks including jpmorgan fined europe cartel...\n",
       "20850  2014-03-19  900 workers already died building qatars world...\n",
       "20998  2014-03-14  900 workers already died building qatars world...\n",
       "27847  2013-06-13     afghan parliament upholds right marry children\n",
       "...           ...                                                ...\n",
       "20311  2014-04-10                 usa troops may sent eastern europe\n",
       "1652   2016-04-26  watchdog says press freedom decline new era pr...\n",
       "1780   2016-04-21  watchdog says press freedom decline new era pr...\n",
       "50278  2010-12-28  wikileaks swedish government hid anti terror o...\n",
       "50602  2010-12-15  wikileaks swedish government hid anti terror o...\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = RedditNews['News'].duplicated(keep=False)\n",
    "print(len(RedditNews[mask]))\n",
    "RedditNews[mask].sort_values('News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = RedditNews.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.drop_duplicates(subset=['News'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = list(algo.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_filt = individual_sum.iloc[lista,].drop(columns=['Unnamed: 0'])\n",
    "average_filt = individual_averages.iloc[lista,].drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {}\n",
    "for i in range(10):\n",
    "    names[str(i)] = 'topic_' + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_full = topics_full.rename(columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "for i in algo.News:\n",
    "    sentiment.append(TextBlob(i).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_only = topics_full[list(names.values())+ ['Top', 'Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_topics = pd.merge(sum_filt, topics_only, left_on=['Date','Top'] , right_on=['Date','Top'] , how='left')\n",
    "averages_topics = pd.merge(average_filt, topics_only, left_on=['Date','Top'] , right_on=['Date','Top'] , how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_topics.to_csv('sum_topics.csv', index=False)\n",
    "averages_topics.to_csv('averages_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_filt['sentiment'] = sentiment\n",
    "average_filt['sentiment'] = sentiment\n",
    "sums_topics['sentiment'] = sentiment\n",
    "averages_topics['sentiment'] = sentiment\n",
    "sum_filt.to_csv('sum_sentiment.csv', index=False)\n",
    "average_filt.to_csv('averages_sentiment.csv', index=False)\n",
    "sums_topics.to_csv('sum_topics_sentiment.csv', index=False)\n",
    "averages_topics.to_csv('averages_topics_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_sentiment = averages_topics[['Top', 'Date' , 'sentiment' , 'Label'] + list(names.values()) ]\n",
    "topics_sentiment.to_csv('topics_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '100',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '140',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '148',\n",
       " '149',\n",
       " '150',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '156',\n",
       " '157',\n",
       " '158',\n",
       " '159',\n",
       " '160',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '164',\n",
       " '165',\n",
       " '166',\n",
       " '167',\n",
       " '168',\n",
       " '169',\n",
       " '170',\n",
       " '171',\n",
       " '172',\n",
       " '173',\n",
       " '174',\n",
       " '175',\n",
       " '176',\n",
       " '177',\n",
       " '178',\n",
       " '179',\n",
       " '180',\n",
       " '181',\n",
       " '182',\n",
       " '183',\n",
       " '184',\n",
       " '185',\n",
       " '186',\n",
       " '187',\n",
       " '188',\n",
       " '189',\n",
       " '190',\n",
       " '191',\n",
       " '192',\n",
       " '193',\n",
       " '194',\n",
       " '195',\n",
       " '196',\n",
       " '197',\n",
       " '198',\n",
       " '199',\n",
       " '200',\n",
       " '201',\n",
       " '202',\n",
       " '203',\n",
       " '204',\n",
       " '205',\n",
       " '206',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '210',\n",
       " '211',\n",
       " '212',\n",
       " '213',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '217',\n",
       " '218',\n",
       " '219',\n",
       " '220',\n",
       " '221',\n",
       " '222',\n",
       " '223',\n",
       " '224',\n",
       " '225',\n",
       " '226',\n",
       " '227',\n",
       " '228',\n",
       " '229',\n",
       " '230',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '235',\n",
       " '236',\n",
       " '237',\n",
       " '238',\n",
       " '239',\n",
       " '240',\n",
       " '241',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '247',\n",
       " '248',\n",
       " '249',\n",
       " '250',\n",
       " '251',\n",
       " '252',\n",
       " '253',\n",
       " '254',\n",
       " '255',\n",
       " '256',\n",
       " '257',\n",
       " '258',\n",
       " '259',\n",
       " '260',\n",
       " '261',\n",
       " '262',\n",
       " '263',\n",
       " '264',\n",
       " '265',\n",
       " '266',\n",
       " '267',\n",
       " '268',\n",
       " '269',\n",
       " '270',\n",
       " '271',\n",
       " '272',\n",
       " '273',\n",
       " '274',\n",
       " '275',\n",
       " '276',\n",
       " '277',\n",
       " '278',\n",
       " '279',\n",
       " '280',\n",
       " '281',\n",
       " '282',\n",
       " '283',\n",
       " '284',\n",
       " '285',\n",
       " '286',\n",
       " '287',\n",
       " '288',\n",
       " '289',\n",
       " '290',\n",
       " '291',\n",
       " '292',\n",
       " '293',\n",
       " '294',\n",
       " '295',\n",
       " '296',\n",
       " '297',\n",
       " '298',\n",
       " '299',\n",
       " 'Label',\n",
       " 'Date',\n",
       " 'Top',\n",
       " 'topic_0',\n",
       " 'topic_1',\n",
       " 'topic_2',\n",
       " 'topic_3',\n",
       " 'topic_4',\n",
       " 'topic_5',\n",
       " 'topic_6',\n",
       " 'topic_7',\n",
       " 'topic_8',\n",
       " 'topic_9',\n",
       " 'sentiment']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(averages_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_topics = topics_full[list(names.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_filt = sum_filt.reset_index(drop = True)\n",
    "only_topics = only_topics.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sum = pd.concat([sum_filt, only_topics] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_filt = average_filt.reset_index(drop = True)\n",
    "full_avg = pd.concat([average_filt, only_topics] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_avg.to_csv('averages_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sum.to_csv('sum_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo['Date'] =  pd.to_datetime(algo['Date'], format='%Y-%m-%d')\n",
    "algo.sort_values('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/Data/averages_topics.zip')\n",
    "sums = pd.read_csv('/home/juan/Documents/Academico/Maestria/Text Minnig/Proyecto/text_mining/Data/sum_topics.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_solo = averages.drop(list(names.values()), axis = 1)\n",
    "sums_solo = sums.drop(list(names.values()), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_solo['sentiment'] = sentiment\n",
    "averages_solo['sentiment'] = sentiment\n",
    "sums['sentiment'] = sentiment\n",
    "averages['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_solo.to_csv('sum_sentiment.csv', index=False)\n",
    "averages_solo.to_csv('averages_sentiment.csv', index=False)\n",
    "sums.to_csv('sum_topics_sentiment.csv', index=False)\n",
    "averages.to_csv('averages_topics_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>117 years old woman mexico city finally receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>imf chief backs athens permanent olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>president france says brexit donald trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>british man must give police 24 hours notice s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>100+ nobel laureates urge greenpeace stop oppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49713</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>pentagon thinks attacking iran bad idea usa ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49714</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>caucasus crisis georgia invades south ossetia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49715</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>indian shoe manufactory series like work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49716</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>visitors suffering mental illnesses banned oly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49717</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>help mexico kidnapping surge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49651 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                               News\n",
       "0      2016-07-01  117 years old woman mexico city finally receiv...\n",
       "1      2016-07-01      imf chief backs athens permanent olympic host\n",
       "2      2016-07-01          president france says brexit donald trump\n",
       "3      2016-07-01  british man must give police 24 hours notice s...\n",
       "4      2016-07-01  100+ nobel laureates urge greenpeace stop oppo...\n",
       "...           ...                                                ...\n",
       "49713  2008-08-08  pentagon thinks attacking iran bad idea usa ne...\n",
       "49714  2008-08-08      caucasus crisis georgia invades south ossetia\n",
       "49715  2008-08-08           indian shoe manufactory series like work\n",
       "49716  2008-08-08  visitors suffering mental illnesses banned oly...\n",
       "49717  2008-08-08                       help mexico kidnapping surge\n",
       "\n",
       "[49651 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = algo.News.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['years', 'old', 'woman', 'mexico', 'city', 'finally', 'received', 'birth', 'certificate', 'died', 'hours', 'later', 'trinidad', 'alvarez', 'lira', 'waited', 'years', 'proof', 'born']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
